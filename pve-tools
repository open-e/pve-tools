#!/bin/bash
#    Copyright (c) 2025 Open-E, Inc.
#    All Rights Reserved.
#
#    This file is licensed under the GNU General Public License, Version 3.0 (the "License");
#    you may not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         https://www.gnu.org/licenses/gpl-3.0
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#    GNU General Public License for more details.

export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8
export NCURSES_NO_UTF8_ACS=1

# GLOBAL VARIABLES
# Debug mode flag - automatically set to false during commits
DEBUG_MODE=false

# Version information - RELEASE_DATE and VERSION automatically managed during commits (Option B logic)
VERSION="0.66"
RELEASE_DATE="2025-08-20"

# Script mode flag
VERSION_MODE=false
TEST_UPDATE_MODE=false


# Product information
VENDOR="Open-E"
PRODUCT="JovianDSS"

# Auto-created dataset prefix
# DATASET_AUTO_PREFIX moved to configurable variable dataset_auto_prefix

CONFIG_DIR="/etc/$(basename "$0" .sh)"
CONFIG_FILE="$CONFIG_DIR/config.conf"
FACTORY_DEFAULTS_FILE="$CONFIG_DIR/factory.defaults"
CONNECTED_STORAGE_SERVERS_DIR="$CONFIG_DIR/connected"
LOGS_DIR="/var/log/$(basename "$0" .sh)"
LOG_FILE="$LOGS_DIR/$(basename "$0" .sh).log"

# Storage system API response cache variables
declare -A API_POOLS_CACHE
declare -A API_NAS_VOLUMES_CACHE
API_CACHE_INITIALIZED=false

# Script state variables
selected_confs=()  # Array of selected config files
selected_host=""
selected_host_index=""
current_step=1
total_steps=3
select_all_mode=false
storage_discovery_done=false
cached_config_list=()  # Cache config list to avoid repeated discovery

# Conflict resolution variables
new_vmct_id=""
conflict_selected_host=""

# Storage to IP mapping table
declare -A storage_ip_map

# NFS wizard variables
selected_pool=""
selected_dataset=""
selected_snapshot=""
selected_nfs_ip_index=""
selected_pool_index=""
selected_dataset_index=""
selected_snapshot_index=""
nfs_current_step=1
nfs_total_steps=6
oodp_destinations_json=""
oodp_destination_datasets=()

# NFS tuning variables
selected_nfs_storages=""
selected_nfs_nconnect=""
selected_nfs_sync=""

# PVE configuration paths
ALL_PVE_VM_CONF="/etc/pve/nodes/*/qemu-server/*.conf"
ALL_PVE_CT_CONF="/etc/pve/nodes/*/lxc/*.conf"

PVE_VM_CONF_PATH="/etc/pve/nodes/*/qemu-server/"
PVE_CT_CONF_PATH="/etc/pve/nodes/*/lxc/"

# Define device type patterns (from pve-config-backup for comprehensive VM/CT disk detection)
DEVICE_PATTERN="(?:(?:rootfs|mp[0-9]+)|(?:scsi|ide|sata|virtio|efidisk|tpmstate|nvme)[0-9]+)"

# CLEANUP FUNCTIONS

# Cleanup function to restore terminal settings and clear screen
cleanup_and_exit() {
    # Skip cleanup if we're just showing version
    if [ "$VERSION_MODE" = true ]; then
        exit 0
    fi
    
    # Kill any background dialog processes
    pkill -f "dialog.*Please wait" 2>/dev/null || true
    
    # Clear the screen
    clear
    
    # Reset terminal settings
    reset
    
    # Show cursor (in case it was hidden)
    tput cnorm 2>/dev/null || true
    
    # Restore terminal to normal mode
    stty sane 2>/dev/null || true
    
    log_info "Script cleanup completed - terminal restored"
    
    exit 0
}

# Setup trap handlers for cleanup
trap cleanup_and_exit EXIT
trap cleanup_and_exit SIGINT
trap cleanup_and_exit SIGTERM
trap cleanup_and_exit SIGTSTP

# STORAGE IP MAPPING

# Check if any NFS storage exists in Proxmox VE
check_nfs_storage_exists() {
    log_debug "check_nfs_storage_exists: Checking for NFS storage in /etc/pve/storage.cfg"
    
    if [[ ! -f "/etc/pve/storage.cfg" ]]; then
        log_error "check_nfs_storage_exists: /etc/pve/storage.cfg not found"
        return 1
    fi
    
    # Check if any NFS storage entries exist
    if grep -q "^nfs:" /etc/pve/storage.cfg; then
        log_debug "check_nfs_storage_exists: NFS storage found"
        return 0
    else
        log_debug "check_nfs_storage_exists: No NFS storage found"
        return 1
    fi
}

# Build storage name to IP address mapping table
build_storage_ip_map() {
    log_debug "build_storage_ip_map: Building storage to IP mapping table"
    
    # Cache file for storage mapping
    local cache_file="/tmp/pve-tools-storage-map.cache"
    local storage_cfg="/etc/pve/storage.cfg"
    
    # Check if cache is valid (storage.cfg not modified since cache creation)
    if [[ -f "$cache_file" ]] && [[ -f "$storage_cfg" ]]; then
        if [[ "$cache_file" -nt "$storage_cfg" ]]; then
            log_debug "build_storage_ip_map: Using cached storage mapping"
            # Load from cache
            storage_ip_map=()
            while IFS='|' read -r storage_name server_ip; do
                storage_ip_map["$storage_name"]="$server_ip"
            done < "$cache_file"
            log_debug "build_storage_ip_map: Loaded ${#storage_ip_map[@]} entries from cache"
            return 0
        fi
    fi
    
    # Clear existing mapping
    storage_ip_map=()
    
    # Read entire file at once for better performance
    local storage_config
    storage_config=$(<"$storage_cfg")
    local current_storage=""
    local in_nfs_section=false
    
    while IFS= read -r line; do
        if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
            current_storage="${BASH_REMATCH[1]}"
            in_nfs_section=true
        elif [[ "$in_nfs_section" == true ]]; then
            if [[ "$line" =~ ^[[:space:]]*server[[:space:]]+([^[:space:]]+) ]]; then
                local server_ip="${BASH_REMATCH[1]}"
                storage_ip_map["$current_storage"]="$server_ip"
                log_debug "build_storage_ip_map: Mapped storage '$current_storage' to IP '$server_ip'"
                in_nfs_section=false
            elif [[ "$line" =~ ^[[:space:]]*$ ]] || [[ "$line" =~ ^[a-zA-Z]+: ]]; then
                # Empty line or start of new section
                in_nfs_section=false
            fi
        fi
    done <<< "$storage_config"
    
    # Save to cache
    if [[ ${#storage_ip_map[@]} -gt 0 ]]; then
        true > "$cache_file"  # Clear cache file
        for storage_name in "${!storage_ip_map[@]}"; do
            echo "${storage_name}|${storage_ip_map[$storage_name]}" >> "$cache_file"
        done
        log_debug "build_storage_ip_map: Saved ${#storage_ip_map[@]} entries to cache"
    fi
    
    log_info "build_storage_ip_map: Built mapping for ${#storage_ip_map[@]} storage entries"
}

# Get IP address for a specific storage name
get_storage_ip() {
    local storage_name="$1"
    
    if [[ -z "$storage_name" ]]; then
        log_error "get_storage_ip: storage_name is required"
        return 1
    fi
    
    log_debug "get_storage_ip: Looking for storage '$storage_name'"
    log_debug "get_storage_ip: storage_ip_map has ${#storage_ip_map[@]} entries"
    
    # Debug: List all available storage names
    if [[ ${#storage_ip_map[@]} -gt 0 ]]; then
        log_debug "get_storage_ip: Available storage names in map:"
        for key in "${!storage_ip_map[@]}"; do
            log_debug "  - '$key'"
        done
    else
        log_error "get_storage_ip: storage_ip_map is empty!"
    fi
    
    if [[ -n "${storage_ip_map[$storage_name]}" ]]; then
        local ip="${storage_ip_map[$storage_name]}"
        log_debug "get_storage_ip: Found IP '$ip' for storage '$storage_name'"
        selected_nfs_ip="$ip"
        echo "$ip"
        return 0
    else
        log_error "get_storage_ip: No IP mapping found for storage '$storage_name'"
        log_error "get_storage_ip: Exact match failed. Checking for similar names..."
        
        # Check for partial matches
        for key in "${!storage_ip_map[@]}"; do
            if [[ "$key" == *"$storage_name"* ]] || [[ "$storage_name" == *"$key"* ]]; then
                log_error "get_storage_ip: Found similar storage name: '$key'"
            fi
        done
        
        return 1
    fi
}

# LOGGING FUNCTIONS

# Initialize logging system
init_logging() {
    if [ ! -d "$LOGS_DIR" ]; then
        mkdir -p "$LOGS_DIR" || {
            echo "Error: Cannot create log directory $LOGS_DIR" >&2
            return 1
        }
    fi
    
    if [ ! -f "$LOG_FILE" ]; then
        touch "$LOG_FILE" || {
            echo "Error: Cannot create log file $LOG_FILE" >&2
            return 1
        }
    fi
    
    return 0
}

# Base logging function with timestamp
log() {
    local level="$1"
    local message="$2"
    local timestamp
    timestamp=$(date '+[%Y-%m-%d %H:%M:%S]')
    
    echo "${timestamp} ${level}: ${message}" >> "$LOG_FILE"
}

# Log info message
log_info() {
    if [[ -f "$LOG_FILE" ]]; then
        log "INFO" "$1"
    fi
}

# Log error message
log_error() {
    if [[ -f "$LOG_FILE" ]]; then
        log "ERROR" "$1"
    fi
}

# Log debug message
log_debug() {
    if [[ "$DEBUG_MODE" == "true" && -f "$LOG_FILE" ]]; then
        log "DEBUG" "$1"
    fi
}

# DEBUG-ONLY ERROR CAPTURE SYSTEM
# Initialize error capture for debug mode only
init_debug_error_capture() {
    if [[ "$DEBUG_MODE" == "true" ]]; then
        log_debug "Initializing debug error capture system"
        
        # Save original stderr for critical system errors
        exec 3>&2
        
        # Redirect all stderr to logger function
        exec 2> >(while read -r line; do
            # Skip empty lines and dialog/terminal control sequences
            if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*$ && ! "$line" =~ $'\e' ]]; then
                log_error "STDERR: $line"
            fi
        done)
        
        # Set up error trap for failed commands
        error_trap() {
            local exit_code=$?
            local line_number=${BASH_LINENO[0]}
            local command="$BASH_COMMAND"
            
            # Skip expected failures and benign operations
            local skip_patterns=(
                '^(test|\[|\[\[)'           # test commands
                '^(grep|egrep|fgrep)'       # grep commands (no matches = exit 1)
                '^dialog'                   # dialog commands (user cancel = exit 1)
                '".*cmd.*options.*2>'       # dialog command arrays (user cancel = exit 1)
                '^read.*-t.*discard'        # timeout reads for buffer clearing
                '^\(\('                     # arithmetic expressions ((var++))
                '^return( [0-9]+| \$[a-zA-Z_][a-zA-Z0-9_]*)?$'  # explicit return statements with numbers or variables
                '=[^=]*\$\('               # command substitutions (var=$(cmd))
            )
            
            local should_skip=false
            for pattern in "${skip_patterns[@]}"; do
                if [[ "$command" =~ $pattern ]]; then
                    should_skip=true
                    break
                fi
            done
            
            # Only log non-zero exits that aren't in our skip list
            if [[ $exit_code -ne 0 && "$should_skip" == "false" ]]; then
                log_error "Command failed at line $line_number: '$command' (exit code: $exit_code)"
            fi
        }
        
        # Enable error trap
        set -E  # Enable ERR trap inheritance
        trap error_trap ERR
        
        log_debug "Debug error capture system initialized"
    fi
}

# API RESPONSE CACHING FUNCTIONS

# Initialize API cache for all connected storage servers  
init_api_cache() {
    if [[ "$API_CACHE_INITIALIZED" == "true" ]]; then
        log_debug "API cache already initialized"
        return 0
    fi
    
    log_debug "Initializing API cache for $PRODUCT servers"
    
    # Clear existing cache
    API_POOLS_CACHE=()
    API_NAS_VOLUMES_CACHE=()
    
    # Get all unique NFS IPs from storage configuration
    local nfs_ips
    nfs_ips=$(awk '/^nfs:[[:space:]]+/ {
        in_block = 1
        storage = $2
    }
    in_block && /^[[:space:]]*server[[:space:]]+/ {
        print $2
        in_block = 0
    }' /etc/pve/storage.cfg | sort -u)
    
    # Cache pools and nas-volumes for each IP
    while IFS= read -r nfs_ip; do
        if [[ -n "$nfs_ip" ]]; then
            log_debug "Caching API data for IP: $nfs_ip"
            cache_pools_for_ip "$nfs_ip"
            cache_nas_volumes_for_ip "$nfs_ip"
        fi
    done <<< "$nfs_ips"
    
    API_CACHE_INITIALIZED=true
    log_debug "API cache initialization completed"
}

# Cache pools data for a specific IP
cache_pools_for_ip() {
    local nfs_ip="$1"
    
    if [[ -z "$nfs_ip" ]]; then
        log_error "cache_pools_for_ip: nfs_ip is required"
        return 1
    fi
    
    local response
    local url="https://$nfs_ip:$rest_api_port/api/v4/pools"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CACHE POOLS ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
    log_debug "=================================================================="
    
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        "$url" 2>/dev/null)
    
    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "cache_pools_for_ip: Failed to cache pools for IP $nfs_ip"
        return 1
    fi
    
    # Store raw response in cache
    API_POOLS_CACHE["$nfs_ip"]="$response"
    log_debug "Cached pools data for IP: $nfs_ip"
}

# Cache nas-volumes data for all pools of a specific IP
cache_nas_volumes_for_ip() {
    local nfs_ip="$1"
    
    if [[ -z "$nfs_ip" ]]; then
        log_error "cache_nas_volumes_for_ip: nfs_ip is required"
        return 1
    fi
    
    # Get pool names from cached pools data
    local pools_response="${API_POOLS_CACHE[$nfs_ip]}"
    if [[ -z "$pools_response" ]]; then
        log_error "cache_nas_volumes_for_ip: No cached pools data for IP $nfs_ip"
        return 1
    fi
    
    # Parse pool names from cached response
    local pool_names
    pool_names=$(printf '%s' "$pools_response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get('data', [])
    for pool in pools:
        name = pool.get('name')
        if name:
            print(name)
except:
    pass
" 2>/dev/null)
    
    # Cache nas-volumes for each pool
    while IFS= read -r pool_name; do
        if [[ -n "$pool_name" ]]; then
            local response
            local url="https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes"
            
            # Log the FULL curl command for manual testing
            log_debug "=============== FULL CURL COMMAND FOR CACHE NAS VOLUMES ==============="
            log_debug "curl -k -s --connect-timeout 5 --max-time 30 -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
            log_debug "========================================================================"
            
            response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" \
                -H 'Content-Type: application/json' \
                "$url" 2>/dev/null)
            
            local curl_exit=$?
            if [[ $curl_exit -eq 0 ]]; then
                API_NAS_VOLUMES_CACHE["${nfs_ip}:${pool_name}"]="$response"
                log_debug "Cached nas-volumes for IP: $nfs_ip, Pool: $pool_name"
            fi
        fi
    done <<< "$pool_names"
}

# Get cached pools data for an IP
get_cached_pools() {
    local nfs_ip="$1"
    
    if [[ -z "$nfs_ip" ]]; then
        log_error "get_cached_pools: nfs_ip is required"
        return 1
    fi
    
    local response="${API_POOLS_CACHE[$nfs_ip]}"
    if [[ -z "$response" ]]; then
        log_debug "get_cached_pools: No cached data for IP $nfs_ip (cache miss)"
        return 1
    fi
    
    # Parse and return pool names
    printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get('data', [])
    for pool in pools:
        name = pool.get('name')
        if name:
            print(name)
except:
    sys.exit(1)
" 2>/dev/null
}

# Get cached nas-volumes data for an IP and pool
get_cached_nas_volumes() {
    local nfs_ip="$1"
    local pool_name="$2"
    
    if [[ -z "$nfs_ip" || -z "$pool_name" ]]; then
        log_error "get_cached_nas_volumes: nfs_ip and pool_name are required"
        return 1
    fi
    
    local response="${API_NAS_VOLUMES_CACHE[${nfs_ip}:${pool_name}]}"
    if [[ -z "$response" ]]; then
        log_debug "get_cached_nas_volumes: No cached nas-volumes for IP $nfs_ip, Pool: $pool_name (cache miss)"
        return 1
    fi
    
    printf '%s' "$response"
}

# Clear API cache (call when storage configuration changes)
clear_api_cache() {
    log_debug "Clearing API cache"
    API_POOLS_CACHE=()
    API_NAS_VOLUMES_CACHE=()
    API_CACHE_INITIALIZED=false
}

# OPTIMIZED CACHED VERSIONS OF API FUNCTIONS

# Optimized version of get_dataset_pool_name using cached data
get_dataset_pool_name_cached() {
    local dataset_name="$1"
    local nfs_ip="$2"

    if [[ -z "$dataset_name" ]]; then
        log_error "get_dataset_pool_name_cached: dataset_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name_cached: nfs_ip is required"
        return 1
    fi

    log_debug "get_dataset_pool_name_cached: Looking for dataset '$dataset_name' on IP '$nfs_ip'"

    # Get cached pools
    local pools
    if ! pools=$(get_cached_pools "$nfs_ip"); then
        log_debug "get_dataset_pool_name_cached: No cached pools list available for IP $nfs_ip"
        return 1
    fi

    # Loop through each pool and check if dataset exists
    while IFS= read -r pool_name; do
        if [[ -z "$pool_name" ]]; then
            continue
        fi

        # Get cached nas-volumes for this pool
        local response
        if ! response=$(get_cached_nas_volumes "$nfs_ip" "$pool_name"); then
            continue  # Skip this pool if we can't get cached data
        fi

        # Check if dataset exists in this pool
        local dataset_found
        dataset_found=$(printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            print('FOUND')
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
" 2>/dev/null)

        if [[ "$dataset_found" == "FOUND" ]]; then
            printf '%s' "$pool_name"
            return 0
        fi
    done <<< "$pools"

    # Dataset not found in any pool
    log_debug "get_dataset_pool_name_cached: Dataset '$dataset_name' not found on IP '$nfs_ip'"
    return 1
}

# Optimized version of is_clone using cached data
is_clone_cached() {
    local volume_name="$1"
    local nfs_ip="$2"

    if [[ -z "$volume_name" ]]; then
        log_error "is_clone_cached: volume_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "is_clone_cached: nfs_ip is required"
        return 1
    fi

    # Parse pool and dataset from volume_name
    local pool_name="${volume_name%%/*}"
    local dataset_name="${volume_name#*/}"

    if [[ -z "$pool_name" || -z "$dataset_name" || "$pool_name" == "$dataset_name" ]]; then
        log_error "is_clone_cached: Invalid volume_name format. Expected: pool_name/dataset_name"
        return 1
    fi

    log_debug "is_clone_cached: Checking if '$volume_name' is a clone on IP '$nfs_ip'"

    # Get cached nas-volumes data for this pool
    local response
    if ! response=$(get_cached_nas_volumes "$nfs_ip" "$pool_name"); then
        log_error "is_clone_cached: Failed to get cached nas-volumes data"
        return 1
    fi

    # Parse JSON response to find the dataset and check its origin
    local origin
    origin=$(printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            origin = d.get('origin')
            if origin is None:
                print('None')
            else:
                print(origin)
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
" 2>/dev/null)

    local python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        log_error "is_clone_cached: Failed to parse cached JSON response"
        return 1
    fi

    if [[ "$origin" == "NOT_FOUND" ]]; then
        log_error "is_clone_cached: Dataset '$dataset_name' not found in pool '$pool_name'"
        return 1
    fi

    if [[ "$origin" == "None" ]]; then
        # origin is None, so it's a dataset (not a clone)
        return 1
    else
        # origin is set, so it's a clone
        return 0
    fi
}

# DIALOG DEPENDENCY CHECK

# Check for dialog and install if missing
check_for_dialog_and_install_if_missing() {
    log_info "Checking for dialog dependency"
    
    # Check if dialog is installed
    if command -v dialog >/dev/null 2>&1; then
        log_info "Dialog is already installed"
        return 0
    fi
    
    log_info "Dialog not found, attempting to install"
    log_info "Installing missing dialog package. Please wait..."
    
    # Update package list
    if ! apt-get update >/dev/null 2>&1; then
        echo "Error: Failed to update package list. Please check your network connection." >&2
        log_error "Failed to update package list"
        exit 1
    fi
    
    # Install dialog
    if ! apt-get install -y dialog >/dev/null 2>&1; then
        echo "Error: Failed to install dialog package. Please install it manually: apt-get install dialog" >&2
        log_error "Failed to install dialog via apt-get"
        exit 1
    fi
    
    # Verify installation
    if ! command -v dialog >/dev/null 2>&1; then
        echo "Error: Dialog installation failed. Please install it manually." >&2
        log_error "Dialog installation verification failed"
        exit 1
    fi
    
    log_info "Dialog installed successfully"
    return 0
}

# CONFIGURATION MANAGEMENT

# Initialize configuration system
init_config() {
    log_info "Initializing configuration system"
    
    if [ ! -d "$CONFIG_DIR" ]; then
        mkdir -p "$CONFIG_DIR" || {
            log_error "Cannot create config directory $CONFIG_DIR"
            return 1
        }
        log_info "Created config directory: $CONFIG_DIR"
    fi

    if [ ! -f "$CONFIG_FILE" ]; then
        cat > "$CONFIG_FILE" << EOF
rest_api_user=admin
rest_api_password='admin'
rest_api_port=82
storage_server_web_gui_password=''
rest_api_dest_user=admin
rest_api_dest_password='admin'
custom_ips=
storage_vm_name_contains=$(lower "$PRODUCT")
new_NFS_storage_volume_running_number_starts_with=00
dataset_auto_prefix=datastore-pve
storage_vm_start_at_boot=yes
storage_vm_start_shutdown_order=1
storage_vm_startup_deleay=300
storage_vm_shutdown_timeout=180
storage_vm_boot_order=scsi0
concurrent_vm_disk_move_count=2
number_of_pve_hosts_in_ha_cluster=2
oodp_backup_destination_port=40000
oodp_src_default_plan='1hour=>1minute,3days=>15minutes,2weeks=>1hour'
oodp_dst_default_plan='12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months'
EOF
        log_info "Created default config file: $CONFIG_FILE"
    fi

    # Create factory defaults file if it doesn't exist
    if [ ! -f "$FACTORY_DEFAULTS_FILE" ]; then
        cat > "$FACTORY_DEFAULTS_FILE" << EOF
# Factory defaults for pve-tools configuration
rest_api_user=admin
rest_api_password='admin'
rest_api_port=82
storage_server_web_gui_password=''
rest_api_dest_user=admin
rest_api_dest_password='admin'
custom_ips=
storage_vm_name_contains=$(lower "$PRODUCT")
new_NFS_storage_volume_running_number_starts_with=00
dataset_auto_prefix=datastore-pve
storage_vm_start_at_boot=yes
storage_vm_start_shutdown_order=1
storage_vm_startup_deleay=300
storage_vm_shutdown_timeout=180
storage_vm_boot_order=scsi0
concurrent_vm_disk_move_count=2
number_of_pve_hosts_in_ha_cluster=2
oodp_backup_destination_port=40000
oodp_src_default_plan='1hour=>1minute,3days=>15minutes,2weeks=>1hour'
oodp_dst_default_plan='12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months'
EOF
        log_info "Created factory defaults file: $FACTORY_DEFAULTS_FILE"
    else
        # Check if factory defaults file is missing boot_order field and update it
        if ! grep -q "^storage_vm_boot_order=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing boot_order field"
            # Add boot_order before concurrent_vm_disk_move_count if it exists, otherwise at the end
            if grep -q "^concurrent_vm_disk_move_count=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
                sed -i '/^concurrent_vm_disk_move_count=/i storage_vm_boot_order=scsi0' "$FACTORY_DEFAULTS_FILE"
            else
                echo "storage_vm_boot_order=scsi0" >> "$FACTORY_DEFAULTS_FILE"
            fi
            log_info "Factory defaults file updated with boot_order field"
        fi
        
        # Check if factory defaults file is missing dataset_auto_prefix field and update it
        if ! grep -q "^dataset_auto_prefix=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing dataset_auto_prefix field"
            # Add dataset_auto_prefix after new_NFS_storage_volume_running_number_starts_with if it exists, otherwise at the end
            if grep -q "^new_NFS_storage_volume_running_number_starts_with=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
                sed -i '/^new_NFS_storage_volume_running_number_starts_with=/a dataset_auto_prefix=datastore-pve' "$FACTORY_DEFAULTS_FILE"
            else
                echo "dataset_auto_prefix=datastore-pve" >> "$FACTORY_DEFAULTS_FILE"
            fi
            log_info "Factory defaults file updated with dataset_auto_prefix field"
        fi
        
        # Check if factory defaults file is missing HA cluster size field and update it
        if ! grep -q "^number_of_pve_hosts_in_ha_cluster=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing HA cluster size field"
            echo "number_of_pve_hosts_in_ha_cluster=2" >> "$FACTORY_DEFAULTS_FILE"
            log_info "Factory defaults file updated with HA cluster size field"
        fi
        
        # Check if factory defaults file is missing OODP fields and update them
        if ! grep -q "^oodp_backup_destination_port=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing OODP backup destination port field"
            echo "oodp_backup_destination_port=40000" >> "$FACTORY_DEFAULTS_FILE"
            log_info "Factory defaults file updated with OODP backup destination port field"
        fi
        
        if ! grep -q "^oodp_src_default_plan=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing OODP source plan field"
            echo "oodp_src_default_plan='1hour=>1minute,3days=>15minutes,2weeks=>1hour'" >> "$FACTORY_DEFAULTS_FILE"
            log_info "Factory defaults file updated with OODP source plan field"
        fi
        
        if ! grep -q "^oodp_dst_default_plan=" "$FACTORY_DEFAULTS_FILE" 2>/dev/null; then
            log_info "Updating factory defaults file with missing OODP destination plan field"
            echo "oodp_dst_default_plan='12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months'" >> "$FACTORY_DEFAULTS_FILE"
            log_info "Factory defaults file updated with OODP destination plan field"
        fi
    fi

    # Load configuration
    if [ -f "$CONFIG_FILE" ]; then
        log_debug "init_config: Config file exists, reading content"
        log_debug "init_config: Config file content:"
        while IFS= read -r line; do
            # Mask password in debug output
            if [[ "$line" =~ ^rest_api_password= ]]; then
                # Extract the password value (handling quotes)
                local pwd_value="${line#rest_api_password=}"
                pwd_value="${pwd_value//\'/}"  # Remove single quotes
                pwd_value="${pwd_value//\"/}"  # Remove double quotes
                
                # Create masked version
                local masked_pwd
                if [[ ${#pwd_value} -le 3 ]]; then
                    masked_pwd=$(printf '%*s' ${#pwd_value} "" | tr ' ' '*')
                else
                    masked_pwd="${pwd_value:0:3}$(printf '%*s' $((${#pwd_value} - 3)) "" | tr ' ' '*')"
                fi
                
                # Log the masked version
                log_debug "init_config: rest_api_password='$masked_pwd'"
            else
                log_debug "init_config: $line"
            fi
        done < "$CONFIG_FILE"
        
        source "$CONFIG_FILE"
        log_info "Loaded configuration from $CONFIG_FILE"
        log_debug "init_config: rest_api_user='$rest_api_user'"
        log_debug "init_config: rest_api_password length=${#rest_api_password}"
        
        # Show masked password for debugging
        local masked_pwd
        if [[ ${#rest_api_password} -le 3 ]]; then
            # For very short passwords, mask all characters
            masked_pwd=$(printf '%*s' ${#rest_api_password} "" | tr ' ' '*')
        else
            # Show first 3 characters and mask the rest
            masked_pwd="${rest_api_password:0:3}$(printf '%*s' $((${#rest_api_password} - 3)) "" | tr ' ' '*')"
        fi
        log_debug "init_config: rest_api_password preview='$masked_pwd'"
        log_debug "init_config: rest_api_port='$rest_api_port'"
        
        # Set defaults for new configuration variables if not present
        storage_vm_name_contains="${storage_vm_name_contains:-$(lower "$PRODUCT")}"
        new_NFS_storage_volume_running_number_starts_with="${new_NFS_storage_volume_running_number_starts_with:-00}"
        dataset_auto_prefix="${dataset_auto_prefix:-datastore-pve}"
        storage_vm_start_at_boot="${storage_vm_start_at_boot:-yes}"
        storage_vm_start_shutdown_order="${storage_vm_start_shutdown_order:-1}"
        storage_vm_startup_deleay="${storage_vm_startup_deleay:-300}"
        storage_vm_shutdown_timeout="${storage_vm_shutdown_timeout:-180}"
        storage_vm_boot_order="${storage_vm_boot_order:-scsi0}"
        concurrent_vm_disk_move_count="${concurrent_vm_disk_move_count:-2}"
        number_of_pve_hosts_in_ha_cluster="${number_of_pve_hosts_in_ha_cluster:-2}"
        custom_ips="${custom_ips:-}"
        oodp_backup_destination_port="${oodp_backup_destination_port:-40000}"
        oodp_src_default_plan="${oodp_src_default_plan:-1hour=>1minute,3days=>15minutes,2weeks=>1hour}"
        oodp_dst_default_plan="${oodp_dst_default_plan:-12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months}"
        
        log_debug "init_config: storage_vm_name_contains='$storage_vm_name_contains'"
        log_debug "init_config: new_NFS_storage_volume_running_number_starts_with='$new_NFS_storage_volume_running_number_starts_with'"
        log_debug "init_config: dataset_auto_prefix='$dataset_auto_prefix'"
        log_debug "init_config: storage_vm_start_at_boot='$storage_vm_start_at_boot'"
        log_debug "init_config: storage_vm_start_shutdown_order='$storage_vm_start_shutdown_order'"
        log_debug "init_config: storage_vm_startup_deleay='$storage_vm_startup_deleay'"
        log_debug "init_config: storage_vm_shutdown_timeout='$storage_vm_shutdown_timeout'"
        log_debug "init_config: storage_vm_boot_order='$storage_vm_boot_order'"
    else
        log_error "Config file not found: $CONFIG_FILE"
        return 1
    fi
}

# UTILITY FUNCTIONS

# Convert string to lowercase
lower() {
    echo "$1" | tr '[:upper:]' '[:lower:]'
}

# Trim leading and trailing whitespace from a string
trim_str() {
    local str="$1"
    # Remove leading and trailing whitespace (spaces, tabs, newlines)
    echo "$str" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//'
}

# URL encode a string for safe use in curl credentials
url_encode() {
    local string="$1"
    local encoded=""
    local length=${#string}
    
    log_debug "url_encode: Input string length: $length"
    
    for (( i=0; i<length; i++ )); do
        local char="${string:$i:1}"
        case "$char" in
            [a-zA-Z0-9.~_-]) 
                encoded+="$char" ;;
            *)
                local hex_val
                hex_val=$(printf '%%%02X' "'$char")
                log_debug "url_encode: Encoding char '$char' as '$hex_val'"
                encoded+="$hex_val" ;;
        esac
    done
    
    log_debug "url_encode: Output string length: ${#encoded}"
    echo "$encoded"
}

# Trim leading and trailing quotes and spaces from user input
trimq() {
    echo "${1}" | tr -d " \"'"
}

# Convert string to uppercase
upper() {
    echo "${1}" | tr '[:lower:]' '[:upper:]'
}

# Get VM configuration file path for given VMID
pve_vm_conf_file() {
    local vmid="$1"
    
    if [[ -z "$vmid" ]]; then
        log_error "pve_vm_conf_file: VMID parameter is required"
        return 1
    fi
    
    # Find the VM config file across all nodes
    for conf_file in ${PVE_VM_CONF_PATH}${vmid}.conf; do
        if [[ -f "$conf_file" ]]; then
            echo "$conf_file"
            return 0
        fi
    done
    
    # VM config file not found
    return 1
}

# Get CT configuration file path for given CTID
pve_ct_conf_file() {
    local ctid="$1"
    
    if [[ -z "$ctid" ]]; then
        log_error "pve_ct_conf_file: CTID parameter is required"
        return 1
    fi
    
    # Find the CT config file across all nodes
    for conf_file in ${PVE_CT_CONF_PATH}${ctid}.conf; do
        if [[ -f "$conf_file" ]]; then
            echo "$conf_file"
            return 0
        fi
    done
    
    # CT config file not found
    return 1
}

# Get VM/CT configuration output (replacement for qm config/pct config)
# This function uses direct file reading for better performance
pve_vm_ct_conf_file() {
    local vmid="$1"
    
    if [[ -z "$vmid" ]]; then
        log_error "pve_vm_ct_conf_file: VMID parameter is required"
        return 1
    fi
    
    # Determine if it's a VM or CT
    local type
    type=$(vm_type "$vmid")
    
    if [[ "$type" == "ct" ]]; then
        # Use pve_ct_conf_file for containers
        local conf_file
        conf_file=$(pve_ct_conf_file "$vmid")
        if [[ -n "$conf_file" ]]; then
            cat "$conf_file"
            return 0
        else
            log_error "CT config file not found for CTID: $vmid"
            return 1
        fi
    else
        # Use pve_vm_conf_file for VMs
        local conf_file
        conf_file=$(pve_vm_conf_file "$vmid")
        if [[ -n "$conf_file" ]]; then
            cat "$conf_file"
            return 0
        else
            log_error "VM config file not found for VMID: $vmid"
            return 1
        fi
    fi
}

# Cleanup inactive NFS storage on all cluster nodes
cleanup_inactive_nfs_storage() {
    log_info "Starting NFS cleanup on all cluster nodes"
    
    # Get node list from corosync configuration
    local node_list
    node_list=$(awk '
        $1 == "name:" {node=$2}
        $1 == "ring0_addr:" {print node, $2}
    ' /etc/pve/corosync.conf)
    
    if [ -z "$node_list" ]; then
        log_error "No nodes found in corosync configuration"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    local total_nodes
    total_nodes=$(echo "$node_list" | wc -l)
    log_info "Found cluster nodes: $total_nodes nodes"
    
    dialog --infobox "Initializing NFS cleanup on $total_nodes cluster nodes..." 5 60
    sleep 1
    
    local current_node=0
    # Process each node
    while read -r node ip; do
        ((current_node++))
        local progress_msg="Processing node $current_node/$total_nodes: $node ($ip)\nChecking inactive NFS mounts and orphaned directories..."
        dialog --infobox "$progress_msg" 7 70
        
        log_info "Processing cleanup for node: $node ($ip)"
        log_debug "Starting SSH connection to $node ($ip) with 60 second timeout"
        (
        timeout 60 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@"$ip" "bash -s" << 'ENDSCRIPT'
log_info() { echo "[$(date '+%F %T')] INFO: $*"; }
log_debug() { echo "[$(date '+%F %T')] DEBUG: $*"; }
log_error() { echo "[$(date '+%F %T')] ERROR: $*"; }
log_info "=== Starting NFS cleanup on remote node $(hostname) ==="
log_debug "Getting all mounted NFS directories..."

# Get all mounted NFS directories
all_nfs_mounts=$(mount | grep -E '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+:/\S+\s.+\snfs\s' | awk '{print $3}' | sort)
log_debug "Found NFS mounts: $(echo "$all_nfs_mounts" | wc -l) entries"

# Get active PVE storage mounts
log_debug "Getting active PVE storage mounts..."
active_pve_mounts=$(pvesm status | awk '$2 == "nfs" && $3 == "active" {print "/mnt/pve/" $1}' | sort)
log_debug "Found active PVE mounts: $(echo "$active_pve_mounts" | wc -l) entries"

# Calculate inactive mounts
log_debug "Calculating inactive mounts..."
inactive_mounts=$(comm -23 <(echo "$all_nfs_mounts") <(echo "$active_pve_mounts"))
local inactive_count=$(echo $inactive_mounts | wc -w)
log_info "Inactive mounts to process: $inactive_count mounts"
log_debug "Inactive mounts list: $inactive_mounts"

if [ -z "$inactive_mounts" ]; then
  log_info "No inactive mounts found to clean up"
fi

for mount_point in $inactive_mounts; do
  [ -z "$mount_point" ] && continue
  log_info "Processing mount point: $mount_point"
  
  # Check if mount point is actually mounted
  if mount | grep -q "$mount_point"; then
    log_info "Unmounting $mount_point"
    if umount -l "$mount_point"; then
      log_info "Unmounted: $mount_point"
    else
      log_error "Failed to unmount: $mount_point"
      continue
    fi
  else
    log_info "Mount point already unmounted: $mount_point"
  fi
  
  # Check if directory exists
  if [ ! -d "$mount_point" ]; then
    continue
  fi
  
  # Attempt to remove directory
  log_info "Attempting to remove directory: $mount_point"
  if rmdir "$mount_point" 2>/dev/null; then
    log_info "Removed: $mount_point"
  else
    log_error "Failed to remove directory: $mount_point"
  fi
done

# Also check for orphaned clone directories in /mnt/pve/
log_info "Checking for orphaned clone directories in /mnt/pve/"
log_debug "Running find command for clone directories..."
pve_directories=$(find /mnt/pve/ -maxdepth 1 -type d -name "*clone*" 2>/dev/null | sort)
log_debug "Found clone directories: $(echo "$pve_directories" | wc -l) entries"

echo "$pve_directories" | while read -r dir; do
  [ -z "$dir" ] || [ "$dir" = "/mnt/pve/" ] && continue
  
  # Check if this directory is in active PVE storage
  dir_name=$(basename "$dir")
  is_active=false
  echo "$active_pve_mounts" | while read -r active_mount; do
    if [ "$(basename "$active_mount")" = "$dir_name" ]; then
      is_active=true
      break
    fi
  done
  
  if [ "$is_active" = "true" ]; then
    continue
  fi
  
  # Check if directory is currently mounted
  if mount | grep -q "$dir"; then
    log_info "Directory is mounted but not active, unmounting: $dir"
    if umount -l "$dir"; then
      log_info "Unmounted orphaned directory: $dir"
    else
      log_error "Failed to unmount orphaned directory: $dir"
      continue
    fi
  else
    log_info "Directory not mounted: $dir"
  fi
  
  # Attempt to remove orphaned directory
  log_info "Attempting to remove orphaned directory: $dir"
  if rmdir "$dir" 2>/dev/null; then
    log_info "Removed orphaned: $dir"
  else
    log_error "Failed to remove orphaned directory: $dir"
  fi
done

log_info "=== NFS cleanup completed on remote node $(hostname) ==="
log_info "Cleanup summary: Processed $inactive_count inactive mounts and $(echo "$pve_directories" | wc -l) clone directories"
log_debug "Cleanup operations finished successfully"
ENDSCRIPT
        ssh_exit_code=$?
        if [ $ssh_exit_code -ne 0 ]; then
            echo "[$(date '+%F %T')] ERROR: SSH cleanup failed for node $node ($ip) with exit code: $ssh_exit_code"
        fi
        ) >> "$LOGS_DIR/pve-nfs-cleanup-$node.log" 2>&1 &
        local ssh_pid=$!
        log_debug "Started SSH cleanup for $node ($ip) with PID: $ssh_pid"
    done < <(echo "$node_list")

    dialog --infobox "Waiting for cleanup operations to complete on all $total_nodes nodes...\nThis may take a few moments..." 6 65
    
    # Wait for all background processes with timeout
    log_debug "Waiting for all $total_nodes cleanup processes to complete..."
    local wait_count=0
    local max_wait=120  # 2 minutes timeout
    
    while [ "$(jobs -r | wc -l)" -gt 0 ] && [ $wait_count -lt $max_wait ]; do
        sleep 1
        ((wait_count++))
        if [ $((wait_count % 10)) -eq 0 ]; then
            local remaining_jobs=$(jobs -r | wc -l)
            log_debug "Still waiting... $remaining_jobs jobs remaining after ${wait_count}s"
        fi
    done
    
    local remaining_jobs=$(jobs -r | wc -l)
    if [ "$remaining_jobs" -gt 0 ]; then
        log_error "Timeout: $remaining_jobs cleanup jobs still running after ${max_wait}s"
        log_error "Killing remaining background processes..."
        jobs -p | xargs -r kill -9 2>/dev/null
        dialog --msgbox "Warning: Some cleanup operations timed out and were terminated.\nCheck logs for details." 8 60
        return 1
    else
        log_debug "All cleanup processes completed successfully"
    fi
    
    dialog --infobox "NFS cleanup completed successfully on all $total_nodes nodes!\nCheck logs for detailed results." 6 60
    sleep 2
    
    log_info "NFS cleanup complete on all $total_nodes nodes"
}

# Check pve-config-backup daemon status on all cluster nodes
check_pve_config_backup_status() {
    log_info "Checking pve-config-backup daemon status on all cluster nodes"
    
    dialog --infobox "Checking pve-config-backup daemon status on all nodes..." 5 60

    # Get node list from corosync configuration (optimized)
    local node_list
    node_list=$(awk '$1 == "name:" {node=$2} $1 == "ring0_addr:" {print node, $2}' /etc/pve/corosync.conf)
    
    if [ -z "$node_list" ]; then
        log_error "No nodes found in corosync configuration"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    local nodes_need_install=()
    local nodes_need_start=()
    local nodes_ok=()
    
    # Check each node in parallel for faster execution
    while read -r node ip; do
        (
            log_info "Checking pve-config-backup status on node: $node ($ip)"
            
            # Check if pve-config-backup is installed and get its status (with timeout)
            local status_output
            status_output=$(timeout 10 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@"$ip" "
                if [ -f /usr/local/sbin/pve-config-backup ]; then
                    pve-config-backup status 2>/dev/null || echo 'INSTALLED_BUT_ERROR'
                else
                    echo 'NOT_INSTALLED'
                fi
            " < /dev/null 2>/dev/null)
            
            if [ "$status_output" = "NOT_INSTALLED" ]; then
                echo "$node $ip" >> /tmp/nodes_need_install
                log_info "Node $node: pve-config-backup not installed"
            elif echo "$status_output" | grep -q "Active: inactive (dead)"; then
                echo "$node $ip" >> /tmp/nodes_need_start
                log_info "Node $node: pve-config-backup installed but inactive"
            elif echo "$status_output" | grep -q "Active: active (running)"; then
                echo "$node $ip" >> /tmp/nodes_ok
                log_info "Node $node: pve-config-backup running correctly"
            else
                echo "$node $ip" >> /tmp/nodes_need_install
                log_info "Node $node: pve-config-backup status unknown, will reinstall"
            fi
        ) &
    done < <(echo "$node_list")
    
    # Wait for all parallel checks to complete
    wait
    
    # Read results from temp files
    if [ -f /tmp/nodes_need_install ]; then
        nodes_need_install=($(cat /tmp/nodes_need_install))
        rm -f /tmp/nodes_need_install
    fi
    
    if [ -f /tmp/nodes_need_start ]; then
        nodes_need_start=($(cat /tmp/nodes_need_start))
        rm -f /tmp/nodes_need_start
    fi
    
    if [ -f /tmp/nodes_ok ]; then
        nodes_ok=($(cat /tmp/nodes_ok))
        rm -f /tmp/nodes_ok
    fi
    
    # Report status
    local total_nodes=$(echo "$node_list" | wc -l)
    local install_count=$((${#nodes_need_install[@]} / 2))
    local start_count=$((${#nodes_need_start[@]} / 2))
    local ok_count=$((${#nodes_ok[@]} / 2))
    
    log_info "pve-config-backup status check complete - Total: $total_nodes, OK: $ok_count, Need install: $install_count, Need start: $start_count"
    
    # Install on nodes that need it
    if [ $install_count -gt 0 ]; then
        install_pve_config_backup_on_nodes "${nodes_need_install[@]}"
    fi
    
    # Start on nodes that need it
    if [ $start_count -gt 0 ]; then
        start_pve_config_backup_on_nodes "${nodes_need_start[@]}"
    fi
    
    # Final status check
    dialog --msgbox "pve-config-backup status check complete.\nTotal nodes: $total_nodes\nAlready running: $ok_count\nInstalled: $install_count\nStarted: $start_count" 12 60
}

# Install pve-config-backup on specified nodes
install_pve_config_backup_on_nodes() {
    local nodes=("$@")
    local node_count=$((${#nodes[@]} / 2))
    
    if [ $node_count -eq 0 ]; then
        return 0
    fi
    
    log_info "Installing pve-config-backup on $node_count nodes"
    
    # Process nodes in pairs (node_name node_ip)
    for ((i=0; i<${#nodes[@]}; i+=2)); do
        local node="${nodes[i]}"
        local ip="${nodes[i+1]}"
        
        log_info "Installing pve-config-backup on node: $node ($ip)"
        
        # Create GitHub URL with lowercase vendor
        local vendor_lower github_url
        vendor_lower=$(lower "$VENDOR")
        github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"
        
        nohup bash -c "
        (
        timeout 30 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@'$ip' \"bash -s\" << ENDSCRIPT
# Install pve-config-backup
cd /tmp
wget '$github_url' \\
    -O /usr/local/sbin/pve-config-backup 2>/dev/null

if [ \$? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
    chmod +x /usr/local/sbin/pve-config-backup
    /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
    if [ \$? -eq 0 ]; then
        echo 'INSTALL_SUCCESS'
    else
        echo 'INSTALL_FAILED'
    fi
else
    echo 'DOWNLOAD_FAILED'
fi
ENDSCRIPT
        ) >> '$LOGS_DIR/pve-config-backup-install-$node.log' 2>&1
        " >/dev/null 2>&1 &
    done
    
    wait
    log_info "pve-config-backup installation complete on all nodes"
}

# Start pve-config-backup daemon on specified nodes
start_pve_config_backup_on_nodes() {
    local nodes=("$@")
    local node_count=$((${#nodes[@]} / 2))
    
    if [ $node_count -eq 0 ]; then
        return 0
    fi
    
    log_info "Starting pve-config-backup daemon on $node_count nodes"
    
    # Process nodes in pairs (node_name node_ip)
    for ((i=0; i<${#nodes[@]}; i+=2)); do
        local node="${nodes[i]}"
        local ip="${nodes[i+1]}"
        
        log_info "Starting pve-config-backup daemon on node: $node ($ip)"
        
        (
        timeout 15 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@"$ip" "bash -s" << 'ENDSCRIPT'
# Start pve-config-backup daemon
if [ -f /usr/local/sbin/pve-config-backup ]; then
    /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
    if [ $? -eq 0 ]; then
        echo "START_SUCCESS"
    else
        echo "START_FAILED"
    fi
else
    echo "NOT_INSTALLED"
fi
ENDSCRIPT
        ) >> "$LOGS_DIR/pve-config-backup-start-$node.log" 2>&1 &
    done
    
    wait
    log_info "pve-config-backup daemon start complete on all nodes"
}

# Ensure pve-config-backup is installed and running on HA nodes (or all nodes if no HA group)
ensure_pve_config_backup_running() {
    log_info "Ensuring pve-config-backup daemon is installed and running on HA nodes"

    # Get HA nodes list (fallback to all nodes if no HA group exists)
    local ha_nodes_list
    ha_nodes_list=$(get_ha_nodes)
    
    if [ -z "$ha_nodes_list" ]; then
        log_error "No nodes found in HA configuration or cluster"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    # Build node list with IPs by looking up each node in corosync.conf
    local node_list=""
    while IFS= read -r node_name; do
        [ -z "$node_name" ] && continue  # Skip empty lines
        local node_ip
        node_ip=$(awk -v node="$node_name" '
            $1 == "name:" && $2 == node {found=1; next}
            found && $1 == "ring0_addr:" {print $2; found=0}
        ' /etc/pve/corosync.conf)
        
        if [ -n "$node_ip" ]; then
            node_list+="$node_name $node_ip"$'\n'
        else
            log_error "Could not find IP for node: $node_name"
        fi
    done <<< "$ha_nodes_list"
    
    # Remove trailing newline
    node_list=$(echo "$node_list" | sed '/^$/d')
    
    if [ -z "$node_list" ]; then
        log_error "No valid node IPs found"
        dialog --msgbox "Error: No valid node IPs found" 8 40
        return 1
    fi
    
    local total_nodes=$(echo "$node_list" | wc -l)
    local installed=0
    local started=0
    local already_running=0
    
    # Clean up any previous results
    rm -f /tmp/node_results
    
    # Process each node
    while IFS= read -r line; do
        [ -z "$line" ] && continue  # Skip empty lines
        
        local node=$(echo "$line" | cut -d' ' -f1)
        local ip=$(echo "$line" | cut -d' ' -f2)
        
        log_info "Ensuring pve-config-backup is ready on node: $node ($ip)"
        
        # Create GitHub URL with lowercase vendor  
        local vendor_lower github_url
        vendor_lower=$(lower "$VENDOR")
        github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"

        # Check and handle pve-config-backup on this node
        local result
        result=$(ssh -o StrictHostKeyChecking=no root@"$ip" "bash -s" << ENDSCRIPT
# Check if pve-config-backup is installed
if [ ! -f /usr/local/sbin/pve-config-backup ]; then
    # Install pve-config-backup
    cd /tmp
    wget '$github_url' \\
        -O /usr/local/sbin/pve-config-backup >/dev/null 2>&1
    
    if [ \$? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
        chmod +x /usr/local/sbin/pve-config-backup
        /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
        if [ \$? -eq 0 ]; then
            echo "INSTALLED"
            exit 0
        else
            echo "INSTALL_FAILED"
            exit 1
        fi
    else
        echo "DOWNLOAD_FAILED"
        exit 1
    fi
fi

# Check daemon status
status_output=\$(/usr/local/sbin/pve-config-backup status 2>/dev/null)
if echo "\$status_output" | grep -q "Active: active (running)"; then
    echo "ALREADY_RUNNING"
elif echo "\$status_output" | grep -q "Active: inactive (dead)"; then
    # Start the daemon
    /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
    if [ \$? -eq 0 ]; then
        echo "STARTED"
    else
        echo "START_FAILED"
        exit 1
    fi
else
    echo "STATUS_UNKNOWN"
    exit 1
fi
ENDSCRIPT
        2>/dev/null)
        
        case "$result" in
            "INSTALLED")
                log_info "Node $node: pve-config-backup installed and started"
                echo "INSTALLED" >> /tmp/node_results
                ;;
            "STARTED")
                log_info "Node $node: pve-config-backup daemon started"
                echo "STARTED" >> /tmp/node_results
                ;;
            "ALREADY_RUNNING")
                log_info "Node $node: pve-config-backup already running"
                echo "ALREADY_RUNNING" >> /tmp/node_results
                ;;
            *)
                log_error "Node $node: Failed to ensure pve-config-backup is running"
                echo "FAILED" >> /tmp/node_results
                ;;
        esac
    done < <(echo "$node_list")
    
    # Count results
    if [ -f /tmp/node_results ]; then
        installed=$(grep -c "INSTALLED" /tmp/node_results)
        started=$(grep -c "STARTED" /tmp/node_results)
        already_running=$(grep -c "ALREADY_RUNNING" /tmp/node_results)
        rm -f /tmp/node_results
    fi
    
    log_info "pve-config-backup setup complete on HA nodes - Total: $total_nodes, Already running: $already_running, Installed: $installed, Started: $started"
    
    # Only show dialog if action was taken (installations or starts)
    if [ "$installed" -gt 0 ] || [ "$started" -gt 0 ]; then
        dialog --msgbox "pve-config-backup setup complete on HA nodes.\nTotal nodes: $total_nodes\nAlready running: $already_running\nInstalled: $installed\nStarted: $started" 12 60
    fi
    
    return 0
}

# Cleanup unused disks in VM/CT configs
cleanup_unused_disks_wizard() {
    log_info "Starting cleanup unused disks wizard"
    
    # Arrays to store found unused disks
    local unused_disks=()
    local unused_display=()
    
    # Scan all VM configs
    for config_file in $ALL_PVE_VM_CONF; do
        [ -f "$config_file" ] || continue
        
        local vmid=$(basename "$config_file" .conf)
        local node=$(basename "$(dirname "$(dirname "$config_file")")")
        
        # Search for unused disk lines
        while IFS= read -r line; do
            if [[ "$line" =~ ^unused[0-9]+: ]]; then
                unused_disks+=("vm:$vmid:$node:$line")
                unused_display+=("$vmid" "VM - $line")
            fi
        done < "$config_file"
    done
    
    # Scan all CT configs
    for config_file in $ALL_PVE_CT_CONF; do
        [ -f "$config_file" ] || continue
        
        local ctid=$(basename "$config_file" .conf)
        local node=$(basename "$(dirname "$(dirname "$config_file")")")
        
        # Search for unused disk lines
        while IFS= read -r line; do
            if [[ "$line" =~ ^unused[0-9]+: ]]; then
                unused_disks+=("ct:$ctid:$node:$line")
                unused_display+=("$ctid" "CT - $line")
            fi
        done < "$config_file"
    done
    
    # Check if any unused disks found
    if [ ${#unused_disks[@]} -eq 0 ]; then
        dialog --msgbox "No unused disks found in VM/CT configurations." 8 50
        return 0
    fi
    
    log_info "Found ${#unused_disks[@]} unused disk entries"
    
    # Create checklist dialog
    local dialog_selected=$(mktemp)
    local checklist_items=()
    local all_selected=false
    
    # Build initial checklist items
    for ((i=0; i<${#unused_display[@]}; i+=2)); do
        local vmid="${unused_display[i]}"
        local desc="${unused_display[i+1]}"
        checklist_items+=("$((i/2))" "$vmid: $desc" "off")
        log_debug "Added checklist item: index=$((i/2)), desc='$vmid: $desc', status=off"
    done
    
    log_debug "Total checklist_items array length: ${#checklist_items[@]}"
    
    while true; do
        local middle_button_label="Select All"
        if [ "$all_selected" = true ]; then
            middle_button_label="Deselect All"
        fi
        
        log_debug "About to show dialog with button: $middle_button_label"
        log_debug "Current all_selected state: $all_selected"
        
        dialog --keep-tite --title "Cleanup Unused Disks" \
               --ok-label "Remove Selected" --cancel-label "Back" \
               --extra-button --extra-label "$middle_button_label" \
               --checklist "Select unused disks to remove (use SPACE to select):" \
               20 90 10 "${checklist_items[@]}" 2> "$dialog_selected"
        
        local exit_code=$?
        log_debug "Dialog exit code: $exit_code"
        
        case $exit_code in
            0)  # Remove Selected
                log_debug "User pressed Remove Selected"
                local selected_indices=$(cat "$dialog_selected")
                log_debug "Selected indices: $selected_indices"
                if [ -z "$selected_indices" ]; then
                    dialog --msgbox "No items selected." 6 30
                    continue
                fi
                
                # Confirm removal
                dialog --keep-tite --title "Confirm Removal" \
                       --yes-label "Yes, Remove" --no-label "Cancel" \
                       --yesno "Are you sure you want to remove the selected unused disk entries?" 8 70
                
                if [ $? -eq 0 ]; then
                    cleanup_selected_unused_disks "$selected_indices" unused_disks
                fi
                break
                ;;
            1)  # Back
                log_debug "User pressed Back"
                rm -f "$dialog_selected"
                return 0
                ;;
            2)  # Toggle Select All / Deselect All (extra button)
                log_debug "User pressed toggle button (exit code 2)"
                if [ "$all_selected" = false ]; then
                    # Select all
                    log_info "Selecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="on"
                        log_debug "Set checklist_items[$i]=on"
                    done
                    all_selected=true
                else
                    # Deselect all
                    log_info "Deselecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="off"
                        log_debug "Set checklist_items[$i]=off"
                    done
                    all_selected=false
                fi
                continue
                ;;
            3)  # Toggle Select All / Deselect All (help button alternative)
                log_debug "User pressed toggle button (exit code 3)"
                if [ "$all_selected" = false ]; then
                    # Select all
                    log_info "Selecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="on"
                        log_debug "Set checklist_items[$i]=on"
                    done
                    all_selected=true
                else
                    # Deselect all
                    log_info "Deselecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="off"
                        log_debug "Set checklist_items[$i]=off"
                    done
                    all_selected=false
                fi
                continue
                ;;
            *)  # Any other exit code
                log_debug "Unexpected dialog exit code: $exit_code"
                rm -f "$dialog_selected"
                return 1
                ;;
        esac
    done
    
    rm -f "$dialog_selected"
    return 0
}

# Remove selected unused disk entries
cleanup_selected_unused_disks() {
    local selected_indices="$1"
    local -n disk_array=$2
    local removed_count=0
    
    log_info "Starting removal of selected unused disk entries"
    
    # Process each selected index
    for index in $selected_indices; do
        local disk_info="${disk_array[index]}"
        IFS=':' read -ra parts <<< "$disk_info"
        local type="${parts[0]}"
        local vmid="${parts[1]}"
        local node="${parts[2]}"
        local unused_line="${parts[3]}"
        
        # Determine config file path
        local config_file
        if [ "$type" = "vm" ]; then
            config_file="/etc/pve/nodes/$node/qemu-server/$vmid.conf"
        else
            config_file="/etc/pve/nodes/$node/lxc/$vmid.conf"
        fi
        
        # Extract unused disk identifier (e.g., "unused0")
        local unused_key
        unused_key=$(echo "$unused_line" | cut -d':' -f1)
        
        log_info "Removing $unused_key from $type $vmid config"
        
        # Remove the unused line using sed
        if sed -i "/^$unused_key:/d" "$config_file"; then
            log_info "Successfully removed $unused_key from $config_file"
            ((removed_count++))
        else
            log_error "Failed to remove $unused_key from $config_file"
        fi
    done
    
    log_info "Cleanup complete. Removed $removed_count unused disk entries"
    dialog --msgbox "Cleanup complete!\nRemoved $removed_count unused disk entries." 8 50
}


# Execute dialog menu and capture result
dialog_menu() {
    log_debug "dialog_menu: About to execute dialog command: ${cmd[*]}"
    "${cmd[@]}" "${options[@]}" 2> "$dialog_selected"
    dialog_exit_code=$?
    selected_option=$(< "$dialog_selected")
    log_debug "dialog_menu: Dialog completed, exit code: $dialog_exit_code, selected: $selected_option"
    log_debug "dialog_menu: Showing 'Please wait...' info box"
    dialog --infobox "Please wait..." 5 20 &
    # Give up to 0.1 second to suck out up to 10 000 bytes from keyboard buffer:
    read -t 0.2 -n 10000 discard
    log_debug "dialog_menu: Finished dialog_menu function"
}

# Show VM/CT conflict dialog and get user decision
resolve_vmct_conflict() {
    local vmid="$1"
    local type="$2"
    local config_path="$3"
    
    if [ -z "$vmid" ] || [ -z "$type" ] || [ -z "$config_path" ]; then
        log_error "resolve_vmct_conflict: Missing required parameters"
        return 1
    fi
    
    # Extract storage name from config path
    local storage_name
    storage_name="$(echo "$config_path" | awk -F'/' '{print $(NF-3)}')"
    
    log_info "Resolving VM/CT conflict for ID $vmid ($type) with storage $storage_name"
    
    local vm_type_upper=$(upper "$type")
    local conflict_msg="$vm_type_upper ID $vmid already exists in PVE repository!\n\n"
    conflict_msg+="Choose an option:\n"
    conflict_msg+="- Cancel: Skip this $vm_type_upper restoration\n"
    conflict_msg+="- New ID: Provide a different ID for restoration"
    
    dialog --keep-tite --title "$vm_type_upper Conflict Detected" \
           --yes-label "New ID" --no-label "Cancel" \
           --yesno "$conflict_msg" 12 70
    
    local choice=$?
    
    
    if [ $choice -eq 0 ]; then
        # User chose "New ID", prompt for input
        log_info "User chose New ID - prompting for input"
        if get_new_vmct_id "$vmid" "$type" "$storage_name"; then
            log_info "User provided new ID: $new_vmct_id"
            return 0
        else
            return 1
        fi
    else
        # User chose "Cancel"
        return 1
    fi
}

# Get new VM/CT ID from user with validation
get_new_vmct_id() {
    local original_id="$1"
    local type="$2"
    local storage_name="$3"
    local input_file=$(mktemp)
    
    log_info "Prompting user for new $(upper "$type") ID for $type $original_id (storage: $storage_name)"
    
    while true; do
        dialog --keep-tite --title "Enter New $(upper "$type") ID" \
               --ok-label "OK" --cancel-label "Cancel" \
               --inputbox "Enter new ID for $type $original_id:" 8 50 2> "$input_file"
        
        local exit_code=$?
        local user_input=$(cat "$input_file")
        
        
        if [ $exit_code -ne 0 ]; then
            # User cancelled
            rm -f "$input_file"
            return 1
        fi
        
        # Validate input
        if [[ ! "$user_input" =~ ^[0-9]+$ ]]; then
            dialog --msgbox "Invalid input. Please enter a numeric ID." 8 40
            continue
        fi
        
        # Check if ID is already in use
        if check_vmct_exists "$user_input"; then
            dialog --msgbox "ID $user_input is already in use. Please choose another ID." 8 50
            continue
        fi
        
        # Check if the cloned storage is already in use by another VM/CT
        CONFLICTING_VMCT_ID=""  # Reset global variable
        if check_cloned_storage_in_use "$storage_name" "$original_id"; then
            local conflicting_type=$(vm_type "$CONFLICTING_VMCT_ID")
            local conflicting_type_upper=$(upper "$conflicting_type")
            local error_msg="Storage '$storage_name' is already in use by $conflicting_type_upper $CONFLICTING_VMCT_ID.\n\n"
            error_msg+="Each cloned storage can only be used by one $conflicting_type_upper at a time.\n"
            error_msg+="Please move disk of $conflicting_type_upper $CONFLICTING_VMCT_ID to not cloned storage or use a different cloned storage."
            dialog --msgbox "$error_msg" 12 70
            rm -f "$input_file"
            return 1  # Return failure since storage is not available
        fi
        
        # Check if the cloned storage has been modified after snapshot creation (staleness check)
        STALE_STORAGE_NAME=""  # Reset global variables
        STALE_SNAPSHOT_DATE=""
        STALE_DISK_DATE=""
        if check_cloned_storage_staleness "$storage_name" "$original_id"; then
            local warning_msg="WARNING: Cloned storage has been modified!\n\n"
            warning_msg+="The cloned storage '$STALE_STORAGE_NAME' was created from a snapshot taken on $STALE_SNAPSHOT_DATE, "
            warning_msg+="but virtual disks in this storage have been modified on $STALE_DISK_DATE.\n\n"
            warning_msg+="This means the cloned storage no longer contains the original snapshot state and may "
            warning_msg+="have been used for previous VM restorations.\n\n"
            warning_msg+="RECOMMENDATION:\n"
            warning_msg+="Use \"Add cloned NFS storage from $PRODUCT snapshot\" function to create a fresh "
            warning_msg+="cloned storage from the original snapshot, then retry the restore operation.\n\n"
            warning_msg+="Do you want to continue anyway? (NOT RECOMMENDED)"
            
            dialog --keep-tite --title "Storage Staleness Warning" \
                   --yes-label "Continue (NOT RECOMMENDED)" --no-label "Cancel" \
                   --yesno "$warning_msg" 20 80
            
            if [ $? -ne 0 ]; then
                # User chose to cancel
                rm -f "$input_file"
                return 1
            fi
            
            log_info "User chose to continue with stale cloned storage '$storage_name' despite warning"
        fi
        
        # Valid new ID provided and storage is available
        new_vmct_id="$user_input"
        rm -f "$input_file"
        log_info "User provided new $(upper "$type") ID: $new_vmct_id for original ID: $original_id"
        return 0
    done
}

# Check if a cloned storage is already in use by any VM/CT
check_cloned_storage_in_use() {
    local storage_name="$1"
    local original_vmid="$2"
    
    if [ -z "$storage_name" ]; then
        log_error "check_cloned_storage_in_use: Missing storage_name parameter"
        return 1
    fi
    
    log_debug "Checking if cloned storage '$storage_name' is already in use (excluding VMID: $original_vmid)"
    
    # Check VM configs directly for the storage name
    for conf_file in /etc/pve/nodes/*/qemu-server/*.conf; do
        [[ -f "$conf_file" ]] || continue
        
        local vmid=$(basename "$conf_file" .conf)
        
        # Skip the original VM ID (it's being restored/replaced)
        if [[ "$vmid" == "$original_vmid" ]]; then
            log_debug "Skipping original VM ID $vmid"
            continue
        fi
        
        # Check if this VM config contains the storage name
        if grep -q ": $storage_name:" "$conf_file" 2>/dev/null; then
            log_info "Found VM $vmid already using storage '$storage_name'"
            # Store the conflicting VM ID for error message
            CONFLICTING_VMCT_ID="$vmid"
            return 0  # Storage is in use
        fi
    done
    
    # Check CT configs directly for the storage name
    for conf_file in /etc/pve/nodes/*/lxc/*.conf; do
        [[ -f "$conf_file" ]] || continue
        
        local ctid=$(basename "$conf_file" .conf)
        
        # Skip the original CT ID (it's being restored/replaced)
        if [[ "$ctid" == "$original_vmid" ]]; then
            log_debug "Skipping original CT ID $ctid"
            continue
        fi
        
        # Check if this CT config contains the storage name
        if grep -q ": $storage_name:" "$conf_file" 2>/dev/null; then
            log_info "Found CT $ctid already using storage '$storage_name'"
            # Store the conflicting VM ID for error message
            CONFLICTING_VMCT_ID="$ctid"
            return 0  # Storage is in use
        fi
    done
    
    log_debug "Storage '$storage_name' is not in use by any other VM/CT"
    return 1  # Storage not in use
}

# Extract snapshot creation timestamp from cloned storage name
# Input: storage name like "datastore00_clone_2025-08-06-1822"
# Output: Unix timestamp of snapshot creation
extract_snapshot_timestamp_from_clone_name() {
    local storage_name="$1"
    
    if [ -z "$storage_name" ]; then
        log_error "extract_snapshot_timestamp_from_clone_name: Missing storage_name parameter"
        return 1
    fi
    
    log_debug "Extracting snapshot timestamp from cloned storage name: '$storage_name'"
    
    # Check if this is a cloned storage (contains "_clone_")
    if [[ ! "$storage_name" =~ _clone_ ]]; then
        log_error "extract_snapshot_timestamp_from_clone_name: Not a cloned storage name: '$storage_name'"
        return 1
    fi
    
    # Extract the timestamp part after "_clone_"
    # Format: datastore00_clone_2025-08-06-1822 -> 2025-08-06-1822
    local timestamp_part="${storage_name##*_clone_}"
    
    if [ -z "$timestamp_part" ]; then
        log_error "extract_snapshot_timestamp_from_clone_name: Could not extract timestamp from '$storage_name'"
        return 1
    fi
    
    log_debug "Extracted timestamp part: '$timestamp_part'"
    
    # Parse the formatted timestamp: 2025-08-06-1822 -> 2025-08-06 18:22
    if [[ "$timestamp_part" =~ ^([0-9]{4})-([0-9]{2})-([0-9]{2})-([0-9]{4})$ ]]; then
        local year="${BASH_REMATCH[1]}"
        local month="${BASH_REMATCH[2]}"
        local day="${BASH_REMATCH[3]}"
        local time="${BASH_REMATCH[4]}"
        
        # Split time: 1822 -> 18:22
        local hour="${time:0:2}"
        local minute="${time:2:2}"
        
        local formatted_date="${year}-${month}-${day} ${hour}:${minute}"
        log_debug "Parsed date: '$formatted_date'"
        
        # Convert to Unix timestamp
        local unix_timestamp
        unix_timestamp=$(date -d "$formatted_date" +%s 2>/dev/null)
        
        if [ $? -eq 0 ] && [ -n "$unix_timestamp" ]; then
            log_debug "Converted to Unix timestamp: $unix_timestamp"
            echo "$unix_timestamp"
            return 0
        else
            log_error "extract_snapshot_timestamp_from_clone_name: Failed to convert '$formatted_date' to Unix timestamp"
            return 1
        fi
    else
        log_error "extract_snapshot_timestamp_from_clone_name: Invalid timestamp format in '$timestamp_part'"
        return 1
    fi
}

# Get most recent virtual disk modification time from cloned storage
# Input: storage path like "/mnt/pve/datastore00_clone_2025-08-06-1822/images/104/"
# Output: Unix timestamp of most recent virtual disk modification
get_most_recent_disk_modification_time() {
    local storage_path="$1"
    
    if [ -z "$storage_path" ]; then
        log_error "get_most_recent_disk_modification_time: Missing storage_path parameter"
        return 1
    fi
    
    log_debug "Getting most recent disk modification time from: '$storage_path'"
    
    # Check if storage path exists
    if [ ! -d "$storage_path" ]; then
        log_error "get_most_recent_disk_modification_time: Storage path does not exist: '$storage_path'"
        return 1
    fi
    
    # Get list of files with timestamps, excluding .conf files
    # Format: YYYY-MM-DDTHH:MM:SS filename
    local file_list
    file_list=$(ls -la --time-style=+%Y-%m-%dT%H:%M:%S "$storage_path" 2>/dev/null | grep -v '\.conf$' | awk 'NR>1 && !/^d/ && !/^total/ {print $6, $7}' | grep -E '\.(qcow2|raw|vmdk|vdi)$|vm-.*-state-.*\.raw$')
    
    if [ -z "$file_list" ]; then
        log_debug "get_most_recent_disk_modification_time: No virtual disk files found in '$storage_path'"
        # Return 0 timestamp if no disks found (should not prevent validation)
        echo "0"
        return 0
    fi
    
    log_debug "Found virtual disk files:"
    while read -r timestamp filename; do
        log_debug "  $filename: $timestamp"
    done <<< "$file_list"
    
    # Find the most recent timestamp
    local most_recent_timestamp=""
    local most_recent_unix=0
    
    while read -r timestamp filename; do
        if [ -n "$timestamp" ]; then
            # Convert ISO timestamp to Unix timestamp
            local unix_time
            unix_time=$(date -d "$timestamp" +%s 2>/dev/null)
            
            if [ $? -eq 0 ] && [ "$unix_time" -gt "$most_recent_unix" ]; then
                most_recent_unix="$unix_time"
                most_recent_timestamp="$timestamp"
                log_debug "New most recent: $filename ($timestamp -> $unix_time)"
            fi
        fi
    done <<< "$file_list"
    
    if [ "$most_recent_unix" -gt 0 ]; then
        log_debug "Most recent disk modification time: $most_recent_timestamp ($most_recent_unix)"
        echo "$most_recent_unix"
        return 0
    else
        log_error "get_most_recent_disk_modification_time: Failed to parse any timestamps"
        return 1
    fi
}

# Check if cloned storage has been modified after snapshot creation (staleness check)
# Input: storage_name, original_vmid (source VMID from cloned storage)
# Output: 0 if stale (disks newer than snapshot), 1 if fresh or undetermined
check_cloned_storage_staleness() {
    local storage_name="$1"
    local original_vmid="$2"
    
    if [ -z "$storage_name" ] || [ -z "$original_vmid" ]; then
        log_error "check_cloned_storage_staleness: Missing parameters (storage_name: '$storage_name', original_vmid: '$original_vmid')"
        return 1
    fi
    
    log_debug "Checking staleness of cloned storage '$storage_name' for original VM/CT $original_vmid"
    
    # Check if this is a cloned storage
    if [[ ! "$storage_name" =~ _clone_ ]]; then
        log_debug "check_cloned_storage_staleness: '$storage_name' is not a cloned storage, skipping staleness check"
        return 1  # Not stale (not a cloned storage)
    fi
    
    # Get snapshot creation timestamp from storage name
    local snapshot_timestamp
    snapshot_timestamp=$(extract_snapshot_timestamp_from_clone_name "$storage_name")
    
    if [ $? -ne 0 ] || [ -z "$snapshot_timestamp" ]; then
        log_error "check_cloned_storage_staleness: Failed to extract snapshot timestamp from '$storage_name'"
        return 1  # Allow continuation if we can't determine staleness
    fi
    
    # Get storage mount path
    local storage_path
    storage_path=$(grep -A5 "^nfs:[[:space:]]*$storage_name" /etc/pve/storage.cfg | grep "^[[:space:]]*path" | awk '{print $2}')
    
    if [ -z "$storage_path" ]; then
        log_error "check_cloned_storage_staleness: Could not find storage path for '$storage_name'"
        return 1  # Allow continuation if we can't find the path
    fi
    
    # Construct full path to VM/CT directory
    local vm_storage_path="${storage_path}/images/${original_vmid}/"
    log_debug "Checking virtual disk files in: '$vm_storage_path'"
    
    # Get most recent disk modification time
    local disk_timestamp
    disk_timestamp=$(get_most_recent_disk_modification_time "$vm_storage_path")
    
    if [ $? -ne 0 ]; then
        log_error "check_cloned_storage_staleness: Failed to get disk modification time from '$vm_storage_path'"
        return 1  # Allow continuation if we can't check disk times
    fi
    
    # Skip check if no disks found
    if [ "$disk_timestamp" -eq 0 ]; then
        log_debug "check_cloned_storage_staleness: No virtual disks found, skipping staleness check"
        return 1  # Not stale (no disks to check)
    fi
    
    # Compare timestamps
    log_debug "Snapshot timestamp: $snapshot_timestamp ($(date -d "@$snapshot_timestamp" 2>/dev/null || echo "invalid"))"
    log_debug "Most recent disk timestamp: $disk_timestamp ($(date -d "@$disk_timestamp" 2>/dev/null || echo "invalid"))"
    
    if [ "$disk_timestamp" -gt "$snapshot_timestamp" ]; then
        local snapshot_date=$(date -d "@$snapshot_timestamp" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || echo "unknown")
        local disk_date=$(date -d "@$disk_timestamp" '+%Y-%m-%d %H:%M:%S' 2>/dev/null || echo "unknown")
        
        log_info "STALENESS DETECTED: Storage '$storage_name' was created from snapshot on $snapshot_date, but disks were modified on $disk_date"
        
        # Store information for user warning (global variables)
        STALE_STORAGE_NAME="$storage_name"
        STALE_SNAPSHOT_DATE="$snapshot_date"
        STALE_DISK_DATE="$disk_date"
        
        return 0  # Storage is stale
    else
        log_debug "check_cloned_storage_staleness: Storage '$storage_name' is fresh (disks not modified after snapshot)"
        return 1  # Storage is fresh
    fi
}

# Select host for conflicting VM/CT restoration
select_host_for_conflict() {
    local vmid="$1"
    local type="$2"
    local line hosts=() menu_items=()
    
    if [ -z "$vmid" ] || [ -z "$type" ]; then
        log_error "select_host_for_conflict: Missing required parameters"
        return 1
    fi
    
    log_info "Selecting host for conflicting $(upper "$type") ID $vmid ($type)"
    
    # Get available hosts
    while IFS= read -r line; do
        hosts+=("$line")
    done < <(get_ha_nodes)
    
    if [ "${#hosts[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE hosts found." 8 40
        return 1
    fi
    
    # Build menu items
    local i
    for ((i=0; i<${#hosts[@]}; i++)); do
        menu_items+=("$i" "${hosts[i]}")
    done
    
    local dialog_selected=$(mktemp)
    dialog --keep-tite --title "Select Target Host for $(upper "$type") $vmid" \
           --ok-label "Select" --cancel-label "Cancel" \
           --menu "Choose PVE host to restore $(upper "$type") $vmid ($type) to:" 15 60 8 \
           "${menu_items[@]}" 2> "$dialog_selected"
    
    local exit_code=$?
    local selected_index=$(cat "$dialog_selected")
    rm -f "$dialog_selected"
    
    if [ $exit_code -eq 0 ] && [ -n "$selected_index" ]; then
        conflict_selected_host="${hosts[selected_index]}"
        log_info "User selected host: $conflict_selected_host for $(upper "$type") $vmid"
        return 0
    else
        return 1
    fi
}

# NFS TUNING FUNCTIONS

get_nfs_storages() {
    grep -Po '^nfs:\s*\K\S+' /etc/pve/storage.cfg 2>/dev/null || true
}

get_current_nconnect() {
    local storage="$1"
    local nconnect_val
    nconnect_val=$(awk -v storage="$storage" '
        /^nfs:/ && $2 == storage {
            found = 1
            next
        }
        found && /^[a-z]+:/ {
            found = 0
            next
        }
        found && /options/ {
            for(i=2; i<=NF; i++) {
                if($i ~ /nconnect=[0-9]+/) {
                    gsub(/.*nconnect=/, "", $i)
                    gsub(/,.*/, "", $i)
                    print $i
                    exit
                }
            }
        }
    ' /etc/pve/storage.cfg)
    echo "${nconnect_val:-1}"
}

get_mounted_nconnect() {
    local storage="$1"
    local current_nconnect
    local mount_output
    
    log_debug "Getting mounted nconnect for storage: $storage"
    mount_output=$(mount | grep nfs | grep "$storage")
    log_debug "Mount output for $storage: $mount_output"
    
    current_nconnect=$(echo "$mount_output" | grep -Eo "nconnect=[0-9]+" | cut -d= -f2)
    log_debug "Extracted nconnect value for $storage: ${current_nconnect:-1}"
    
    echo "${current_nconnect:-1}"
}

get_current_sync() {
    local storage="$1"
    local sync_val
    
    log_debug "Getting current sync option for storage: $storage"
    
    # Check storage.cfg for sync/async option in options field
    sync_val=$(awk -v storage="$storage" '
        $1 == "nfs:" && $2 == storage { in_block = 1; next }
        in_block && /^[[:space:]]*options[[:space:]]+/ {
            options_line = $0
            # Look for sync or async in the options line
            if (match(options_line, /sync/)) {
                if (match(options_line, /async/)) {
                    print "async"
                } else {
                    print "sync"
                }
            } else {
                print "sync"  # Default to sync for HA reliability
            }
            exit
        }
        /^nfs:/ && $2 != storage { in_block = 0 }
    ' /etc/pve/storage.cfg)
    
    log_debug "Extracted sync value for $storage: ${sync_val:-sync}"
    echo "${sync_val:-sync}"
}

get_mounted_sync() {
    local storage="$1"
    local current_sync
    local mount_output
    
    log_debug "Getting mounted sync option for storage: $storage"
    mount_output=$(mount | grep nfs | grep "$storage")
    log_debug "Mount output for $storage: $mount_output"
    
    # Check if sync is present in mount options
    if echo "$mount_output" | grep -q ",sync,\|,sync "; then
        current_sync="sync"
    else
        # NFS defaults to async if not specified, but we treat missing option as sync for HA
        current_sync="async"
    fi
    
    log_debug "Extracted sync value for $storage: $current_sync"
    echo "$current_sync"
}

nfs_tuning_step1_storage_selection() {
    local dialog_selected=$(mktemp)
    
    local nfs_storages
    nfs_storages=$(get_nfs_storages)
    
    if [[ -z "$nfs_storages" ]]; then
        dialog --title "Error" --msgbox "No NFS storage found in configuration" 8 50
        rm -f "$dialog_selected"
        return 1
    fi
    
    local dialog_args=()
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            local config_nconnect
            local mounted_nconnect
            local config_sync
            local mounted_sync
            config_nconnect=$(get_current_nconnect "$storage")
            mounted_nconnect=$(get_mounted_nconnect "$storage")
            config_sync=$(get_current_sync "$storage")
            mounted_sync=$(get_mounted_sync "$storage")
            dialog_args+=("$storage" "nconnect cfg=$(printf '%2s' "$config_nconnect") mnt=$(printf '%2s' "$mounted_nconnect") sync: cfg=$config_sync mnt=$mounted_sync" "on")
        fi
    done <<< "$nfs_storages"
    
    if [[ ${#dialog_args[@]} -eq 0 ]]; then
        dialog --title "Error" --msgbox "No valid NFS storage found" 8 40
        rm -f "$dialog_selected"
        return 1
    fi
    
    cmd=(dialog --keep-tite --title "Step 1/3: Storage Selection"
         --ok-label "Next" --cancel-label "Back"
         --checklist "Select storage to update (use SPACE to select):" 20 100 10)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select at least one storage." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_storages=$(echo "$selected_option" | tr -d '"' | tr ' ' '\n')
            log_info "Selected storages: $(echo "$selected_nfs_storages" | tr '\n' ' ')"
            return 0  # Proceed to next step
            ;;
        1)  # Cancel button (Back)
            return 1
            ;;
    esac
}

nfs_tuning_step2_nconnect_selection() {
    local dialog_selected=$(mktemp)
    
    local dialog_args=()
    
    for i in {1..16}; do
        if [[ $i -eq 4 ]]; then
            dialog_args+=("$i" "nconnect = $i (default)" "on")
        else
            dialog_args+=("$i" "nconnect = $i" "off")
        fi
    done
    
    cmd=(dialog --keep-tite --title "Step 2/3: nconnect Value"
         --ok-label "Next" --cancel-label "Back"
         --radiolist "Choose new nconnect value:" 20 100 10)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select an nconnect value." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_nconnect="$selected_option"
            log_info "Selected nconnect value: $selected_nfs_nconnect"
            return 0  # Proceed to next step
            ;;
        1)  # Cancel button (Back)
            return 2  # Go back to previous step
            ;;
    esac
}

nfs_tuning_step3_sync_selection() {
    local dialog_selected=$(mktemp)
    
    local dialog_args=()
    dialog_args+=("sync" "sync (default - guaranteed write completion for HA)" "on")
    dialog_args+=("async" "async (better performance - less reliable)" "off")
    
    cmd=(dialog --keep-tite --title "Step 3/3: Sync Option"
         --ok-label "Next" --cancel-label "Back"
         --radiolist "Choose sync option for NFS mounts:" 15 70 5)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select a sync option." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_sync="$selected_option"
            log_info "Selected sync option: $selected_nfs_sync"
            return 0  # Proceed to execute
            ;;
        1)  # Cancel button (Back)
            return 2  # Go back to previous step
            ;;
    esac
}

execute_nfs_tuning_update() {
    local temp_dir="/tmp/nfs-tuning-$$"
    local results_file="$temp_dir/results.txt"
    
    mkdir -p "$temp_dir"
    
    log_info "Starting NFS tuning update process"
    echo "Starting NFS tuning update process" > "$results_file"
    echo "Selected storages: $(echo "$selected_nfs_storages" | tr '\n' ' ')" >> "$results_file"
    echo "Selected nconnect value: $selected_nfs_nconnect" >> "$results_file"
    echo "Selected sync option: $selected_nfs_sync" >> "$results_file"
    echo "" >> "$results_file"
    
    # Count total storages for progress calculation
    local storage_count=0
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            storage_count=$((storage_count + 1))
        fi
    done <<< "$selected_nfs_storages"
    
    # Progress tracking
    local current_step=0
    local total_steps=$((storage_count * 2 + storage_count + 1))  # config+umount, mount, verify, +1 for stabilize
    
    # Create progress pipe
    local progress_pipe
    progress_pipe=$(mktemp -u)
    mkfifo "$progress_pipe"
    
    # Start progress dialog in background
    dialog --title "Updating NFS Tuning Settings" \
           --gauge "Initializing..." 8 60 0 < "$progress_pipe" &
    local dialog_pid=$!
    
    # Function to update progress
    update_progress() {
        local message="$1"
        local percentage=$((current_step * 100 / total_steps))
        echo "$percentage"
        echo "XXX"
        echo "$message"
        echo "XXX"
    }
    
    # Redirect to progress pipe
    exec 3>"$progress_pipe"
    
    # Step 1: Update storage config and umount for each storage
    log_info "Step 1: Updating storage configurations and unmounting"
    echo "Step 1: Updating storage configurations and unmounting" >> "$results_file"
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            # Update storage configuration
            current_step=$((current_step + 1))
            update_progress "Updating $storage configuration..." >&3
            
            log_info "Updating $storage configuration with nconnect=$selected_nfs_nconnect, sync=$selected_nfs_sync"
            
            # Build options string based on selected sync option
            local options_string="nconnect=$selected_nfs_nconnect"
            if [[ "$selected_nfs_sync" == "sync" ]]; then
                options_string="${options_string},sync"
            else
                options_string="${options_string},async"
            fi
            
            log_debug "Executing: pvesm set $storage --options $options_string"
            pvesm set "$storage" --options "$options_string" 2>/dev/null || true
            
            # Attempt to unmount
            current_step=$((current_step + 1))
            update_progress "Unmounting $storage..." >&3
            
            log_info "Attempting to unmount $storage"
            local mount_path="/mnt/pve/$storage"
            log_debug "Executing: umount $mount_path"
            umount "$mount_path" 2>/dev/null || true
        fi
    done <<< "$selected_nfs_storages"
    
    # Step 2: Mount all storages
    log_info "Step 2: Mounting all storages"
    echo "Step 2: Mounting all storages" >> "$results_file"
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            current_step=$((current_step + 1))
            update_progress "Mounting $storage..." >&3
            
            log_info "Mounting $storage"
            local escaped
            escaped=$(systemd-escape "$storage")
            local unit="mnt-pve-${escaped}.mount"
            log_debug "Executing: systemctl start $unit"
            systemctl start "$unit" 2>/dev/null || true
        fi
    done <<< "$selected_nfs_storages"
    
    # Step 3: Wait for mounts to stabilize
    current_step=$((current_step + 1))
    update_progress "Waiting for mounts to stabilize..." >&3
    
    log_info "Step 3: Waiting for mounts to stabilize"
    log_debug "Starting 5-second wait for mounts to stabilize"
    echo "Step 3: Waiting for mounts to stabilize, then verifying results" >> "$results_file"
    sleep 5
    log_debug "Mount stabilization wait completed"
    
    # Step 4: Verify results
    log_info "Step 4: Verifying results"
    echo "============================" >> "$results_file"
    
    local has_failures=false
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            update_progress "Verifying $storage..." >&3
            
            local mounted_nconnect
            local mounted_sync
            local attempts=0
            local max_attempts=10
            
            log_debug "Starting verification for $storage (expected: nconnect=$selected_nfs_nconnect, sync=$selected_nfs_sync)"
            
            # Try multiple times to get the mount info, as it may take time to update
            while [[ $attempts -lt $max_attempts ]]; do
                attempts=$((attempts + 1))
                log_debug "Verification attempt $attempts/$max_attempts for $storage"
                
                mounted_nconnect=$(get_mounted_nconnect "$storage")
                mounted_sync=$(get_mounted_sync "$storage")
                log_debug "Got values for $storage: nconnect=$mounted_nconnect, sync=$mounted_sync"
                
                # Check if we got the expected values
                if [[ "$mounted_nconnect" == "$selected_nfs_nconnect" && "$mounted_sync" == "$selected_nfs_sync" ]]; then
                    log_debug "Expected values found for $storage, stopping retries"
                    break
                fi
                
                if [[ $attempts -lt $max_attempts ]]; then
                    log_debug "Sleeping 2 seconds before next attempt for $storage"
                    sleep 2
                fi
            done
            
            log_debug "Final verification for $storage: expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
            
            if [[ "$mounted_nconnect" == "$selected_nfs_nconnect" && "$mounted_sync" == "$selected_nfs_sync" ]]; then
                log_info "$storage: SUCCESS - nconnect=$mounted_nconnect, sync=$mounted_sync"
                log_debug "RESULT: $storage SUCCESS"
                echo "$storage: SUCCESS - nconnect=$mounted_nconnect, sync=$mounted_sync" >> "$results_file"
            else
                log_info "$storage: FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
                log_debug "RESULT: $storage FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
                echo "$storage: FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync" >> "$results_file"
                has_failures=true
            fi
        fi
    done <<< "$selected_nfs_storages"
    
    # Complete progress
    echo "100" >&3
    echo "XXX" >&3
    echo "Update completed!" >&3
    echo "XXX" >&3
    
    # Close progress pipe and wait for dialog to finish
    exec 3>&-
    sleep 1
    kill "$dialog_pid" 2>/dev/null || true
    wait "$dialog_pid" 2>/dev/null || true
    
    # Clean up
    rm -f "$progress_pipe"
    
    # Add failure note if needed
    if [[ "$has_failures" == "true" ]]; then
        echo "" >> "$results_file"
        echo "NOTE: To apply and mount NFS with new nconnect and sync option values," >> "$results_file"
        echo " all VMs must be stopped or their disks migrated to other storage." >> "$results_file"
    fi
    
    dialog --exit-label "Continue" --title "Operation Summary" --textbox "$results_file" 20 80

    # Clean up temp directory
    rm -rf "$temp_dir"
}

nfs_tuning_wizard() {
    local nfs_current_step=1
    local nfs_total_steps=3
    
    # Clear previous selections
    selected_nfs_storages=""
    selected_nfs_nconnect=""
    selected_nfs_sync=""
    
    while true; do
        case $nfs_current_step in
            1)
                nfs_tuning_step1_storage_selection
                case $? in
                    0)  # Next - advance to step 2
                        nfs_current_step=2
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Stay on same step
                        ;;
                esac
                ;;
            2)
                nfs_tuning_step2_nconnect_selection
                case $? in
                    0)  # Next - advance to step 3
                        nfs_current_step=3
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Back - return to step 1
                        nfs_current_step=1
                        ;;
                esac
                ;;
            3)
                nfs_tuning_step3_sync_selection
                case $? in
                    0)  # Next - execute and return to main menu
                        execute_nfs_tuning_update
                        return 0
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Back - return to step 2
                        nfs_current_step=2
                        ;;
                esac
                ;;
        esac
    done
}

# POST KNOWN ISSUES FIXES AND TUNING

# Check ZFS import scan service status
check_zfs_import_scan_service() {
    local service_status
    service_status=$(systemctl is-enabled zfs-import-scan.service 2>/dev/null)
    
    if [ "$service_status" = "disabled" ]; then
        return 1  # Already disabled
    else
        return 0  # Enabled or other status
    fi
}

# Fix ZFS import scan service
fix_zfs_import_scan_service() {
    log_info "Disabling zfs-import-scan.service"
    
    if systemctl disable zfs-import-scan.service >> "$LOG_FILE" 2>&1; then
        log_info "Successfully disabled zfs-import-scan.service"
        return 0
    else
        log_error "Failed to disable zfs-import-scan.service"
        return 1
    fi
}

# Check HA policy shutdown setting
check_ha_policy_shutdown() {
    local ha_policy
    
    # Check only /etc/pve/datacenter.cfg for HA configuration
    if [ -f "/etc/pve/datacenter.cfg" ]; then
        ha_policy=$(grep -E "^ha:" /etc/pve/datacenter.cfg 2>/dev/null | grep -oE "shutdown_policy=[^, ]*" | cut -d= -f2)
    fi
    
    # If no explicit setting in datacenter.cfg, default to conditional
    if [ -z "$ha_policy" ]; then
        ha_policy="conditional"
    fi
    
    log_info "Current HA shutdown policy: $ha_policy"
    
    if [ "$ha_policy" = "migrate" ]; then
        return 1  # Already set to migrate
    else
        return 0  # Not set to migrate (includes missing line = default conditional)
    fi
}

# Fix HA policy shutdown setting
fix_ha_policy_shutdown() {
    log_info "Setting HA shutdown policy to migrate"
    
    local datacenter_cfg="/etc/pve/datacenter.cfg"
    
    if [ -f "$datacenter_cfg" ]; then
        # Check if ha section exists
        if grep -q "^ha:" "$datacenter_cfg"; then
            # Update existing ha section
            sed -i 's/^ha:.*$/ha: shutdown_policy=migrate/' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
        else
            # Add ha section
            echo "ha: shutdown_policy=migrate" >> "$datacenter_cfg" 2>> "$LOG_FILE"
        fi
        
        if [ $? -eq 0 ]; then
            log_info "Successfully set HA shutdown policy to migrate in datacenter.cfg"
            return 0
        else
            log_error "Failed to set HA shutdown policy to migrate"
            return 1
        fi
    else
        log_error "datacenter.cfg not found"
        return 1
    fi
}

# Restore HA policy shutdown setting to conditional (default)
restore_ha_policy_shutdown() {
    log_info "Restoring HA shutdown policy to conditional (default)"
    
    local datacenter_cfg="/etc/pve/datacenter.cfg"
    
    if [ -f "$datacenter_cfg" ]; then
        # Check if ha section exists
        if grep -q "^ha:" "$datacenter_cfg"; then
            # Check if it only contains shutdown_policy
            local ha_line=$(grep "^ha:" "$datacenter_cfg")
            if [[ "$ha_line" =~ ^ha:[[:space:]]*shutdown_policy=[^,]*$ ]]; then
                # Only shutdown_policy is set, remove the entire line
                sed -i '/^ha:[[:space:]]*shutdown_policy=/d' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
                log_info "Removed ha: shutdown_policy line from datacenter.cfg (restored to default)"
            else
                # Other HA settings exist, just remove shutdown_policy
                sed -i 's/,\?[[:space:]]*shutdown_policy=[^,]*//g; s/shutdown_policy=[^,]*,\?[[:space:]]*//g' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
                log_info "Removed shutdown_policy from ha line in datacenter.cfg (restored to default)"
            fi
            
            if [ $? -eq 0 ]; then
                log_info "Successfully restored HA shutdown policy to conditional (default) in datacenter.cfg"
                return 0
            else
                log_error "Failed to restore HA shutdown policy to conditional (default)"
                return 1
            fi
        else
            # No ha section exists, already at default
            log_info "No ha section found in datacenter.cfg, already at default (conditional)"
            return 0
        fi
    else
        log_error "datacenter.cfg not found"
        return 1
    fi
}

# Check if hardware watchdog is configured
check_watchdog_module() {
    local pve_ha_config="/etc/default/pve-ha-manager"
    
    if [ ! -f "$pve_ha_config" ]; then
        log_error "pve-ha-manager config file not found: $pve_ha_config"
        return 1
    fi
    
    # Check if WATCHDOG_MODULE line exists and is uncommented
    if grep -q "^WATCHDOG_MODULE=" "$pve_ha_config"; then
        return 1  # Already configured (uncommented)
    else
        return 0  # Not configured (commented or missing)
    fi
}

# Fix hardware watchdog configuration
fix_watchdog_module() {
    log_info "Configuring hardware watchdog (ipmi_watchdog)"
    
    local pve_ha_config="/etc/default/pve-ha-manager"
    
    if [ ! -f "$pve_ha_config" ]; then
        log_error "pve-ha-manager config file not found: $pve_ha_config"
        return 1
    fi
    
    # Check if commented line exists
    if grep -q "^#WATCHDOG_MODULE=" "$pve_ha_config"; then
        # Uncomment the line
        sed -i 's/^#WATCHDOG_MODULE=/WATCHDOG_MODULE=/' "$pve_ha_config" >> "$LOG_FILE" 2>&1
        log_info "Uncommented WATCHDOG_MODULE line in $pve_ha_config"
    else
        # Add the line if it doesn't exist
        echo "WATCHDOG_MODULE=ipmi_watchdog" >> "$pve_ha_config"
        log_info "Added WATCHDOG_MODULE=ipmi_watchdog to $pve_ha_config"
    fi
    
    if [ $? -eq 0 ]; then
        log_info "Successfully configured hardware watchdog (ipmi_watchdog)"
        return 0
    else
        log_error "Failed to configure hardware watchdog"
        return 1
    fi
}

# Check for missing OODP tasks for existing NFS storage
check_missing_oodp_tasks() {
    log_info "Checking for missing OODP tasks"
    
    # Get all regular NFS storage
    local storages=$(get_regular_nfs_storage)
    if [[ -z "$storages" ]]; then
        log_info "No regular NFS storage found"
        return 1
    fi
    
    local missing_count=0
    
    # Check each storage for OODP task
    while IFS='|' read -r storage_name path server _; do
        local ip_addr="${storage_ip_map[$storage_name]}"
        
        if [[ -z "$ip_addr" ]]; then
            log_debug "No IP mapping found for storage: $storage_name"
            continue
        fi
        
        # Extract pool and dataset from path
        local pool_name="Pool-0"  # Default pool
        local dataset_name="${path#/}"  # Remove leading slash
        
        # Check if path contains pool name (format: /pool-name/dataset-name)
        if [[ "$path" =~ ^/([^/]+)/(.+)$ ]]; then
            pool_name="${BASH_REMATCH[1]}"
            dataset_name="${BASH_REMATCH[2]}"
        fi
        
        log_debug "Checking OODP task for $storage_name: $pool_name/$dataset_name on $ip_addr"
        
        # Check if OODP task exists
        if ! check_oodp_task_exists "$pool_name" "$dataset_name" "$ip_addr"; then
            log_info "Missing OODP task for storage: $storage_name ($pool_name/$dataset_name)"
            ((missing_count++))
        fi
    done <<< "$storages"
    
    if [[ $missing_count -gt 0 ]]; then
        log_info "Found $missing_count storage(s) with missing OODP tasks"
        return 0  # Return 0 to indicate fixes are needed
    else
        log_info "All NFS storage have OODP tasks configured"
        return 1  # Return 1 to indicate no fixes needed
    fi
}

# Fix missing OODP tasks for existing NFS storage
fix_missing_oodp_tasks() {
    log_info "Fixing missing OODP tasks"
    
    # Check if REST API credentials are configured
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]] || [[ -z "${rest_api_port}" ]]; then
        log_error "REST API credentials not configured. Please configure them in Setup menu."
        return 1
    fi
    
    # Get all regular NFS storage
    local storages=$(get_regular_nfs_storage)
    if [[ -z "$storages" ]]; then
        log_info "No regular NFS storage found"
        return 1
    fi
    
    local fixed_count=0
    local failed_count=0
    
    # Process each storage
    while IFS='|' read -r storage_name path server _; do
        local ip_addr="${storage_ip_map[$storage_name]}"
        
        if [[ -z "$ip_addr" ]]; then
            log_debug "No IP mapping found for storage: $storage_name"
            continue
        fi
        
        # Extract pool and dataset from path
        local pool_name="Pool-0"  # Default pool
        local dataset_name="${path#/}"  # Remove leading slash
        
        # Check if path contains pool name (format: /pool-name/dataset-name)
        if [[ "$path" =~ ^/([^/]+)/(.+)$ ]]; then
            pool_name="${BASH_REMATCH[1]}"
            dataset_name="${BASH_REMATCH[2]}"
        fi
        
        log_debug "Processing $storage_name: $pool_name/$dataset_name on $ip_addr"
        
        # Check if OODP task exists
        if ! check_oodp_task_exists "$pool_name" "$dataset_name" "$ip_addr"; then
            log_info "Creating OODP task for $storage_name"
            
            # Check if backup destinations are available
            local backup_servers=$(get_backup_dr_servers)
            local destinations_json=""
            
            if [[ -n "$backup_servers" ]]; then
                # Create destinations JSON for backup servers
                local dest_array=()
                while read -r backup_ip; do
                    # Get destination pool for this backup server
                    local dest_pool="Pool-DR-node-a"  # Default destination pool
                    local dest_port="${oodp_dst_port:-40000}"
                    local dest_dataset="${dataset_name}-backup"
                    
                    dest_array+=("{\"plan\":\"$oodp_dst_default_plan\",\"dataset\":\"$backup_ip:$dest_port/$dest_pool/$dest_dataset\"}")
                done <<< "$backup_servers"
                
                if [[ ${#dest_array[@]} -gt 0 ]]; then
                    destinations_json="[$(IFS=,; echo "${dest_array[*]}")]"
                    log_debug "Created destinations JSON for ${#dest_array[@]} backup server(s)"
                fi
            fi
            
            # Create OODP task
            if create_oodp_task "$pool_name" "$dataset_name" "$storage_name" "$ip_addr" "$destinations_json"; then
                log_info "Successfully created OODP task for $storage_name"
                ((fixed_count++))
            else
                log_error "Failed to create OODP task for $storage_name"
                ((failed_count++))
            fi
        else
            log_debug "OODP task already exists for $storage_name"
        fi
    done <<< "$storages"
    
    if [[ $fixed_count -gt 0 ]]; then
        log_info "Successfully created $fixed_count OODP task(s)"
    fi
    
    if [[ $failed_count -gt 0 ]]; then
        log_error "Failed to create $failed_count OODP task(s)"
        return 1
    fi
    
    return 0
}

# Main function for post known issues fixes and tuning
post_known_issues_fixes() {
    local dialog_selected=$(mktemp)
    local fixes_applied=0
    
    # ZFS Import Scan Service Fix
    if check_zfs_import_scan_service; then
        # Service is enabled, offer to disable it
        dialog --title "ZFS Import Scan Service Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "Proxmox VE Host Attempts to Import $VENDOR $PRODUCT ZFS Pool After Reboot\n\nDescription:\nWhen running $VENDOR $PRODUCT as a virtual machine on a Proxmox VE host, the host system may attempt to import a ZFS pool that belongs to the $VENDOR $PRODUCT VM. This typically occurs if the $VENDOR $PRODUCT VM accesses its data disks via PCI passthrough.\n\nObserved Behavior:\nAfter rebooting the Proxmox VE host, the system tries to auto-import the ZFS pool from the passthrough disks, which may lead to errors.\n\nWorkaround:\nDisable the zfs-import-scan.service on the Proxmox VE host to prevent automatic import of ZFS pools that belong to the $VENDOR $PRODUCT VM.\n\nDo you want to apply this fix?" 22 80
        
        if [ $? -eq 0 ]; then
            if fix_zfs_import_scan_service; then
                dialog --msgbox "ZFS Import Scan Service Fix Applied Successfully!\n\nThe zfs-import-scan.service has been disabled." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply ZFS Import Scan Service fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "ZFS Import Scan Service Fix Already Applied\n\nThe zfs-import-scan.service is already disabled." 10 60
    fi
    
    # HA Policy Shutdown Fix
    if check_ha_policy_shutdown; then
        # Policy is not set to migrate, offer to fix it
        dialog --title "HA Policy Shutdown Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "How is the HA policy for a node shutdown configured?\n\nDescription:\nThe shutdown policy can be configured in the Web UI (Datacenter → Options → HA Settings). By default, the setting is 'Conditional' for backward compatibility.\n\nTo achieve the expected behavior—starting VMs/CTs on another cluster host—you must change the setting to 'Migrate'.\n\nThis fix will set the HA shutdown policy to 'Migrate' to ensure VMs/CTs are properly migrated to other nodes during shutdown.\n\nDo you want to apply this fix?" 18 80
        
        if [ $? -eq 0 ]; then
            if fix_ha_policy_shutdown; then
                dialog --msgbox "HA Policy Shutdown Fix Applied Successfully!\n\nThe HA shutdown policy has been set to 'Migrate'." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply HA Policy Shutdown fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "HA Policy Shutdown Fix Already Applied\n\nThe HA shutdown policy is already set to 'Migrate'." 10 60
    fi
    
    # Hardware Watchdog Configuration Fix
    if check_watchdog_module; then
        # Hardware watchdog is not configured, offer to configure it
        dialog --title "Hardware Watchdog Configuration Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "Hardware Watchdog Configuration\n\nDescription:\nHardware watchdogs are preferred for reliability, as they operate independently of the operating system. Software watchdogs, which are kernel-based, act as a fallback when hardware options are unavailable.\n\nTo configure the watchdog module, edit the configuration file:\n /etc/default/pve-ha-manager\n\nToggle the setting by (un)commenting the line:\n#WATCHDOG_MODULE=ipmi_watchdog    Commented => PVE keeps the default softdog software watchdog.\nWATCHDOG_MODULE=ipmi_watchdog    Uncommented => watchdog-mux auto-loads the ipmi_watchdog driver, turning on hardware fencing.\n\nRemoving the # activates the specified hardware watchdog, replacing the software-based fallback.\n\nDo you want to apply this fix?" 24 80
        
        if [ $? -eq 0 ]; then
            if fix_watchdog_module; then
                dialog --msgbox "Hardware Watchdog Configuration Fix Applied Successfully!\n\nThe WATCHDOG_MODULE has been configured to use ipmi_watchdog." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply Hardware Watchdog Configuration fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "Hardware Watchdog Configuration Fix Already Applied\n\nThe WATCHDOG_MODULE is already configured." 10 60
    fi
    
    # Missing OODP Tasks Fix
    if check_missing_oodp_tasks; then
        # Missing OODP tasks found, offer to fix them
        dialog --title "Missing OODP Tasks Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "Missing OODP Tasks for NFS Storage\n\nDescription:\nSome NFS storage entries in Proxmox VE do not have corresponding OODP (On/Off-site Data Protection) tasks configured in $VENDOR $PRODUCT. Without OODP tasks, automatic snapshots and replication will not work for these datasets.\n\nThis can happen when:\n- NFS storage was added manually without using pve-tools\n- OODP task creation failed during initial setup\n- OODP tasks were deleted from $PRODUCT\n\nThis fix will automatically create the missing OODP tasks with default snapshot schedules.\n\nDo you want to apply this fix?" 20 80
        
        if [ $? -eq 0 ]; then
            dialog --title "Creating OODP Tasks" --infobox "Creating missing OODP tasks...\nThis may take a moment." 8 60
            
            if fix_missing_oodp_tasks; then
                dialog --msgbox "Missing OODP Tasks Fix Applied Successfully!\n\nOODP tasks have been created for all NFS storage." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to create some OODP tasks.\nCheck the log file for details.\n\nYou may need to:\n1. Verify REST API credentials in Setup menu\n2. Check network connectivity to $PRODUCT\n3. Ensure datasets exist in $PRODUCT" 14 70
            fi
        fi
    else
        dialog --msgbox "OODP Tasks Fix Already Applied\n\nAll NFS storage have OODP tasks configured." 10 60
    fi
    
    # Summary
    if [ $fixes_applied -gt 0 ]; then
        dialog --msgbox "Post Known Issues Fixes Complete!\n\nApplied $fixes_applied fix(es) successfully." 10 50
    else
        dialog --msgbox "Post Known Issues Fixes Complete!\n\nNo fixes were needed or applied." 10 50
    fi
    
    rm -f "$dialog_selected"
    log_info "Post known issues fixes completed - $fixes_applied fixes applied"
}

# AUTO-UPDATE FUNCTIONS

# Ensure jq is installed for JSON parsing
ensure_jq_installed() {
    if ! command -v jq >/dev/null 2>&1; then
        log_info "Installing jq for JSON parsing..."
        if command -v apt-get >/dev/null 2>&1; then
            apt-get update >/dev/null 2>&1 && apt-get install -y jq >/dev/null 2>&1
        elif command -v yum >/dev/null 2>&1; then
            yum install -y jq >/dev/null 2>&1
        elif command -v dnf >/dev/null 2>&1; then
            dnf install -y jq >/dev/null 2>&1
        else
            log_error "Could not install jq: package manager not supported"
            return 1
        fi
        
        if command -v jq >/dev/null 2>&1; then
            log_info "jq installed successfully"
        else
            log_error "Failed to install jq"
            return 1
        fi
    fi
    return 0
}

# Get commit information from GitHub API
get_commit_info() {
    local commit_message=""
    local vendor_lower
    local api_url
    
    # Convert VENDOR to lowercase for URL
    vendor_lower=$(lower "$VENDOR")
    api_url="https://api.github.com/repos/${vendor_lower}/pve-tools/commits/main"
    
    log_debug "Getting commit info from: $api_url"
    
    # Try to get commit info from GitHub API
    if command -v jq >/dev/null 2>&1; then
        log_debug "Using jq for JSON parsing"
        # Use jq if available
        commit_message=$(curl -s "$api_url" 2>/dev/null | jq -r '.commit.message // empty' 2>/dev/null)
        log_debug "Commit message via jq: '$commit_message'"
    else
        log_debug "Using grep/sed fallback for JSON parsing"
        # Fallback to grep/sed parsing
        commit_message=$(curl -s "$api_url" 2>/dev/null | grep -o '"message"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"message"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1)
        log_debug "Commit message via grep/sed: '$commit_message'"
    fi
    
    if [ -n "$commit_message" ] && [ "$commit_message" != "null" ] && [ "$commit_message" != "empty" ]; then
        # Truncate very long commit messages and clean up formatting
        local formatted_message
        formatted_message=$(echo "$commit_message" | head -c 200 | tr '\n' ' ')
        formatted_message=$(trim_str "$formatted_message")
        log_debug "Formatted commit message: '$formatted_message'"
        echo "$formatted_message"
    else
        log_error "Failed to retrieve commit information from GitHub API: $api_url"
        log_debug "Raw commit_message value: '$commit_message'"
        return 1
    fi
}

# Get all commits since a specific version from GitHub API
get_commits_since_version() {
    local current_version="$1"
    local current_date="$2"
    local vendor_lower
    local api_url
    local commits_json
    local formatted_commits=""
    local commit_count=0
    
    # Convert VENDOR to lowercase for URL
    vendor_lower=$(lower "$VENDOR")
    
    # First, try to fetch recent commits (up to 50)
    api_url="https://api.github.com/repos/${vendor_lower}/pve-tools/commits?sha=main&per_page=50"
    
    log_debug "Fetching commit history from: $api_url"
    log_debug "Current version: $current_version, Release date: $current_date"
    
    # Fetch commits from GitHub API with timeout and rate limit handling
    commits_json=$(curl -s --connect-timeout 10 --max-time 30 "$api_url" 2>/dev/null)
    
    # Check for various failure conditions
    if [ -z "$commits_json" ] || [ "$commits_json" = "null" ]; then
        log_error "Failed to retrieve commit history from GitHub API"
        return 1
    fi
    
    # Check for GitHub API rate limiting
    if echo "$commits_json" | grep -q '"message".*API rate limit exceeded'; then
        log_error "GitHub API rate limit exceeded"
        log_debug "Rate limit response: $commits_json"
        return 1
    fi
    
    # Check for not found or other API errors
    if echo "$commits_json" | grep -q '"message".*Not Found'; then
        log_error "GitHub repository not found or inaccessible"
        return 1
    fi
    
    # Parse commits based on available tools
    if command -v jq >/dev/null 2>&1; then
        log_debug "Using jq to parse commit history"
        
        # First let's see the raw commit data for debugging
        local raw_commits=$(echo "$commits_json" | jq -r '.[] | "\(.commit.author.date | split("T")[0])|\(.commit.message | split("\n")[0])"' 2>/dev/null | head -10)
        log_debug "Raw commits (first 10): $raw_commits"
        
        # Parse each commit and format it
        while IFS= read -r commit_line; do
            # Skip if this line is empty
            [ -z "$commit_line" ] && continue
            
            # Parse the JSON line (format: date|message)
            local commit_date=$(echo "$commit_line" | cut -d'|' -f1)
            local commit_message=$(echo "$commit_line" | cut -d'|' -f2-)
            
            # First check if commit message contains version bump to our current version or older
            if echo "$commit_message" | grep -q "VERSION=$current_version"; then
                log_debug "Found exact version bump to $current_version in commit: $commit_message"
                break
            fi
            
            # Also check for older versions that would be before our current version
            if echo "$commit_message" | grep -q "version.*$current_version\|VERSION.*$current_version"; then
                log_debug "Found version reference to $current_version in commit: $commit_message"
                # Don't break here, check if it's actually setting this version
                if echo "$commit_message" | grep -q "VERSION=$current_version\|version to $current_version"; then
                    log_debug "Confirmed version bump to $current_version, stopping"
                    break
                fi
            fi
            
            # Compare dates to see if this commit is newer than our version
            # For very old versions (0.x where x < 10), be more lenient with date filtering
            local current_ver_major=$(echo "$current_version" | cut -d. -f1)
            local current_ver_minor=$(echo "$current_version" | cut -d. -f2 2>/dev/null || echo "0")
            
            if [ "$current_ver_major" = "0" ] && [ "$current_ver_minor" -lt "10" ]; then
                log_debug "Very old version detected ($current_version), using extended date filtering for commit from $commit_date"
                # For very old versions, only stop if commit is significantly older
                if [ -n "$current_date" ]; then
                    local commit_date_num=$(echo "$commit_date" | tr -d '-')
                    local current_date_num=$(echo "$current_date" | tr -d '-')
                    
                    # Only stop if commit is from before our version date (not same day)
                    if [ "$commit_date_num" -lt "$current_date_num" ]; then
                        log_debug "Found older commit from $commit_date (before $current_date), stopping"
                        break
                    else
                        log_debug "Commit from $commit_date is same day or newer than $current_date, including it"
                    fi
                fi
            elif [ -n "$current_date" ]; then
                # For normal versions, use stricter date filtering but still check same-day commits
                local commit_date_num=$(echo "$commit_date" | tr -d '-')
                local current_date_num=$(echo "$current_date" | tr -d '-')
                
                # If commit is from before our version date, stop
                if [ "$commit_date_num" -lt "$current_date_num" ]; then
                    log_debug "Reached commit from $commit_date (before $current_date), stopping"
                    break
                fi
                
                log_debug "Processing commit from $commit_date: '$commit_message'"
            fi
            
            # Add this commit to our list
            if [ -n "$formatted_commits" ]; then
                formatted_commits+="\n\n"
            fi
            # Trim leading and trailing spaces from commit message
            commit_message=$(trim_str "$commit_message")
            formatted_commits+="$commit_date - $commit_message"
            ((commit_count++))
            
            # Limit to reasonable number of commits for display
            if [ $commit_count -ge 20 ]; then
                formatted_commits+="\n\n... and more commits (showing latest 20)"
                formatted_commits+="\nVisit https://github.com/${vendor_lower}/pve-tools/commits for full history"
                break
            fi
        done < <(echo "$commits_json" | jq -r '.[] | "\(.commit.author.date | split("T")[0])|\(.commit.message | split("\n")[0])"' 2>/dev/null)
        
    else
        log_debug "Using grep/sed fallback for commit parsing"
        
        # Fallback parsing without jq - extract basic commit info
        # This is more limited but functional
        local in_commit=false
        local temp_date=""
        local temp_author=""
        local temp_message=""
        
        while IFS= read -r line; do
            # Look for commit date
            if echo "$line" | grep -q '"date"[[:space:]]*:'; then
                temp_date=$(echo "$line" | sed 's/.*"date"[[:space:]]*:[[:space:]]*"\([^T]*\)T.*/\1/')
            fi
            
            # Look for author name  
            if echo "$line" | grep -q '"name"[[:space:]]*:'; then
                temp_author=$(echo "$line" | sed 's/.*"name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            fi
            
            # Look for commit message
            if echo "$line" | grep -q '"message"[[:space:]]*:'; then
                temp_message=$(echo "$line" | sed 's/.*"message"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
                
                # We have all parts, check if we should include this commit
                if [ -n "$temp_date" ] && [ -n "$temp_message" ]; then
                    # First check if commit message contains version bump to our current version
                    if echo "$temp_message" | grep -q "VERSION=$current_version"; then
                        log_debug "Found exact version bump to $current_version in commit: $temp_message"
                        break
                    fi
                    
                    # Also check for other version references
                    if echo "$temp_message" | grep -q "version.*$current_version\|VERSION.*$current_version"; then
                        log_debug "Found version reference to $current_version in commit: $temp_message"
                        if echo "$temp_message" | grep -q "VERSION=$current_version\|version to $current_version"; then
                            log_debug "Confirmed version bump to $current_version, stopping"
                            break
                        fi
                    fi
                    
                    # Compare dates (updated logic matching jq parsing)
                    local current_ver_major=$(echo "$current_version" | cut -d. -f1)
                    local current_ver_minor=$(echo "$current_version" | cut -d. -f2 2>/dev/null || echo "0")
                    
                    if [ "$current_ver_major" = "0" ] && [ "$current_ver_minor" -lt "10" ]; then
                        log_debug "Very old version detected ($current_version), using extended date filtering for commit from $temp_date"
                        if [ -n "$current_date" ]; then
                            local commit_date_num=$(echo "$temp_date" | tr -d '-')
                            local current_date_num=$(echo "$current_date" | tr -d '-')
                            
                            if [ "$commit_date_num" -lt "$current_date_num" ]; then
                                log_debug "Found older commit from $temp_date (before $current_date), stopping"
                                break
                            else
                                log_debug "Commit from $temp_date is same day or newer than $current_date, including it"
                            fi
                        fi
                    elif [ -n "$current_date" ]; then
                        local commit_date_num=$(echo "$temp_date" | tr -d '-')
                        local current_date_num=$(echo "$current_date" | tr -d '-')
                        
                        if [ "$commit_date_num" -lt "$current_date_num" ]; then
                            log_debug "Reached commit from $temp_date (before $current_date), stopping"
                            break
                        fi
                        
                        log_debug "Processing commit from $temp_date: '$temp_message'"
                    fi
                    
                    # Add commit
                    if [ -n "$formatted_commits" ]; then
                        formatted_commits+="\n\n"
                    fi
                    # Trim leading and trailing spaces from commit message
                    temp_message=$(trim_str "$temp_message")
                    formatted_commits+="$temp_date - $temp_message"
                    ((commit_count++))
                    
                    if [ $commit_count -ge 20 ]; then
                        formatted_commits+="\n\n... and more commits (showing latest 20)"
                        formatted_commits+="\nVisit https://github.com/${vendor_lower}/pve-tools/commits for full history"
                        break
                    fi
                    
                    # Reset for next commit
                    temp_date=""
                    temp_author=""
                    temp_message=""
                fi
            fi
        done <<< "$commits_json"
    fi
    
    if [ $commit_count -eq 0 ]; then
        log_debug "No commits found since version $current_version"
        return 1
    fi
    
    log_debug "Successfully parsed $commit_count commits since version $current_version"
    log_debug "First commit in list: $(echo -e "$formatted_commits" | head -1)"
    echo "$formatted_commits"
    return 0
}

# Check for script updates from GitHub
check_for_updates() {
    local current_version="$VERSION"
    local remote_version
    local vendor_lower
    local github_url
    
    log_info "Checking for updates - current version: $current_version"
    
    # Convert VENDOR to lowercase for URL
    vendor_lower=$(lower "$VENDOR")
    github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-tools/main/pve-tools"
    
    log_debug "Fetching remote version from: $github_url"
    
    # Get remote version from GitHub
    remote_version=$(wget -qO- "$github_url" | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote version from: $github_url"
        log_debug "wget command failed or no VERSION found in remote script"
        return 1
    fi
    
    log_info "Remote version: $remote_version"
    log_debug "Version comparison: current='$current_version' vs remote='$remote_version'"
    
    # Compare versions (simple string comparison)
    if [ "$remote_version" != "$current_version" ]; then
        log_debug "Versions are different, checking if remote is newer"
        # Version is different, check if it's newer
        if [ "$remote_version" \> "$current_version" ]; then
            log_info "New version available: $remote_version"
            log_debug "Update available: remote version '$remote_version' > current version '$current_version'"
            return 0  # Update available
        else
            log_info "Current version is newer or equal"
            log_debug "No update needed: remote version '$remote_version' <= current version '$current_version'"
            return 1  # No update needed
        fi
    else
        log_info "Already running latest version"
        log_debug "Versions are identical: '$current_version'"
        return 1  # No update needed
    fi
}

# Update the script from GitHub
update_script() {
    local current_version="$VERSION"
    local script_path
    local vendor_lower
    local github_url
    
    # Get the path of the currently running script
    script_path=$(readlink -f "$0")
    
    log_info "Starting script update from $script_path"
    log_debug "Script path resolved to: $script_path"
    
    # Convert VENDOR to lowercase for URL
    vendor_lower=$(lower "$VENDOR")
    github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-tools/main/pve-tools"
    
    log_debug "Download URL: $github_url"
    
    # Create backup of current script
    local backup_path="${script_path}.ver.${current_version}"
    log_debug "Creating backup at: $backup_path"
    
    if ! cp "$script_path" "$backup_path" >> "$LOG_FILE" 2>&1; then
        log_error "Failed to create backup of current script at: $backup_path"
        log_debug "cp command failed, check permissions and disk space"
        dialog --msgbox "Error: Failed to create backup of current script" 8 50
        return 1
    fi
    
    log_info "Created backup: $backup_path"
    
    # Download new version
    log_debug "Downloading new version from: $github_url"
    if ! wget "$github_url" -O "$script_path" >> "$LOG_FILE" 2>&1; then
        log_error "Failed to download new version from: $github_url"
        log_debug "wget command failed, check network connectivity and URL accessibility"
        dialog --msgbox "Error: Failed to download new version\nRestoring backup..." 8 50
        
        # Restore backup on failure
        cp "${script_path}.ver.${current_version}" "$script_path" >> "$LOG_FILE" 2>&1
        return 1
    fi
    
    # Make executable
    chmod +x "$script_path" >> "$LOG_FILE" 2>&1
    
    log_info "Script updated successfully"
    
    # Show success message and exit
    dialog --msgbox "Update completed successfully!\n\nPlease run pve-tools again to use the new version." 10 60
    
    exit 0
}

# Prompt user for update
prompt_for_update() {
    local remote_version
    local commit_info
    local commits_list
    local commit_count
    local update_message
    local dialog_height=20
    
    # Convert VENDOR to lowercase for URL
    local vendor_lower
    local github_url
    vendor_lower=$(lower "$VENDOR")
    github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-tools/main/pve-tools"
    
    log_debug "Fetching remote version for update prompt from: $github_url"
    
    # Get remote version for display
    remote_version=$(wget -qO- "$github_url" | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    log_debug "Retrieved remote version for prompt: '$remote_version'"
    
    # Get all commits since current version
    log_debug "Fetching commit history since version $VERSION ($RELEASE_DATE)"
    commits_list=$(get_commits_since_version "$VERSION" "$RELEASE_DATE" 2>/dev/null)
    
    if [ -n "$commits_list" ]; then
        # Count number of commits (count lines that start with a date)
        commit_count=$(echo -e "$commits_list" | grep -c "^[0-9][0-9][0-9][0-9]-" || echo "0")
        log_debug "Successfully retrieved $commit_count commits since current version"
        
        # Adjust dialog height based on number of commits
        if [ "$commit_count" -gt 5 ]; then
            dialog_height=25  # Use larger dialog for many commits
        fi
    else
        log_debug "No commit history available, trying single commit fallback"
        # Fallback to single commit info if full history fails
        commit_info=$(get_commit_info 2>/dev/null)
        if [ -n "$commit_info" ]; then
            commits_list="Latest change:\n  $commit_info"
            commit_count=1
        else
            log_debug "No commit info available at all"
            commit_count=0
        fi
    fi
    
    # Build update message with commit history
    if [ -n "$commits_list" ] && [ "$commit_count" -gt 0 ]; then
        if [ "$commit_count" -eq 1 ]; then
            update_message="A new version of pve-tools is available!\n\nCurrent version: $VERSION ($RELEASE_DATE)\nNew version: $remote_version\n\nWhat's new:\n$commits_list\n\nWould you like to update now?"
        else
            update_message="A new version of pve-tools is available!\n\nCurrent version: $VERSION ($RELEASE_DATE)\nNew version: $remote_version\n\nChanges since your version ($commit_count commits):\n\n$commits_list\n\nWould you like to update now?"
        fi
        log_debug "Built update message with $commit_count commits"
    else
        update_message="A new version of pve-tools is available!\n\nCurrent version: $VERSION ($RELEASE_DATE)\nNew version: $remote_version\n\nWould you like to update now?"
        log_debug "Built update message without commit info"
    fi
    
    log_debug "Showing update dialog to user (height: $dialog_height)"
    
    # Use --scrolltext for dialogs with many commits
    if [ "$commit_count" -gt 10 ]; then
        # For many commits, use scrollable text
        echo -e "$update_message" | dialog --title "Update Available" \
               --yes-label "Update" \
               --no-label "Continue" \
               --scrolltext \
               --yesno "$(cat)" $dialog_height 100
    else
        # For fewer commits, use regular dialog
        dialog --title "Update Available" \
               --yes-label "Update" \
               --no-label "Continue" \
               --yesno "$update_message" $dialog_height 100
    fi
    
    case $? in
        0)  # Yes - update
            update_script
            ;;
        1)  # No - continue
            log_info "User declined update"
            return 1
            ;;
        *)  # Error or ESC
            log_info "Update dialog cancelled"
            return 1
            ;;
    esac
}

# Check for pve-config-backup updates from GitHub
check_pve_config_backup_updates() {
    local current_version
    local remote_version
    
    log_info "Checking for pve-config-backup updates"
    
    # Get current version from local file
    if [ ! -f /usr/local/sbin/pve-config-backup ]; then
        log_info "pve-config-backup not installed locally"
        return 2  # Not installed
    fi
    
    current_version=$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '"' | head -1)
    
    if [ -z "$current_version" ]; then
        log_error "Failed to retrieve current pve-config-backup version"
        return 1
    fi
    
    log_info "Current pve-config-backup version: $current_version"
    
    # Convert VENDOR to lowercase for URL
    local vendor_lower
    local github_url
    vendor_lower=$(lower "$VENDOR")
    github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"
    
    log_debug "Fetching remote pve-config-backup version from: $github_url"
    
    # Get remote version from GitHub
    remote_version=$(wget -qO- "$github_url" | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote pve-config-backup version from: $github_url"
        log_debug "wget command failed or no VERSION found in remote pve-config-backup script"
        return 1
    fi
    
    log_info "Remote pve-config-backup version: $remote_version"
    log_debug "pve-config-backup version comparison: current='$current_version' vs remote='$remote_version'"
    
    # Compare versions (simple string comparison)
    if [ "$remote_version" != "$current_version" ]; then
        # Version is different, check if it's newer
        if [ "$remote_version" \> "$current_version" ]; then
            log_info "New pve-config-backup version available: $remote_version"
            return 0  # Update available
        else
            log_info "Current pve-config-backup version is newer or equal"
            return 1  # No update needed
        fi
    else
        log_info "Already running latest pve-config-backup version"
        return 1  # No update needed
    fi
}

# Update pve-config-backup on all cluster nodes
update_pve_config_backup() {
    local remote_version
    local vendor_lower
    local github_url
    
    log_info "Starting pve-config-backup update on all cluster nodes"
    
    # Convert VENDOR to lowercase for URL
    vendor_lower=$(lower "$VENDOR")
    github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"
    
    log_debug "Fetching pve-config-backup for update from: $github_url"
    
    # Get remote version for logging
    remote_version=$(wget -qO- "$github_url" | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote pve-config-backup version for update from: $github_url"
        log_debug "wget command failed or no VERSION found in remote pve-config-backup script"
        return 1
    fi
    
    log_info "Updating to pve-config-backup version: $remote_version"
    log_debug "pve-config-backup update URL: $github_url"
    
    # Get HA nodes list (fallback to all nodes if no HA group exists)
    local ha_nodes_list
    ha_nodes_list=$(get_ha_nodes)
    
    if [ -z "$ha_nodes_list" ]; then
        log_error "No nodes found in HA configuration or cluster"
        dialog --msgbox "Error: No cluster nodes found for update" 8 40
        return 1
    fi
    
    # Build node list with IPs
    local node_list=""
    while IFS= read -r node_name; do
        [ -z "$node_name" ] && continue
        local node_ip
        node_ip=$(awk -v node="$node_name" '
            $1 == "name:" && $2 == node {found=1; next}
            found && $1 == "ring0_addr:" {print $2; found=0}
        ' /etc/pve/corosync.conf)
        
        if [ -n "$node_ip" ]; then
            node_list+="$node_name $node_ip"$'\n'
        else
            log_error "Could not find IP for node: $node_name"
        fi
    done <<< "$ha_nodes_list"
    
    # Remove trailing newline
    node_list=$(echo "$node_list" | sed '/^$/d')
    
    if [ -z "$node_list" ]; then
        log_error "No valid node IPs found for update"
        dialog --msgbox "Error: No valid node IPs found for update" 8 40
        return 1
    fi
    
    local total_nodes=$(echo "$node_list" | wc -l)
    local updated=0
    local failed=0
    
    dialog --infobox "Updating pve-config-backup on $total_nodes nodes..." 5 60
    
    # Process each node
    while IFS= read -r line; do
        [ -z "$line" ] && continue
        
        local node=$(echo "$line" | cut -d' ' -f1)
        local ip=$(echo "$line" | cut -d' ' -f2)
        
        log_info "Updating pve-config-backup on node: $node ($ip)"
        
        # Create GitHub URL with lowercase vendor
        local vendor_lower github_url
        vendor_lower=$(lower "$VENDOR")
        github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"

        # Update pve-config-backup on this node
        log_debug "SSH connecting to $node ($ip) for pve-config-backup update"
        log_debug "Using update URL: $github_url"
        local result
        result=$(ssh -o StrictHostKeyChecking=no root@"$ip" "bash -s" << ENDSCRIPT

# Stop the daemon before update
if [ -f /usr/local/sbin/pve-config-backup ]; then
    /usr/local/sbin/pve-config-backup stop >/dev/null 2>&1
fi

# Create backup of current version
if [ -f /usr/local/sbin/pve-config-backup ]; then
    current_ver=\$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '\"' | head -1)
    if [ -n \"\$current_ver\" ]; then
        cp /usr/local/sbin/pve-config-backup \"/usr/local/sbin/pve-config-backup.ver.\${current_ver}\" 2>/dev/null
    fi
fi

# Download new version
cd /tmp
wget '$github_url' \
    -O /usr/local/sbin/pve-config-backup >/dev/null 2>&1

if [ $? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
    chmod +x /usr/local/sbin/pve-config-backup
    # Reinstall and restart the service
    /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
    if [ $? -eq 0 ]; then
        /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
        if [ $? -eq 0 ]; then
            echo "UPDATE_SUCCESS"
        else
            echo "START_FAILED"
        fi
    else
        echo "INSTALL_FAILED" 
    fi
else
    echo "DOWNLOAD_FAILED"
fi
ENDSCRIPT
        2>/dev/null)
        
        case "$result" in
            "UPDATE_SUCCESS")
                log_info "Node $node: pve-config-backup updated successfully"
                ((updated++))
                ;;
            *)
                log_error "Node $node: pve-config-backup update failed - $result"
                ((failed++))
                ;;
        esac
    done < <(echo "$node_list")
    
    log_info "pve-config-backup update complete - Total: $total_nodes, Updated: $updated, Failed: $failed"
    
    if [ $updated -gt 0 ]; then
        dialog --msgbox "pve-config-backup update complete!\nTotal nodes: $total_nodes\nUpdated: $updated\nFailed: $failed\n\nNew version: $remote_version" 12 60
    else
        dialog --msgbox "pve-config-backup update failed on all nodes.\nPlease check logs for details." 10 50
    fi
    
    return 0
}

# Check and prompt for pve-config-backup updates
check_and_update_pve_config_backup() {
    local update_status
    update_status=$(check_pve_config_backup_updates)
    local check_result=$?
    
    case $check_result in
        0)  # Update available
            local current_version remote_version vendor_lower github_url
            current_version=$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '"' | head -1)
            
            vendor_lower=$(lower "$VENDOR")
            github_url="https://raw.githubusercontent.com/${vendor_lower}/pve-config-backup/main/pve-config-backup"
            
            log_debug "Fetching remote pve-config-backup version for dialog from: $github_url"
            remote_version=$(wget -qO- "$github_url" | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
            log_debug "Dialog will show: current='$current_version' vs remote='$remote_version'"
            
            dialog --title "pve-config-backup Update Available" \
                   --yes-label "Update" \
                   --no-label "Skip" \
                   --yesno "A new version of pve-config-backup is available!\n\nCurrent version: $current_version\nNew version: $remote_version\n\nWould you like to update now?" 12 60
            
            case $? in
                0)  # Yes - update
                    update_pve_config_backup
                    ;;
                1)  # No - skip
                    log_info "User declined pve-config-backup update"
                    ;;
                *)  # Error or ESC
                    log_info "pve-config-backup update dialog cancelled"
                    ;;
            esac
            ;;
        1)  # No update needed
            log_info "pve-config-backup is up to date"
            ;;
        2)  # Not installed
            log_info "pve-config-backup not installed, will be handled by ensure_pve_config_backup_running"
            ;;
        *)  # Error
            log_error "Failed to check pve-config-backup updates"
            ;;
    esac
}

# OODP (ON- & OFF-SITE DATA PROTECTION) FUNCTIONS

# Function to get OODP task details
# Arguments: ip_addr pool_name dataset_name
get_oodp_details() {
    local ip_addr="$1"
    local pool_name="$2"
    local dataset_name="$3"
    
    # Validate required parameters
    if [[ -z "$ip_addr" || -z "$pool_name" || -z "$dataset_name" ]]; then
        log_error "get_oodp_details: Missing required parameters"
        log_error "Usage: get_oodp_details <ip_addr> <pool_name> <dataset_name>"
        return 1
    fi
    
    log_info "Getting OODP details for $pool_name/$dataset_name from $ip_addr"
    log_debug "get_oodp_details: ip_addr='$ip_addr'"
    log_debug "get_oodp_details: pool_name='$pool_name'"
    log_debug "get_oodp_details: dataset_name='$dataset_name'"
    
    # Make REST API call to get tasks
    local url="https://$ip_addr:$rest_api_port/api/v4/odp/tasks"
    log_debug "get_oodp_details: Making request to: $url"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR GET OODP DETAILS ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
    log_debug "======================================================================"
    
    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X GET -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_oodp_details: Failed to connect to OODP API at $ip_addr:$rest_api_port"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    log_debug "get_oodp_details: HTTP status: $http_status"
    log_debug "get_oodp_details: Response body: ${body:0:200}..."
    
    # Check for authentication failure
    check_rest_api_auth_failure "$http_status" "$body" "get_oodp_details"
    local auth_check_result=$?
    
    if [[ $auth_check_result -eq 10 ]]; then
        log_debug "get_oodp_details: Authentication failure (401), returning special code 10"
        return 10
    elif [[ $auth_check_result -eq 11 ]]; then
        log_debug "get_oodp_details: Access forbidden (403), returning special code 11"
        return 11
    elif [[ $auth_check_result -ne 0 ]]; then
        log_debug "get_oodp_details: Authentication check failed with code $auth_check_result"
        return 1
    fi
    
    # Check for successful response
    if [[ "$http_status" != "200" ]]; then
        log_error "get_oodp_details: REST API call failed with HTTP status $http_status"
        return 1
    fi
    
    log_debug "get_oodp_details: Raw response body: ${body:0:200}..."
    
    # Check if response contains error (but not null)
    if echo "$body" | grep -q '"error":[[:space:]]*"[^"]*"'; then
        local error_msg=$(echo "$body" | grep -o '"error"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"error"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        log_error "get_oodp_details: API returned error: $error_msg"
        return 1
    fi
    
    # Store results in temporary file for processing
    local temp_file
    temp_file=$(mktemp)
    echo "$body" > "$temp_file"
    
    # Search for matching task based on pool_name and dataset_name
    local task_found=false
    local task_name=""
    local task_type=""
    local task_enabled=""
    local task_plan=""
    local task_destinations=""
    
    local search_pattern="$pool_name/$dataset_name"
    log_debug "get_oodp_details: Searching for pattern: '$search_pattern'"
    
    # Check if jq is available for JSON parsing
    if command -v jq >/dev/null 2>&1; then
        log_debug "get_oodp_details: Using jq for JSON parsing"
        
        # Extract task details using jq - looking for tasks that match our dataset
        local task_data
        task_data=$(jq -r --arg pattern "$search_pattern" '
            .data.entries[] | 
            select(.name | contains($pattern)) | 
            "\(.name)|\(.type)|\(.enabled)|\(.plan)|\(.destinations | tostring)"
        ' "$temp_file" 2>/dev/null)
        
        if [[ -n "$task_data" ]]; then
            task_found=true
            IFS='|' read -r task_name task_type task_enabled task_plan task_destinations <<< "$task_data"
            log_debug "get_oodp_details: Found matching task via jq"
        fi
    else
        log_debug "get_oodp_details: Using grep/sed fallback for JSON parsing"
        
        # Fallback parsing using grep and sed
        if grep -q "\"name\"[[:space:]]*:[[:space:]]*\"[^\"]*$search_pattern" "$temp_file"; then
            task_found=true
            log_debug "get_oodp_details: Found matching task via grep/sed"
            
            # Extract task details using grep/sed
            task_name=$(grep -A 20 "\"name\"[[:space:]]*:[[:space:]]*\"[^\"]*$search_pattern" "$temp_file" | grep -o '"name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1)
            task_type=$(grep -A 20 "\"name\"[[:space:]]*:[[:space:]]*\"[^\"]*$search_pattern" "$temp_file" | grep -o '"type"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"type"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1)
            task_enabled=$(grep -A 20 "\"name\"[[:space:]]*:[[:space:]]*\"[^\"]*$search_pattern" "$temp_file" | grep -o '"enabled"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"enabled"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1)
            task_plan=$(grep -A 20 "\"name\"[[:space:]]*:[[:space:]]*\"[^\"]*$search_pattern" "$temp_file" | grep -o '"plan"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"plan"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/' | head -1)
        fi
    fi
    
    # Clean up temporary file
    rm -f "$temp_file"
    
    if [[ "$task_found" == "true" ]]; then
        log_info "get_oodp_details: Successfully found OODP task details"
        log_info "Task Name: $task_name"
        log_info "Task Type: $task_type"
        log_info "Task Enabled: $task_enabled"
        log_info "Task Plan: $task_plan"
        
        # Store results in global variables for script use
        OODP_TASK_NAME="$task_name"
        OODP_TASK_TYPE="$task_type"
        OODP_TASK_ENABLED="$task_enabled"
        OODP_TASK_PLAN="$task_plan"
        OODP_TASK_DESTINATIONS="$task_destinations"
        
        return 0
    else
        log_error "get_oodp_details: No matching OODP task found for $pool_name/$dataset_name"
        return 1
    fi
}

# Parse and format OODP Task Destinations
parse_oodp_destinations() {
    local destinations_raw="$1"
    local formatted_output=""
    
    log_debug "parse_oodp_destinations: Input raw data: '$destinations_raw'"
    
    # Extract content between [ and ]
    local json_content
    json_content=$(echo "$destinations_raw" | sed 's/.*\[\(.*\)\].*/\1/')
    log_debug "parse_oodp_destinations: Extracted JSON content: '$json_content'"
    
    # Extract dataset and plan using sed/grep
    local dataset_full
    local plan_content
    
    # Extract dataset value (everything between "dataset":" and "plan")
    dataset_full=$(echo "$json_content" | sed 's/.*"dataset"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
    log_debug "parse_oodp_destinations: Full dataset: '$dataset_full'"
    
    # Extract plan value (everything between "plan":" and the end)
    plan_content=$(echo "$json_content" | sed 's/.*"plan"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
    log_debug "parse_oodp_destinations: Plan content: '$plan_content'"
    
    # Parse dataset: split IP:port and dataset path
    local ip_port
    local dataset_path
    
    if [[ "$dataset_full" =~ ^([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+:[0-9]+)/(.+)$ ]]; then
        ip_port="${BASH_REMATCH[1]}"
        dataset_path="${BASH_REMATCH[2]}"
        log_debug "parse_oodp_destinations: IP:port='$ip_port', dataset path='$dataset_path'"
    else
        log_error "parse_oodp_destinations: Failed to parse dataset format: '$dataset_full'"
        return 1
    fi
    
    # Format output as requested
    formatted_output="Task Destinations:\n"
    formatted_output="${formatted_output}  DST:  ${dataset_path}    ${ip_port}\n"
    formatted_output="${formatted_output}  plan: ${plan_content}"
    
    log_debug "parse_oodp_destinations: Formatted output: '$formatted_output'"
    echo -e "$formatted_output"
    
    return 0
}

# Get all NFS storage entries that are NOT cloned storage
get_regular_nfs_storage() {
    local regular_storage=""
    
    # Check if storage.cfg exists
    if [[ ! -f "/etc/pve/storage.cfg" ]]; then
        log_error "get_regular_nfs_storage: /etc/pve/storage.cfg not found"
        return 1
    fi
    
    # Read storage.cfg and find NFS entries
    while IFS= read -r line; do
        if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
            local storage_name="${BASH_REMATCH[1]}"
            local storage_path=""
            local storage_server=""
            
            # Read the configuration block for this storage
            while IFS= read -r config_line; do
                if [[ "$config_line" =~ ^[[:space:]]*export[[:space:]]+([^[:space:]]+) ]]; then
                    storage_path="${BASH_REMATCH[1]}"
                elif [[ "$config_line" =~ ^[[:space:]]*server[[:space:]]+([^[:space:]]+) ]]; then
                    storage_server="${BASH_REMATCH[1]}"
                elif [[ "$config_line" =~ ^[[:space:]]*$ ]] || [[ "$config_line" =~ ^[a-zA-Z]+: ]]; then
                    # Empty line or start of new section, stop looking
                    break
                fi
            done
            
            # Check if this is NOT a cloned storage (cloned storage typically has "clone" in path or name)
            if [[ ! "$storage_name" =~ clone ]] && [[ ! "$storage_path" =~ clone ]]; then
                if [ -n "$regular_storage" ]; then
                    regular_storage="${regular_storage}\n${storage_name}|${storage_path}|${storage_server}"
                else
                    regular_storage="${storage_name}|${storage_path}|${storage_server}"
                fi
                log_debug "get_regular_nfs_storage: Found regular NFS storage: $storage_name (path: $storage_path, server: $storage_server)"
            else
                log_debug "get_regular_nfs_storage: Skipping cloned storage: $storage_name (path: $storage_path)"
            fi
        fi
    done < /etc/pve/storage.cfg
    
    echo -e "$regular_storage"
}

# Validate OODP task requirements and format output
validate_oodp_task() {
    local storage_name="$1"
    local task_enabled="$2"
    local task_plan="$3"
    local task_destinations="$4"
    
    local PASSED='PASSED'
    local FAILED='FAILED !'
    local MISSING_DST='MISSING DST'
    local TASK_DISABLED='TASK DISABLED'
    
    local status="$PASSED"
    local details=""
    
    log_debug "validate_oodp_task: Validating $storage_name - enabled=$task_enabled, plan=$task_plan"
    
    # Check 1: Task must be enabled
    if [[ "$task_enabled" != "on" ]]; then
        status="$TASK_DISABLED"
        details="Task disabled"
        log_debug "validate_oodp_task: Task is disabled"
        
        # Format output and return early for disabled tasks
        local header_line="    NFS Storage: ${storage_name}"
        local status_length=${#status}
        local total_width=94
        local padding=$((total_width - ${#header_line} - status_length))
        if [ $padding -lt 1 ]; then padding=1; fi
        
        local output
        output=$(printf "%s%*s%s\n" "$header_line" $padding "" "$status")
        output="${output}\n    ${details}"
        
        echo -e "$output"
        log_info "validate_oodp_task: $storage_name - $status"
        return 0
    fi
    
    # Check 2: Must have SRC plan (Task Plan)
    if [[ -z "$task_plan" ]] || [[ "$task_plan" == "null" ]]; then
        status="$FAILED"
        details="${details}No SRC plan; "
        log_debug "validate_oodp_task: No SRC plan found"
    fi
    
    # Check 3: Must have at least one DST plan (in destinations)
    if [[ -z "$task_destinations" ]] || [[ "$task_destinations" == "null" ]] || [[ "$task_destinations" == "[]" ]]; then
        status="$MISSING_DST"
        details="${details}No DST plan; "
        log_debug "validate_oodp_task: No DST destinations found"
    elif ! echo "$task_destinations" | grep -q "plan.*=>"; then
        status="$MISSING_DST"
        details="${details}No DST plan; "
        log_debug "validate_oodp_task: DST destinations exist but no plan found"
    fi
    
    # Format output for checkup_summary
    local header_line="    NFS Storage: ${storage_name}"
    local status_length=${#status}
    local total_width=94
    local padding=$((total_width - ${#header_line} - status_length))
    if [ $padding -lt 1 ]; then padding=1; fi
    
    local output
    output=$(printf "%s%*s%s\n" "$header_line" $padding "" "$status")
    
    if [[ "$status" == "$FAILED" ]] || [[ "$status" == "$MISSING_DST" ]]; then
        output="${output}\n    Issues: ${details%%; }"
    else
        # Show detailed breakdown for PASSED status with each item on separate lines
        output="${output}\n    Task enabled"
        output="${output}\n    SRC plan: ${task_plan}"
        
        # Count and show DST plans
        local dst_count=1
        if echo "$task_destinations" | grep -c "plan.*=>" >/dev/null 2>&1; then
            dst_count=$(echo "$task_destinations" | grep -c "plan.*=>")
        fi
        output="${output}\n    DST plans: $dst_count destination(s)"
        
        # Parse and show destination details
        if [[ -n "$task_destinations" ]] && [[ "$task_destinations" != "null" ]]; then
            local destinations_formatted
            destinations_formatted=$(parse_oodp_destinations "Task Destinations: $task_destinations" 2>/dev/null | tail -n +2)
            if [[ -n "$destinations_formatted" ]]; then
                # Add proper indentation to destination details
                destinations_formatted=$(echo "$destinations_formatted" | sed 's/^/    /')
                output="${output}\n${destinations_formatted}"
            fi
        fi
    fi
    
    echo -e "$output"
    log_info "validate_oodp_task: $storage_name - $status"
    return 0
}

# Main OODP checkup function for system checkup integration
perform_oodp_checkup() {
    local temp_file="$1"
    
    log_info "perform_oodp_checkup: Starting OODP task status checkup"
    
    # Ensure we have REST API credentials
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]]; then
        echo "   Configuration error: REST API credentials not configured" >> "$temp_file"
        echo "   Please run: pve-tools -> Setup -> REST API Configuration" >> "$temp_file"
        echo "" >> "$temp_file"
        return 1
    fi
    
    # Test REST API authentication before starting OODP tasks
    log_debug "perform_oodp_checkup: Testing REST API authentication"
    # We need to test with a known IP - let's use the first available storage IP
    local test_ip=""
    
    # Try to get the first NFS storage IP for authentication test
    while IFS= read -r line; do
        if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
            local storage_name="${BASH_REMATCH[1]}"
            # Read the next few lines to find the server IP
            while IFS= read -r config_line; do
                if [[ "$config_line" =~ ^[[:space:]]*server[[:space:]]+([^[:space:]]+) ]]; then
                    test_ip="${BASH_REMATCH[1]}"
                    log_debug "perform_oodp_checkup: Using IP $test_ip for authentication test"
                    break 2  # Break out of both loops
                elif [[ "$config_line" =~ ^[[:space:]]*$ ]] || [[ "$config_line" =~ ^[a-zA-Z]+: ]]; then
                    break
                fi
            done
        fi
    done < /etc/pve/storage.cfg
    
    if [[ -n "$test_ip" ]]; then
        # Test authentication with a simple API call
        local auth_url="https://$test_ip:$rest_api_port/api/v4/conn_test"
        
        # Log the FULL curl command for manual testing
        log_debug "=============== FULL CURL COMMAND FOR AUTH TEST ==============="
        log_debug "curl -k -s --connect-timeout 2 --max-time 5 -w '\\n%{http_code}' -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$auth_url'"
        log_debug "==============================================================="
        
        local auth_test_response
        auth_test_response=$(curl -k -s --connect-timeout 2 --max-time 5 -w "\n%{http_code}" -X GET -u "${rest_api_user}:${rest_api_password}" \
            -H 'Content-Type: application/json' \
            "$auth_url" 2>/dev/null)
        local auth_curl_exit=$?
        
        if [[ $auth_curl_exit -eq 0 ]]; then
            local auth_http_status=$(echo "$auth_test_response" | tail -n1)
            local auth_body=$(echo "$auth_test_response" | sed '$d')
            
            log_debug "perform_oodp_checkup: Authentication test status: $auth_http_status"
            
            if [[ "$auth_http_status" == "401" ]]; then
                # Format with same style as other checkup statuses
                local header_line="    OODP Tasks"
                local status_text="Authentication FAILED"
                local status_length=${#status_text}
                local padding=$((104 - ${#header_line} - status_length))
                if [ $padding -lt 1 ]; then padding=1; fi
                
                printf "%s%*s%s\n" "$header_line" $padding "" "$status_text" >> "$temp_file"
                echo "    Issues: REST API credentials are incorrect" >> "$temp_file"
                echo "            The username or password has been changed on the $PRODUCT." >> "$temp_file"
                echo "            Please run: pve-tools -> Setup -> Configure REST API" >> "$temp_file"
                echo "            Current user: $rest_api_user" >> "$temp_file"
                echo "" >> "$temp_file"
                log_error "perform_oodp_checkup: REST API authentication failed (401) - skipping OODP checkup"
                return 1
            elif [[ "$auth_http_status" == "403" ]]; then
                # Format with same style as other checkup statuses
                local header_line="    OODP Tasks"
                local status_text="Authentication FAILED"
                local status_length=${#status_text}
                local padding=$((104 - ${#header_line} - status_length))
                if [ $padding -lt 1 ]; then padding=1; fi
                
                printf "%s%*s%s\n" "$header_line" $padding "" "$status_text" >> "$temp_file"
                echo "    Issues: Access forbidden" >> "$temp_file"
                echo "            The user '$rest_api_user' does not have permission for this operation." >> "$temp_file"
                echo "            Please check user permissions on the $PRODUCT." >> "$temp_file"
                echo "" >> "$temp_file"
                log_error "perform_oodp_checkup: REST API access forbidden (403) - skipping OODP checkup"
                return 1
            elif [[ "$auth_http_status" != "200" ]]; then
                # Format with same style as other checkup statuses
                local header_line="    OODP Tasks"
                local status_text="Connection FAILED"
                local status_length=${#status_text}
                local padding=$((104 - ${#header_line} - status_length))
                if [ $padding -lt 1 ]; then padding=1; fi
                
                printf "%s%*s%s\n" "$header_line" $padding "" "$status_text" >> "$temp_file"
                echo "    Issues: Cannot connect to storage server ($test_ip)" >> "$temp_file"
                echo "            HTTP status: $auth_http_status" >> "$temp_file"
                echo "            Please check network connectivity and server configuration." >> "$temp_file"
                echo "" >> "$temp_file"
                log_error "perform_oodp_checkup: REST API connection failed (HTTP $auth_http_status) - skipping OODP checkup"
                return 1
            fi
            
            log_debug "perform_oodp_checkup: Authentication test successful"
        else
            # Format with same style as other checkup statuses
            local header_line="    OODP Tasks"
            local status_text="Connection FAILED"
            local status_length=${#status_text}
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "$status_text" >> "$temp_file"
            echo "    Issues: Cannot connect to storage server ($test_ip)" >> "$temp_file"
            echo "            Please check network connectivity and server configuration." >> "$temp_file"
            echo "" >> "$temp_file"
            log_error "perform_oodp_checkup: Failed to connect for authentication test - skipping OODP checkup"
            return 1
        fi
    else
        log_debug "perform_oodp_checkup: No NFS storage IP found for authentication test, proceeding"
    fi
    
    # Build storage IP mapping if not already available
    if [[ ${#storage_ip_map[@]} -eq 0 ]]; then
        log_debug "perform_oodp_checkup: Building storage IP map"
        # This would typically be called from pve-tools context where build_storage_ip_map is available
        # For now, we'll need to build it locally
        declare -A local_storage_ip_map
        
        while IFS= read -r line; do
            if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
                local storage_name="${BASH_REMATCH[1]}"
                local server_ip=""
                
                # Read the next few lines to find the server IP
                while IFS= read -r config_line; do
                    if [[ "$config_line" =~ ^[[:space:]]*server[[:space:]]+([^[:space:]]+) ]]; then
                        server_ip="${BASH_REMATCH[1]}"
                        break
                    elif [[ "$config_line" =~ ^[[:space:]]*$ ]] || [[ "$config_line" =~ ^[a-zA-Z]+: ]]; then
                        break
                    fi
                done
                
                if [[ -n "$server_ip" ]]; then
                    local_storage_ip_map["$storage_name"]="$server_ip"
                    log_debug "perform_oodp_checkup: Mapped $storage_name to $server_ip"
                fi
            fi
        done < /etc/pve/storage.cfg
    fi
    
    # Get list of regular (non-cloned) NFS storage
    local nfs_storage_list
    nfs_storage_list=$(get_regular_nfs_storage)
    
    if [ -z "$nfs_storage_list" ]; then
        echo "   No regular NFS storage found (excluding cloned storage)" >> "$temp_file"
        echo "" >> "$temp_file"
        return 0
    fi
    
    # Process each NFS storage
    echo -e "$nfs_storage_list" | while IFS='|' read -r storage_name storage_path storage_server; do
        [ -z "$storage_name" ] && continue
        
        # For backward compatibility, if no separator found, treat whole line as storage_name
        if [[ -z "$storage_path" ]]; then
            storage_name="$(echo "$storage_name" | tr -d '|')"
        fi
        
        log_info "perform_oodp_checkup: Processing storage $storage_name"
        
        # Get storage IP
        local storage_ip
        if [[ -n "${storage_ip_map[$storage_name]}" ]]; then
            storage_ip="${storage_ip_map[$storage_name]}"
        elif [[ -n "${local_storage_ip_map[$storage_name]}" ]]; then
            storage_ip="${local_storage_ip_map[$storage_name]}"
        fi
        
        if [ -z "$storage_ip" ]; then
            local header_line="    NFS Storage: ${storage_name}"
            local status_length=9  # Length of "FAILED !"
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "FAILED !" >> "$temp_file"
            printf "    Issues: Cannot determine storage IP address\n\n" >> "$temp_file"
            continue
        fi
        
        # Get dataset path - use passed value or fall back to storage.cfg
        local dataset_path
        if [[ -n "$storage_path" ]]; then
            dataset_path="$storage_path"
        else
            # Fallback to reading from storage.cfg for backward compatibility
            dataset_path=$(awk -v storage="$storage_name" '
                $0 ~ "^nfs: " storage "$" { in_storage=1; next }
                in_storage && /^[[:space:]]*export[[:space:]]/ { 
                    gsub(/^[[:space:]]*export[[:space:]]+/, ""); 
                    print; 
                    exit 
                }
                in_storage && (/^[[:space:]]*$/ || /^[a-zA-Z]+:/) { exit }
            ' /etc/pve/storage.cfg)
        fi
        
        # Extract pool and dataset names from path
        # Handle two formats: /Pool-0/datastore00 or /datastore00
        local pool_name
        local dataset_name
        if [[ "$dataset_path" =~ ^/([^/]+)/([^/]+)$ ]]; then
            # Format: /Pool-0/datastore00
            pool_name="${BASH_REMATCH[1]}"
            dataset_name="${BASH_REMATCH[2]}"
        elif [[ "$dataset_path" =~ ^/([^/]+)$ ]]; then
            # Format: /datastore00 - try to determine pool from dataset name
            dataset_name="${BASH_REMATCH[1]}"
            
            # Try to determine pool name from dataset name pattern
            if [[ "$dataset_name" =~ datastore([0-9]+) ]]; then
                local dataset_num="${BASH_REMATCH[1]}"
                # Remove leading zeros from the number
                dataset_num=$((10#$dataset_num))
                pool_name="Pool-${dataset_num}"
                log_debug "perform_oodp_checkup: Determined pool '$pool_name' from dataset '$dataset_name'"
            else
                # Fallback to Pool-0 if we can't determine
                pool_name="Pool-0"
                log_debug "perform_oodp_checkup: Using default pool 'Pool-0' for dataset path '$dataset_path'"
            fi
        else
            local header_line="    NFS Storage: ${storage_name}"
            local status_length=9  # Length of "FAILED !"
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "FAILED !" >> "$temp_file"
            printf "    Issues: Cannot parse dataset path: %s\n\n" "$dataset_path" >> "$temp_file"
            continue
        fi
        
        log_debug "perform_oodp_checkup: Parsed path $dataset_path -> pool=$pool_name, dataset=$dataset_name"
        
        # Get OODP task details using existing function
        get_oodp_details "$storage_ip" "$pool_name" "$dataset_name" >/dev/null 2>&1
        local oodp_exit=$?
        
        if [[ $oodp_exit -eq 0 ]]; then
            # Validate task requirements and get formatted output
            local validation_output
            validation_output=$(validate_oodp_task "$storage_name" "$OODP_TASK_ENABLED" "$OODP_TASK_PLAN" "$OODP_TASK_DESTINATIONS")
            
            echo "$validation_output" >> "$temp_file"
            echo "" >> "$temp_file"
        elif [[ $oodp_exit -eq 10 ]]; then
            # Authentication failure - this should not happen here since we tested at the beginning
            local header_line="    NFS Storage: ${storage_name}"
            local status_length=9  # Length of "FAILED !"
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "FAILED !" >> "$temp_file"
            printf "    Issues: REST API authentication failed for %s\n\n" "$storage_ip" >> "$temp_file"
        elif [[ $oodp_exit -eq 11 ]]; then
            # Access forbidden - this should not happen here since we tested at the beginning
            local header_line="    NFS Storage: ${storage_name}"
            local status_length=9  # Length of "FAILED !"
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "FAILED !" >> "$temp_file"
            printf "    Issues: REST API access forbidden for %s\n\n" "$storage_ip" >> "$temp_file"
        else
            # Other error (no task found, connection error, etc.)
            local header_line="    NFS Storage: ${storage_name}"
            local status_length=9  # Length of "FAILED !"
            local padding=$((104 - ${#header_line} - status_length))
            if [ $padding -lt 1 ]; then padding=1; fi
            
            printf "%s%*s%s\n" "$header_line" $padding "" "FAILED !" >> "$temp_file"
            printf "    Issues: Cannot retrieve OODP task details from %s\n\n" "$storage_ip" >> "$temp_file"
        fi
    done
    
    log_info "perform_oodp_checkup: OODP checkup completed"
    return 0
}

# MAIN MENU SYSTEM

# Check background pve-config-backup status
check_background_status() {
    if [ -f /tmp/pve-config-backup-status ]; then
        local status_info=$(cat /tmp/pve-config-backup-status)
        if [[ "$status_info" == COMPLETED:* ]]; then
            local completed_time="${status_info#COMPLETED:}"
            PVE_CONFIG_BACKUP_STATUS="completed"
            PVE_CONFIG_BACKUP_RESULT="✓ pve-config-backup check completed at $completed_time"
        fi
    elif [ -n "$PVE_CONFIG_BACKUP_PID" ]; then
        if ! kill -0 "$PVE_CONFIG_BACKUP_PID" 2>/dev/null; then
            PVE_CONFIG_BACKUP_STATUS="failed"
            PVE_CONFIG_BACKUP_RESULT="✗ pve-config-backup check failed"
        else
            PVE_CONFIG_BACKUP_STATUS="checking"
            PVE_CONFIG_BACKUP_RESULT="⏳ pve-config-backup check in progress..."
        fi
    fi
}

# System checkup summary
checkup_summary() {
    log_info "Starting system checkup summary"
    
    # Status display variables - customize these to change status appearance
    local PASSED='PASSED'
    local FAILED='FAILED !'
    local STOPPED='STOPPED'
    local MISSING='MISSING'
    local DISABLED='DISABLED'
    
    local summary=""
    local temp_file=$(mktemp)
    
    # Header
    summary="SYSTEM CHECKUP SUMMARY\n"
    summary="${summary}========================\n\n"
    
    # 1. VENDOR PRODUCT On- & Off-site Data Protection tasks status
    summary="${summary}1. $VENDOR $PRODUCT On- & Off-site Data Protection tasks status:\n"
    
    # Perform OODP checkup (functions are now integrated)
    {
        local oodp_temp_file=$(mktemp)
        
        # Ensure storage IP mapping is available
        if [[ ${#storage_ip_map[@]} -eq 0 ]]; then
            log_debug "checkup_summary: Building storage IP map for OODP checkup"
            build_storage_ip_map
        fi
        
        # Perform OODP checkup
        perform_oodp_checkup "$oodp_temp_file"
        
        if [[ -f "$oodp_temp_file" ]] && [[ -s "$oodp_temp_file" ]]; then
            summary="${summary}$(cat "$oodp_temp_file")\n"
            log_debug "checkup_summary: Added OODP checkup results to summary"
        else
            summary="${summary}   No OODP checkup results available\n"
            summary="${summary}\n"
            log_debug "checkup_summary: No OODP checkup results available"
        fi
        
        rm -f "$oodp_temp_file"
    }
    
    summary="${summary}\n"
    
    # 2. pve-config-backup daemon STATUS
    summary="${summary}2. pve-config-backup Daemon Status:\n"
    
    # Check if ha-nodes group exists
    local ha_nodes_list
    ha_nodes_list=$(get_ha_nodes)
    log_debug "HA nodes list: '$ha_nodes_list'"
    
    # Check if ha-nodes placement constraint exists (version-aware)
    local MAJOR
    MAJOR="$(get_pve_major)"
    local ha_placement_exists=0
    
    if [ "$MAJOR" -ge 9 ]; then
        # PVE 9+: Check for node-affinity rules
        local existing_rules
        existing_rules=$(ha-manager rules list --type node-affinity 2>/dev/null | tail -n +4 | head -n -1 | grep -v '^[[:space:]]*$' | sed 's/│//g' | awk '{print $1}')
        if echo "$existing_rules" | grep -qx "ha-nodes"; then
            ha_placement_exists=1
        fi
        log_debug "PVE $MAJOR: HA node-affinity rules exist: $ha_placement_exists"
    else
        # PVE 8: Check for HA groups
        ha_placement_exists=$(get_groups | grep -c "^ha-nodes$")
        log_debug "PVE $MAJOR: HA groups exist count: $ha_placement_exists"
    fi
    
    if [ "$ha_placement_exists" -gt 0 ]; then
        if [ "$MAJOR" -ge 9 ]; then
            summary="${summary}   The ha-nodes rule is present, checkup is done for rule hosts\n"
            log_info "HA-nodes rule exists, checking only HA nodes: $(echo "$ha_nodes_list" | tr '\n' ',')"
        else
            summary="${summary}   The ha-nodes group is present, checkup is done for group hosts\n"
            log_info "HA-nodes group exists, checking only HA nodes: $(echo "$ha_nodes_list" | tr '\n' ',')"
        fi
    else
        if [ "$MAJOR" -ge 9 ]; then
            summary="${summary}   The ha-nodes rule is not defined, checkup is done for all PVE hosts\n"
            log_info "HA-nodes rule does not exist, checking all nodes"
        else
            summary="${summary}   The ha-nodes group is not defined, checkup is done for all PVE hosts\n"
            log_info "HA-nodes group does not exist, checking all nodes"
        fi
    fi
    
    # Get node list from corosync configuration
    local node_list
    node_list=$(awk '$1 == "name:" {node=$2} $1 == "ring0_addr:" {print node, $2}' /etc/pve/corosync.conf)
    log_debug "Node list from corosync.conf: '$node_list'"
    local node_list_line_count
    node_list_line_count=$(echo "$node_list" | wc -l)
    log_debug "Node list has $node_list_line_count lines"
    log_info "TESTING: About to start processing $node_list_line_count nodes"
    
    # Use clustered location for collecting SSH results
    local cluster_temp_file="/etc/pve/.pve-tools-status-$$"
    true > "$cluster_temp_file"  # Initialize empty file
    
    log_debug "Starting to process node list"
    log_info "TESTING: Entering while loop now"
    local node_count=0
    while read -r node ip; do
        node_count=$((node_count + 1))
        log_info "TESTING: Loop iteration $node_count: Processing node: '$node' with IP: '$ip'"
        log_debug "Loop iteration $node_count: Processing node: '$node' with IP: '$ip'"
        
        # Debug: Check if we have empty values
        if [ -z "$node" ]; then
            log_debug "Empty node name detected, skipping"
            continue
        fi
        if [ -z "$ip" ]; then
            log_debug "Empty IP detected for node '$node', skipping"
            continue
        fi
        
        # Check if node should be checked (if ha-nodes placement constraint exists, only check those)
        if [ "$ha_placement_exists" -gt 0 ]; then
            log_debug "Checking if node '$node' is in ha-nodes list: '$ha_nodes_list'"
            local grep_result
            grep_result=$(echo "$ha_nodes_list" | grep "$node")
            log_debug "Grep result for '$node': '$grep_result'"
            if ! echo "$ha_nodes_list" | grep -q "^${node}$"; then
                log_debug "Skipping node '$node' - not in ha-nodes group"
                continue
            else
                log_debug "Node '$node' found in ha-nodes group - will check"
            fi
        fi
        
        log_info "Checking pve-config-backup status on node: $node ($ip)"
        
        local status_output
        status_output=$(timeout 5 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=3 root@"$ip" "
            if [ -f /usr/local/sbin/pve-config-backup ]; then
                if pve-config-backup status >/dev/null 2>&1; then
                    echo 'active (running)'
                else
                    echo 'inactive (dead)'
                fi
            else
                echo 'not-installed'
            fi" < /dev/null 2>/dev/null)
        
        if [ -z "$status_output" ]; then
            status_output="connection failed"
            log_debug "SSH connection failed for node: $node ($ip)"
        else
            log_debug "SSH status result for $node: '$status_output'"
        fi
        
        # Determine overall status
        local node_status="$PASSED"
        if [ "$status_output" != "active (running)" ]; then
            node_status="$STOPPED"
        fi
        
        # Format with right-aligned status
        local info_text="   ${node}:    ${status_output}"
        local text_length=${#info_text}
        local status_length=${#node_status}
        local total_width=94
        local padding=$((total_width - text_length - status_length))
        
        if [ $padding -lt 1 ]; then
            padding=1
        fi
        
        printf "%s%*s%s\n" "$info_text" $padding "" "$node_status" >> "$cluster_temp_file"
        log_debug "Added to temp file: '$info_text' with status '$node_status'"
        log_info "Node $node: pve-config-backup is $(echo "$status_output" | sed 's/active (running)/running/; s/inactive (dead)/not running/; s/not-installed/not installed/')"
    done < <(echo "$node_list")
    
    log_info "TESTING: Exited while loop. Total iterations: $node_count"
    log_debug "Finished processing all nodes. Total loop iterations: $node_count"
    log_debug "Finished processing all nodes. Temp file contents:"
    if [ -f "$cluster_temp_file" ]; then
        log_debug "$(cat "$cluster_temp_file")"
    else
        log_debug "Temp file does not exist!"
    fi
    
    if [ -f "$cluster_temp_file" ]; then
        summary="${summary}$(cat "$cluster_temp_file")\n"
        rm -f "$cluster_temp_file"
    fi
    
    # Also clean up the original temp_file if it exists
    if [ -f "$temp_file" ]; then
        rm -f "$temp_file"
    fi
    
    summary="${summary}\n"
    
    # 3. ha-nodes placement constraint STATUS (version-aware)
    if [ "$MAJOR" -ge 9 ]; then
        summary="${summary}3. HA Nodes Rule Status:\n"
        
        # HA nodes rule status
        local ha_placement_status="$PASSED"
        local ha_placement_info="ha-nodes rule          PRESENT"
        if [ "$ha_placement_exists" -eq 0 ]; then
            ha_placement_status="$MISSING"
            ha_placement_info="ha-nodes rule          not defined yet"
        fi
    else
        summary="${summary}3. HA Nodes Group Status:\n"
        
        # HA nodes group status
        local ha_placement_status="$PASSED"
        local ha_placement_info="ha-nodes group         PRESENT"
        if [ "$ha_placement_exists" -eq 0 ]; then
            ha_placement_status="$MISSING"
            ha_placement_info="ha-nodes group         not defined yet"
        fi
    fi
    
    # Format ha-nodes placement constraint line with right-aligned status
    local info_text="   ${ha_placement_info}"
    local text_length=${#info_text}
    local status_length=${#ha_placement_status}
    local total_width=94
    local padding=$((total_width - text_length - status_length))
    
    if [ $padding -lt 1 ]; then
        padding=1
    fi
    
    printf "%s%*s%s\n" "$info_text" $padding "" "$ha_placement_status" >> "$temp_file"
    
    if [ -f "$temp_file" ]; then
        summary="${summary}$(cat "$temp_file")\n"
        rm -f "$temp_file"
    fi
    
    summary="${summary}   Proxmox VE HA Resources:\n"
    
    # Create temp file for HA resources
    local ha_temp_file=$(mktemp)
    
    # List HA resources - try file first, then ha-manager command
    local ha_resource_count=0
    
    # Method 1: Try reading from resources.cfg file
    if [ -f /etc/pve/ha/resources.cfg ] && [ -r /etc/pve/ha/resources.cfg ]; then
        while IFS= read -r line; do
            if [[ "$line" =~ ^(vm|ct):[[:space:]]*([0-9]+) ]]; then
                local vm_type="${BASH_REMATCH[1]}"
                local vmid="${BASH_REMATCH[2]}"
                
                local vm_info
                vm_info=$(vm_manager_get_vm_type_status_node "$vmid" 2>/dev/null)
                
                local resource_status="$PASSED"
                local resource_info=""
                
                if [ -n "$vm_info" ]; then
                    IFS='|' read -r vm_info_type vm_info_status vm_info_node <<< "$vm_info"
                    resource_info="   ${vm_type}:${vmid}  ${vm_info_type}|${vm_info_status}|${vm_info_node}"
                else
                    resource_info="   ${vm_type}:${vmid}  missing"
                    resource_status="$MISSING"
                fi
                
                # Format HA resource line with right-aligned status
                local res_text_length=${#resource_info}
                local res_status_length=${#resource_status}
                local res_padding=$((total_width - res_text_length - res_status_length))
                
                if [ $res_padding -lt 1 ]; then
                    res_padding=1
                fi
                
                printf "%s%*s%s\n" "$resource_info" $res_padding "" "$resource_status" >> "$ha_temp_file"
                ha_resource_count=$((ha_resource_count + 1))
            fi
        done < /etc/pve/ha/resources.cfg
        
    # Method 2: Fallback to ha-manager command if file not accessible
    elif command -v ha-manager >/dev/null 2>&1; then
        local ha_manager_output
        ha_manager_output=$(ha-manager config 2>/dev/null)
        
        if [ $? -eq 0 ] && [ -n "$ha_manager_output" ]; then
            # Parse resource lines from ha-manager config output
            while IFS= read -r line; do
                if [[ "$line" =~ ^(vm|ct):([0-9]+)$ ]]; then
                    local vm_type="${BASH_REMATCH[1]}"
                    local vmid="${BASH_REMATCH[2]}"
                    
                    local vm_info
                    vm_info=$(vm_manager_get_vm_type_status_node "$vmid" 2>/dev/null)
                    
                    local resource_status="$PASSED"
                    local resource_info=""
                    
                    if [ -n "$vm_info" ]; then
                        IFS='|' read -r vm_info_type vm_info_status vm_info_node <<< "$vm_info"
                        resource_info="   ${vm_type}:${vmid}  ${vm_info_type}|${vm_info_status}|${vm_info_node}"
                    else
                        resource_info="   ${vm_type}:${vmid}  missing"
                        resource_status="$MISSING"
                    fi
                    
                    # Format HA resource line with right-aligned status
                    local res_text_length=${#resource_info}
                    local res_status_length=${#resource_status}
                    local res_padding=$((total_width - res_text_length - res_status_length))
                    
                    if [ $res_padding -lt 1 ]; then
                        res_padding=1
                    fi
                    
                    printf "%s%*s%s\n" "$resource_info" $res_padding "" "$resource_status" >> "$ha_temp_file"
                    ha_resource_count=$((ha_resource_count + 1))
                fi
            done <<< "$ha_manager_output"
        fi
    fi
    
    # Show appropriate message based on results
    if [ $ha_resource_count -eq 0 ]; then
        if [ -f /etc/pve/ha/resources.cfg ] || command -v ha-manager >/dev/null 2>&1; then
            printf "%s%*s%s\n" "   No HA resources configured" $((total_width - 30 - 4)) "" "NONE" >> "$ha_temp_file"
        else
            printf "%s%*s%s\n" "   HA not configured or not available" $((total_width - 36 - 4)) "" "N/A" >> "$ha_temp_file"
        fi
    fi
    
    if [ -f "$ha_temp_file" ]; then
        summary="${summary}$(cat "$ha_temp_file")\n"
        rm -f "$ha_temp_file"
    fi
    
    summary="${summary}\n"
    
    # 3. $PRODUCT VM Start-at-boot STATUS
    summary="${summary}4. $PRODUCT VM Start-at-boot Status:\n"
    
    log_debug " Starting point 3 checkup - $PRODUCT VM Start-at-boot Status"
    
    # Get storage VMs with their start-at-boot settings using existing function
    local storage_vms
    storage_vms=$(get_storage_vms)
    
    log_debug " get_storage_vms output: $storage_vms"
    log_debug " PRODUCT variable value: '$PRODUCT'"
    
    if [ -n "$storage_vms" ]; then
        log_debug " storage_vms is not empty, processing entries"
        local vm_count=0
        local matched_count=0
        
        echo "$storage_vms" | while IFS='|' read -r vmid vm_name vm_type start_at_boot order up down conf_file; do
            vm_count=$((vm_count + 1))
            log_debug " Processing VM #$vm_count: vmid='$vmid' vm_name='$vm_name' vm_type='$vm_type' start_at_boot='$start_at_boot' order='$order' up='$up' down='$down'"
            
            # Case-insensitive pattern matching
            if [[ "${vm_name,,}" =~ ${PRODUCT,,} ]]; then
                matched_count=$((matched_count + 1))
                log_debug " VM '$vm_name' matches PRODUCT pattern '$PRODUCT' (match #$matched_count)"
                
                local vm_status="$PASSED"
                
                # Set defaults for missing values
                [ -z "$up" ] && up="300"
                [ -z "$down" ] && down="180"
                
                log_debug " Final values - order='$order' up='$up' down='$down'"
                
                # Determine overall status - PASSED only if all settings match factory defaults
                if [ "$start_at_boot" != "yes" ]; then
                    vm_status="$DISABLED"
                    log_debug " VM status set to DISABLED because start_at_boot='$start_at_boot' != 'yes'"
                elif [ -z "$order" ] || [ "$order" != "$storage_vm_start_shutdown_order" ] || [ "$up" != "$storage_vm_startup_deleay" ] || [ "$down" != "$storage_vm_shutdown_timeout" ]; then
                    vm_status="$FAILED"
                    log_debug " VM status set to FAILED because settings don't match factory defaults - order='$order' (expected '$storage_vm_start_shutdown_order'), up='$up' (expected '$storage_vm_startup_deleay'), down='$down' (expected '$storage_vm_shutdown_timeout')"
                else
                    vm_status="$PASSED"
                    log_debug " VM status set to PASSED because all settings match factory defaults"
                fi
                
                # Format with right-aligned status - use the VM header line for alignment
                local header_line="   ${vm_type}:${vmid} ${vm_name}"
                local header_length=${#header_line}
                local status_length=${#vm_status}
                local total_width=94
                local padding=$((total_width - header_length - status_length))
                
                if [ $padding -lt 1 ]; then
                    padding=1
                fi
                
                log_debug " Writing VM data to temp file: $temp_file"
                
                # Write the header with status, then the details
                printf "%s%*s%s\n" "$header_line" $padding "" "$vm_status" >> "$temp_file"
                printf "     Start-at-boot: %s\n" "$start_at_boot" >> "$temp_file"
                printf "     Start/shutdown order: %s\n" "${order:-any}" >> "$temp_file"
                printf "     Startup delay: %s seconds\n" "$up" >> "$temp_file"
                printf "     Shutdown timeout: %s seconds\n" "$down" >> "$temp_file"
                printf "\n" >> "$temp_file"
            else
                log_debug " VM '$vm_name' does NOT match PRODUCT pattern '$PRODUCT'"
            fi
        done
        
        log_debug " Processed $vm_count total VMs, $matched_count matched PRODUCT pattern"
        
        if [ -f "$temp_file" ]; then
            local temp_content
            temp_content=$(cat "$temp_file")
            log_debug " temp_file exists and contains: $temp_content"
            summary="${summary}$(cat "$temp_file")\n"
            rm -f "$temp_file"
        else
            log_debug " temp_file does not exist or is empty"
            summary="${summary}   No $PRODUCT VMs found or no startup configuration data available\n"
        fi
    else
        log_debug " storage_vms is empty - no VM data returned from get_storage_vms"
        summary="${summary}   No VM data available from get_storage_vms function\n"
    fi
    
    log_debug " Point 3 checkup completed"
    
    summary="${summary}\n"
    
    # 4. NFS tuning STATUS
    summary="${summary}5. NFS Tuning Status:\n"
    
    # Get NFS storages using existing function
    local nfs_storages
    nfs_storages=$(get_nfs_storages)
    local nfs_overall_status="$PASSED"
    
    if [ -n "$nfs_storages" ]; then
        echo "$nfs_storages" | while read -r storage; do
            [ -z "$storage" ] && continue
            
            # Get current configuration using existing functions
            local config_nconnect config_sync
            config_nconnect=$(get_current_nconnect "$storage")
            config_sync=$(get_current_sync "$storage")
            
            # Set defaults if empty
            [ -z "$config_nconnect" ] && config_nconnect="1"
            [ -z "$config_sync" ] && config_sync="sync"
            
            # Determine status for this storage
            local storage_status="$PASSED"
            local details=""
            
            # Check if nconnect is in valid range (1-16)
            if [ "$config_nconnect" != "default" ] && [ "$config_nconnect" -gt 16 -o "$config_nconnect" -lt 1 ] 2>/dev/null; then
                storage_status="$FAILED"
                details="${details}nconnect out of range(1-16) "
            fi
            
            # Check if sync option is async
            if [ "$config_sync" = "async" ]; then
                storage_status="$FAILED"
                details="${details}async option "
            fi
            
            # Format the line with right-aligned status (total width ~94 chars for 100-wide dialog)
            local info_text="   ${storage}:  nconnect=${config_nconnect}  sync=${config_sync}"
            if [ -n "$details" ]; then
                info_text="${info_text} (${details})"
            fi
            
            # Calculate padding for right alignment
            local text_length=${#info_text}
            local status_length=${#storage_status}
            local total_width=94
            local padding=$((total_width - text_length - status_length))
            
            if [ $padding -lt 1 ]; then
                padding=1
            fi
            
            printf "%s%*s%s\n" "$info_text" $padding "" "$storage_status" >> "$temp_file"
        done
        
        if [ -f "$temp_file" ]; then
            summary="${summary}$(cat "$temp_file")\n"
            rm -f "$temp_file"
        fi
    fi
    
    summary="${summary}\n"
    
    # 5. Proxmox VE : ZFS import scan service STATUS
    summary="${summary}6. Proxmox VE ZFS Import Scan Service Status:\n"
    
    local zfs_status="$PASSED"
    local zfs_info="   disabled (as expected)"
    
    if check_zfs_import_scan_service; then
        zfs_status="$FAILED"
        zfs_info="   enabled (needs setup)"
    fi
    
    # Format with right-aligned status
    local info_text_length=${#zfs_info}
    local status_length=${#zfs_status}
    local total_width=94
    local padding=$((total_width - info_text_length - status_length))
    
    if [ $padding -lt 1 ]; then
        padding=1
    fi
    
    printf "%s%*s%s\n" "$zfs_info" $padding "" "$zfs_status" >> "$temp_file"
    
    if [ -f "$temp_file" ]; then
        summary="${summary}$(cat "$temp_file")\n"
        rm -f "$temp_file"
    fi
    
    summary="${summary}\n"
    
    # 6. Proxmox VE HA policy
    summary="${summary}7. Proxmox VE HA Policy:\n"
    
    local ha_policy_status="$PASSED"
    local ha_policy_info="   set to Migrate"
    
    if check_ha_policy_shutdown; then
        ha_policy_status="$FAILED"
        ha_policy_info="   not set to Migrate yet"
    fi
    
    # Format with right-aligned status
    local info_text_length=${#ha_policy_info}
    local status_length=${#ha_policy_status}
    local total_width=94
    local padding=$((total_width - info_text_length - status_length))
    
    if [ $padding -lt 1 ]; then
        padding=1
    fi
    
    printf "%s%*s%s\n" "$ha_policy_info" $padding "" "$ha_policy_status" >> "$temp_file"
    
    if [ -f "$temp_file" ]; then
        summary="${summary}$(cat "$temp_file")\n"
        rm -f "$temp_file"
    fi
    
    summary="${summary}\n"
    
    # 7. Proxmox VE Hardware watchdog
    summary="${summary}8. Proxmox VE Hardware Watchdog:\n"
    
    local watchdog_status="$PASSED"
    local watchdog_info="   configured"
    
    if check_watchdog_module; then
        watchdog_status="$MISSING"
        watchdog_info="   not configured yet"
    fi
    
    # Format with right-aligned status
    local info_text_length=${#watchdog_info}
    local status_length=${#watchdog_status}
    local total_width=94
    local padding=$((total_width - info_text_length - status_length))
    
    if [ $padding -lt 1 ]; then
        padding=1
    fi
    
    printf "%s%*s%s\n" "$watchdog_info" $padding "" "$watchdog_status" >> "$temp_file"
    
    if [ -f "$temp_file" ]; then
        summary="${summary}$(cat "$temp_file")\n"
        rm -f "$temp_file"
    fi
    
    # Display the summary
    echo -e "$summary" > "$temp_file"
    dialog --title "System Checkup Summary" \
           --ok-label "Close" \
           --textbox "$temp_file" 40 110
    
    rm -f "$temp_file"
    log_info "System checkup summary completed"
}

# Global variable to remember main menu selection across calls
MAIN_MENU_LAST_SELECTED="0"

# Global variable to remember last selected storage in delete unused NFS storage wizard
DELETE_UNUSED_NFS_LAST_SELECTED="1"

# Display main menu and handle user selections
main_menu() {
    local dialog_selected=$(mktemp)
    
    # Check background status
    check_background_status
    
    # Build menu title
    local menu_title="$VENDOR $PRODUCT Proxmox VE (PVE) Tools - Main Menu"

    cmd=(dialog --keep-tite --title "$menu_title"
         --ok-label "Select" --cancel-label "Exit" --extra-button --extra-label "Setup"
         --default-item "$MAIN_MENU_LAST_SELECTED"
         --menu "Choose an option:" 22 85 13)
    options=("0" "Add new NFS storage from $PRODUCT pool"
             "1" "Add cloned NFS storage from $PRODUCT datastore-snapshot"
             "2" "Restore VM/CT from cloned NFS storage"
             "3" "Move disk for VM/CT from cloned NFS storage to regular NFS storage"
             "4" "Delete cloned NFS storage in PVE and the cloned dataset in $PRODUCT"
             "5" "Delete unused NFS storage in PVE and the dataset in $PRODUCT"
             "6" "Clean up inactive NFS storage"
             "7" "Clean up unused disks in VM/CT configurations"
             "8" "Bulk VM/CT operations – start, stop, reboot, and more"
             "9" "NFS tuning – set nconnect, sync options"
             "10" "Add VM/CT to Proxmox VE HA resources"
             "11" "Set up auto-start at boot for $PRODUCT VMs"
             "12" "Apply known issue fixes and tuning"
             "13" "System check-up"
             "14" "Help & About pve-tools")

    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Select
            MAIN_MENU_LAST_SELECTED="$selected_option"  # Remember the selected option
            case "$selected_option" in
                0)  # Add new NFS storage from $PRODUCT pool
                    add_new_nfs_storage_wizard
                    ;;
                1)  # Add cloned NFS storage from $PRODUCT snapshot
                    add_nfs_storage_wizard
                    ;;
                2)  # Restore VM/CT from cloned NFS storage
                    restore_from_clone_wizard
                    ;;
                3)  # Move storage for VM/CT using cloned NFS storage
                    move_storage_cloned_nfs
                    ;;
                4)  # Delete cloned NFS storage in PVE and the cloned dataset in $PRODUCT
                    delete_cloned_nfs_storage_wizard
                    ;;
                5)  # Delete unused NFS storage in PVE and the dataset in $PRODUCT
                    delete_unused_nfs_storage_wizard
                    ;;
                6)  # Clean up inactive NFS storage
                    cleanup_inactive_nfs_storage
                    ;;
                7)  # Clean up unused disks in VM/CT configurations
                    cleanup_unused_disks_wizard
                    ;;
                8)  # Bulk VM/CT operations – start, stop, reboot, and more
                    vm_ct_manager_bulk_actions
                    ;;
                9)  # NFS tuning – set nconnect, sync options
                    nfs_tuning_wizard
                    ;;
                10)  # Add VM/CT to Proxmox VE HA resources
                    add_vmct_to_ha_resources
                    ;;
                11)  # Set up auto-start at boot for $PRODUCT VMs
                    setup_storage_vm_startup_wizard
                    ;;
                12)  # Apply known issue fixes and tuning
                    post_known_issues_fixes
                    ;;
                13)  # System check-up
                    checkup_summary
                    ;;
                14)  # Help & About pve-tools
                    help_about_dialog
                    ;;
            esac
            ;;
        1)  # Cancel
            clear
            exit 0
            ;;
        3)  # Setup
            setup_dialog
            ;;
    esac
    
    return 0
}

# Display Help & About dialog
help_about_dialog() {
    local help_text=""
    
    # Version and release info
    help_text+="pve-tools: version $VERSION\n"
    help_text+="Release Date: $RELEASE_DATE\n"
    help_text+="\n"
    
    # Log paths information
    help_text+="LOG PATHS:\n"
    help_text+="----------\n"
    help_text+="Main log file: $LOG_FILE\n"
    help_text+="Log directory: $LOGS_DIR\n"
    help_text+="Config file: $CONFIG_FILE\n"
    help_text+="\n"
    
    # Information from CLAUDE.md
    help_text+="PROGRAM OVERVIEW:\n"
    help_text+="-----------------\n"
    help_text+="PVE Tools is a bash script that provides Proxmox VE (PVE) integration\n"
    help_text+="with $VENDOR $PRODUCT storage. This tool enables management of NFS storage\n"
    help_text+="clones from $PRODUCT snapshots for VM/CT restore if missing, or clone VM/CT\n"
    help_text+="if original one is existing. The OODP (On Off-site Data Protection) must be\n"
    help_text+="configured and running in $PRODUCT, additionally the pve-config-backup\n"
    help_text+="daemon must be running on every Proxmox VE host. The pve-tools will\n"
    help_text+="automatically install and start the pve-config-backup daemon if missing.\n"
    help_text+="The pve-config-backup daemon provides a unique mechanism for instant backup\n"
    help_text+="of VM/CT configuration files. These backed-up configuration files are stored\n"
    help_text+="in the same directory as the associated virtual disks, simplifying backup\n"
    help_text+="management. Administrators familiar with virtualization platforms where all\n"
    help_text+="VM components reside in a single directory will appreciate the similar\n"
    help_text+="convenience provided in Proxmox VE. OODP provides continuous backup and\n"
    help_text+="disaster recovery, with intervals as short as one minute. Its highly\n"
    help_text+="efficient architecture minimizes system resource usage, enabling\n"
    help_text+="uninterrupted backup operations even under heavy workloads and in\n"
    help_text+="environments hosting thousands of virtual machines (VMs) or containers (CTs).\n"
    help_text+="\n"
    help_text+="MAIN FUNCTIONS:\n"
    help_text+="---------------\n"
    help_text+="0. Add new NFS storage from $PRODUCT pool\n"
    help_text+="1. Add cloned NFS storage from $PRODUCT datastore-snapshot\n"
    help_text+="2. Restore VM/CT from cloned NFS storage\n"
    help_text+="3. Move storage for VM/CT using cloned NFS storage\n"
    help_text+="4. Delete cloned NFS storage in PVE and the cloned dataset in $PRODUCT\n"
    help_text+="5. Delete unused NFS storage in PVE and the dataset in $PRODUCT\n"
    help_text+="6. Clean up inactive NFS storage\n"
    help_text+="7. Clean up unused disks in VM/CT configurations\n"
    help_text+="8. Bulk VM/CT operations – start, stop, reboot, and more\n"
    help_text+="9. NFS tuning – set nconnect, sync options\n"
    help_text+="10. Add VM/CT to Proxmox VE HA resources\n"
    help_text+="11. Set up auto-start at boot for $PRODUCT VMs\n"
    help_text+="12. Apply known issue fixes and tuning\n"
    help_text+="13. System check-up\n"
    help_text+="14. Help & About pve-tools\n"
    help_text+="\n"
    help_text+="REQUIREMENTS:\n"
    help_text+="-------------\n"
    help_text+="- Proxmox VE cluster environment\n"
    help_text+="- $VENDOR $PRODUCT storage system\n"
    help_text+="- Dialog package for TUI interface\n"
    help_text+="- REST API access to $PRODUCT\n"
    help_text+="\n"
    help_text+="STARTUP FEATURES:\n"
    help_text+="-----------------\n"
    help_text+="- Automatic software update checking from GitHub repository\n"
    help_text+="- Automatic pve-config-backup daemon detection on startup\n"
    help_text+="- Auto-installation of pve-config-backup from GitHub when not found\n"
    help_text+="- Background status monitoring of cluster backup daemons\n"
    help_text+="\n"
    help_text+="SETUP CONFIGURATION:\n"
    help_text+="--------------------\n"
    help_text+="- REST API credentials and connection settings\n"
    help_text+="- Storage VM startup and shutdown configuration\n"
    help_text+="- Storage server and NFS-storage naming rules\n"
    help_text+="- Concurrent VM disk move count optimization\n"
    help_text+="- HA cluster size settings\n"
    help_text+="- OODP backup destination and scheduling configuration\n"
    
    dialog --keep-tite --title "Help & About pve-tools" \
           --ok-label "Close" \
           --msgbox "$help_text" 35 80
    
    return 0
}

# Display setup dialog menu
setup_dialog() {
    local last_selected_option="1"  # Default to first option initially
    
    while true; do
        local dialog_selected=$(mktemp)
        
        cmd=(dialog --keep-tite --title "Setup Configuration"
             --ok-label "Select" --cancel-label "Back" 
             --default-item "$last_selected_option"
             --menu "Choose setup option:" 18 70 6)
        options=("1" "REST API Configuration"
                 "2" "Storage VM Startup Settings"
                 "3" "Storage server and NFS-storage names rules"
                 "4" "Move Storage: Concurrent VM Disk Move Count"
                 "5" "Number of Proxmox VE hosts in HA cluster"
                 "6" "OODP Configuration")

        dialog_menu
        rm -f "$dialog_selected"

        case $dialog_exit_code in
            0)  # Select
                last_selected_option="$selected_option"  # Remember the selected option
                case "$selected_option" in
                    1)  # REST API Configuration
                        setup_rest_api_dialog
                        ;;
                    2)  # Storage VM Startup Settings
                        setup_storage_vm_dialog
                        ;;
                    3)  # Storage server and NFS-storage names rules
                        setup_storage_naming_dialog
                        ;;
                    4)  # Move Storage: Concurrent VM Disk Move Count
                        setup_disk_move_dialog
                        ;;
                    5)  # Number of Proxmox VE hosts in HA cluster
                        setup_ha_cluster_size_dialog
                        ;;
                    6)  # OODP Configuration
                        setup_oodp_dialog
                        ;;
                esac
                # After any setup function completes, loop back to setup menu with last option highlighted
                ;;
            1)  # Back
                return 0
                ;;
        esac
    done
}

# Display setup dialog for REST API configuration
setup_rest_api_dialog() {
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)
    local temp_user="$rest_api_user"
    local temp_password="${rest_api_password}"
    local temp_port="$rest_api_port"

    # Step 1: Get username and port
    dialog --keep-tite --title "REST API Configuration - Step 1/2" \
           --ok-label "Next" --cancel-label "Back" \
           --form "Configure $PRODUCT REST API settings:" 10 60 2 \
           "User:" 1 1 "$temp_user" 1 20 20 0 \
           "Port:" 2 1 "$temp_port" 2 20 10 0 \
           2> "$form_data"

    local exit_code=$?

    if [ $exit_code -eq 0 ]; then
        # Parse username and port
        local line_count=1
        while IFS= read -r line; do
            case $line_count in
                1) temp_user="$line" ;;
                2) temp_port="$line" ;;
            esac
            ((line_count++))
        done < "$form_data"

        # Step 2: Get password with masking
        local masked_password=$(printf '%*s' ${#temp_password} | tr ' ' '*')
        dialog --keep-tite --title "REST API Configuration - Step 2/2" \
               --ok-label "Save" --cancel-label "Back" \
               --insecure --passwordbox "Enter REST API password (supports copy&paste):\n" 12 70 \
               2> "$form_data"

        exit_code=$?

        if [ $exit_code -eq 0 ]; then
            temp_password=$(cat "$form_data")
            
            # Only update if password is not empty
            if [ -n "${temp_password}" ]; then
                rest_api_user="$temp_user"
                rest_api_password="${temp_password}"
                rest_api_port="$temp_port"

                log_info "User saved REST API configuration changes"
                log_debug "setup_rest_api: Parsed user='$rest_api_user'"
                log_debug "setup_rest_api: Parsed password length=${#rest_api_password}"
                log_debug "setup_rest_api: Parsed port='$rest_api_port'"

                save_config_file
                dialog --msgbox "REST API configuration saved successfully!" 6 50
            else
                dialog --msgbox "Password cannot be empty. Configuration not saved." 6 50
            fi
        fi
    fi
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# Display setup dialog for storage VM startup settings
setup_storage_vm_dialog() {
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)
    local temp_start_at_boot="$storage_vm_start_at_boot"
    local temp_start_order="$storage_vm_start_shutdown_order"
    local temp_startup_delay="$storage_vm_startup_deleay"
    local temp_shutdown_timeout="$storage_vm_shutdown_timeout"
    local temp_boot_order="$storage_vm_boot_order"

    log_debug "setup_storage_vm_dialog: temp_start_at_boot='$temp_start_at_boot'"
    log_debug "setup_storage_vm_dialog: temp_start_order='$temp_start_order'"
    log_debug "setup_storage_vm_dialog: temp_startup_delay='$temp_startup_delay'"
    log_debug "setup_storage_vm_dialog: temp_shutdown_timeout='$temp_shutdown_timeout'"
    log_debug "setup_storage_vm_dialog: temp_boot_order='$temp_boot_order'"

    dialog --keep-tite --title "Storage VM Startup Configuration" \
           --ok-label "Save" --cancel-label "Back" --extra-button --extra-label "Restore to Factory" \
           --form "Configure storage VM startup settings:" 14 85 5 \
           "Start-at-boot (yes/no):" 1 1 "$temp_start_at_boot" 1 50 10 0 \
           "Start/shutdown order:" 2 1 "$temp_start_order" 2 50 10 0 \
           "Startup delay (sec):" 3 1 "$temp_startup_delay" 3 50 10 0 \
           "Shutdown timeout (sec):" 4 1 "$temp_shutdown_timeout" 4 50 10 0 \
           "Boot order (e.g., scsi0, ide0):" 5 1 "$temp_boot_order" 5 50 20 0 \
           2> "$form_data"

    local exit_code=$?

    if [ $exit_code -eq 0 ]; then
        # Parse form data
        local line_count=1
        while IFS= read -r line; do
            case $line_count in
                1) temp_start_at_boot="$line" ;;
                2) temp_start_order="$line" ;;
                3) temp_startup_delay="$line" ;;
                4) temp_shutdown_timeout="$line" ;;
                5) temp_boot_order="$line" ;;
            esac
            ((line_count++))
        done < "$form_data"

        # Validate inputs
        if [[ "$temp_start_at_boot" != "yes" && "$temp_start_at_boot" != "no" ]]; then
            dialog --msgbox "Start-at-boot must be 'yes' or 'no'." 6 40
        elif [[ ! "$temp_start_order" =~ ^[0-9]+$ ]]; then
            dialog --msgbox "Start/shutdown order must be a number." 6 40
        elif [[ ! "$temp_startup_delay" =~ ^[0-9]+$ ]]; then
            dialog --msgbox "Startup delay must be a number." 6 40
        elif [[ ! "$temp_shutdown_timeout" =~ ^[0-9]+$ ]]; then
            dialog --msgbox "Shutdown timeout must be a number." 6 40
        elif [[ -z "$temp_boot_order" ]]; then
            dialog --msgbox "Boot order cannot be empty (e.g., scsi0, ide0, net0)." 6 50
        elif [[ ! "$temp_boot_order" =~ ^(scsi|ide|sata|virtio|net)[0-9]+$ ]]; then
            dialog --msgbox "Boot order must be a valid device (e.g., scsi0, ide0, sata0, virtio0, net0)." 6 60
        else
            # Update variables
            storage_vm_start_at_boot="$temp_start_at_boot"
            storage_vm_start_shutdown_order="$temp_start_order"
            storage_vm_startup_deleay="$temp_startup_delay"
            storage_vm_shutdown_timeout="$temp_shutdown_timeout"
            storage_vm_boot_order="$temp_boot_order"

            log_info "User saved storage VM startup configuration changes"
            save_config_file
            dialog --msgbox "Storage VM startup configuration saved successfully!" 6 60
        fi
    elif [ $exit_code -eq 3 ]; then
        # Restore to Factory button pressed
        if restore_factory_defaults_storage_vm; then
            # Reload dialog with factory defaults
            setup_storage_vm_dialog
            return $?
        fi
    fi
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# Display setup dialog for storage server and NFS-storage names rules  
setup_storage_naming_dialog() {
    local start_step="${1:-1}"  # Default to step 1 if no parameter provided
    local temp_name_contains="${2:-$storage_vm_name_contains}"
    local temp_number_pattern="${3:-$new_NFS_storage_volume_running_number_starts_with}"
    local temp_dataset_prefix="${4:-$dataset_auto_prefix}"
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)

    log_debug "setup_storage_naming_dialog: start_step=$start_step"
    log_debug "setup_storage_naming_dialog: temp_name_contains='$temp_name_contains'"
    log_debug "setup_storage_naming_dialog: temp_number_pattern='$temp_number_pattern'"
    log_debug "setup_storage_naming_dialog: temp_dataset_prefix='$temp_dataset_prefix'"

    case $start_step in
        1)
            # Step 1: Get JovianDSS VM name contains
            while true; do
                dialog --keep-tite --title "Storage server and NFS-storage names rules" \
                       --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "Restore to Factory" \
                       --inputbox "Configure storage server and NFS-storage naming rules:\n\n$PRODUCT VM name contains (case insensitive):\nAllowed: letters, numbers, underscore (_), hyphen (-)" 14 70 "$temp_name_contains" \
                       2> "$form_data"

                local exit_code=$?

                if [ $exit_code -eq 0 ]; then
                    # Parse form data and validate
                    temp_name_contains=$(< "$form_data")
                    
                    if validate_alphanumeric_name "$temp_name_contains"; then
                        # Valid input, proceed to step 2
                        setup_storage_naming_dialog 2 "$temp_name_contains" "$temp_number_pattern" "$temp_dataset_prefix"
                        return $?
                    else
                        dialog --msgbox "Invalid input!\n\nOnly letters, numbers, underscore (_), and hyphen (-) are allowed.\nSpaces and other characters are not permitted." 10 60
                        # Continue the loop to show the dialog again
                    fi
                elif [ $exit_code -eq 3 ]; then
                    # Restore to Factory button pressed from step 1
                    if restore_factory_defaults_storage_vm; then
                        # Update temp variables with restored factory defaults and stay on step 1
                        temp_name_contains="$storage_vm_name_contains"
                        temp_number_pattern="$new_NFS_storage_volume_running_number_starts_with" 
                        temp_dataset_prefix="$dataset_auto_prefix"
                        # Continue the loop to show step 1 again with factory defaults
                    fi
                else
                    # User pressed Back or Cancel
                    rm -f "$form_data" "$dialog_selected"
                    return 0
                fi
            done
            ;;
        2)
            # Step 2: Get NFS storage volume running number pattern
            dialog --keep-tite --title "Storage server and NFS-storage names rules" \
                   --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "Restore to Factory" \
                   --default-item "$temp_number_pattern" \
                   --menu "Select NFS-storage volume running number starts with:" 15 60 6 \
                   "00" "Pattern: joviandss-nfs00, joviandss-nfs01, ..." \
                   "01" "Pattern: joviandss-nfs01, joviandss-nfs02, ..." \
                   "000" "Pattern: joviandss-nfs000, joviandss-nfs001, ..." \
                   "001" "Pattern: joviandss-nfs001, joviandss-nfs002, ..." \
                   "10" "Pattern: joviandss-nfs10, joviandss-nfs11, ..." \
                   "100" "Pattern: joviandss-nfs100, joviandss-nfs101, ..." \
                   2> "$dialog_selected"

            local menu_exit_code=$?
            local selected_pattern=$(< "$dialog_selected")

            if [ $menu_exit_code -eq 0 ]; then
                temp_number_pattern="$selected_pattern"
                # Proceed to step 3
                setup_storage_naming_dialog 3 "$temp_name_contains" "$temp_number_pattern" "$temp_dataset_prefix"
                return $?
            elif [ $menu_exit_code -eq 3 ]; then
                # Restore to Factory button pressed from step 2
                if restore_factory_defaults_storage_vm; then
                    # Stay on step 2 with factory defaults
                    setup_storage_naming_dialog 2 "$storage_vm_name_contains" "$new_NFS_storage_volume_running_number_starts_with" "$dataset_auto_prefix"
                    return $?
                fi
            else
                # User pressed Back or Cancel from step 2, go back to step 1
                setup_storage_naming_dialog 1 "$temp_name_contains" "$temp_number_pattern" "$temp_dataset_prefix"
                return $?
            fi
            ;;
        3)
            # Step 3: Get dataset auto prefix
            while true; do
                dialog --keep-tite --title "Storage server and NFS-storage names rules" \
                       --ok-label "Save" --cancel-label "Back" --extra-button --extra-label "Restore to Factory" \
                       --inputbox "Dataset auto prefix:\nThis prefix is used for automatically generated dataset names.\n\nExample: ${temp_dataset_prefix}-01, ${temp_dataset_prefix}-02, ...\n\nAllowed: letters, numbers, underscore (_), hyphen (-)" 16 70 "$temp_dataset_prefix" \
                       2> "$form_data"

                local dataset_exit_code=$?

                if [ $dataset_exit_code -eq 0 ]; then
                    # Parse form data and validate
                    temp_dataset_prefix=$(< "$form_data")
                    
                    if validate_alphanumeric_name "$temp_dataset_prefix"; then
                        # All steps completed successfully, save configuration
                        storage_vm_name_contains="$temp_name_contains"
                        new_NFS_storage_volume_running_number_starts_with="$temp_number_pattern"
                        dataset_auto_prefix="$temp_dataset_prefix"

                        log_info "User saved storage server and NFS-storage naming configuration changes"
                        save_config_file
                        dialog --msgbox "Storage server and NFS-storage naming rules saved successfully!" 6 70
                        break
                    else
                        dialog --msgbox "Invalid dataset prefix!\n\nOnly letters, numbers, underscore (_), and hyphen (-) are allowed.\nSpaces and other characters are not permitted." 10 60
                        # Continue the loop to show the dialog again
                    fi
                elif [ $dataset_exit_code -eq 3 ]; then
                    # Restore to Factory button pressed from step 3
                    if restore_factory_defaults_storage_vm; then
                        # Update temp variables with restored factory defaults and stay on step 3
                        temp_dataset_prefix="$dataset_auto_prefix"
                        # Continue the loop to show step 3 again with factory defaults
                    fi
                else
                    # User pressed Back or Cancel from step 3, go back to step 2
                    setup_storage_naming_dialog 2 "$temp_name_contains" "$temp_number_pattern" "$temp_dataset_prefix"
                    return $?
                fi
            done
            ;;
    esac
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# Display setup dialog for disk move configuration
setup_disk_move_dialog() {
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)
    local temp_count="$concurrent_vm_disk_move_count"
    
    # Create dialog with inputbox
    dialog --keep-tite --title "Move Storage: Concurrent VM Disk Move Count" \
           --ok-label "Save" --cancel-label "Back" \
           --inputbox "This setting controls how many VMs can be migrated simultaneously during storage move operations.\n\nHigher values allow faster migration but consume more system resources.\nRecommended values: 1-4 (default: 2)\n\nEnter concurrent VM disk move count:" 14 70 "$temp_count" \
           2> "$form_data"
    
    local exit_code=$?
    
    if [ $exit_code -eq 0 ]; then
        temp_count=$(cat "$form_data")
        
        # Validate input (must be a positive integer)
        if [[ "$temp_count" =~ ^[1-9][0-9]*$ ]]; then
            concurrent_vm_disk_move_count="$temp_count"
            log_info "User saved concurrent VM disk move count: $concurrent_vm_disk_move_count"
            save_config_file
            dialog --msgbox "Concurrent VM disk move count saved successfully!\n\nSet to: $concurrent_vm_disk_move_count VMs" 8 60
        else
            dialog --msgbox "Invalid input. Please enter a positive integer (1 or greater)." 8 60
        fi
    fi
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# Display setup dialog for custom IPs configuration
setup_custom_ips_dialog() {
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)
    local temp_custom_ips="$custom_ips"

    dialog --keep-tite --title "Custom IPs Configuration" \
           --ok-label "Save" --cancel-label "Back" \
           --inputbox "Enter custom IPs (comma-separated):" 10 60 "$temp_custom_ips" \
           2> "$form_data"

    local exit_code=$?

    if [ $exit_code -eq 0 ]; then
        temp_custom_ips=$(cat "$form_data")
        custom_ips="$temp_custom_ips"

        log_info "User saved custom IPs configuration changes"
        save_config_file
        dialog --msgbox "Custom IPs configuration saved successfully!" 6 50
    fi
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# Display setup dialog for HA cluster size configuration
setup_ha_cluster_size_dialog() {
    log_info "Starting HA cluster size configuration dialog"
    
    # Get current cluster size
    local all_nodes
    all_nodes=$(get_all_nodes)
    if [ -z "$all_nodes" ]; then
        dialog --msgbox "Error: No PVE cluster nodes found.\n\nPlease ensure this node is part of a Proxmox VE cluster." 10 60
        return 1
    fi
    
    # Count total nodes in cluster
    local total_cluster_nodes
    total_cluster_nodes=$(echo "$all_nodes" | wc -w)
    log_info "Detected $total_cluster_nodes nodes in PVE cluster: $all_nodes"
    
    if [ "$total_cluster_nodes" -lt 2 ]; then
        dialog --msgbox "Error: At least 2 nodes are required for HA configuration.\n\nCurrent cluster has only $total_cluster_nodes node(s)." 10 60
        return 1
    fi
    
    # Build menu options (2 to total_cluster_nodes)
    local menu_items=()
    local i
    for ((i=2; i<=total_cluster_nodes; i++)); do
        if [ "$i" -eq "$number_of_pve_hosts_in_ha_cluster" ]; then
            menu_items+=("$i" "$i nodes (current)")
        else
            menu_items+=("$i" "$i nodes")
        fi
    done
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "HA Cluster Size Configuration"
           --ok-label "Save" --cancel-label "Back"
           --menu "Select number of Proxmox VE hosts to include in HA cluster:" 15 60 $((total_cluster_nodes-1)))
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Save
            if [ -n "$selected_option" ]; then
                number_of_pve_hosts_in_ha_cluster="$selected_option"
                log_info "User changed HA cluster size to: $number_of_pve_hosts_in_ha_cluster hosts"
                save_config_file
                dialog --msgbox "HA cluster size configuration saved successfully!\n\nNumber of HA hosts: $number_of_pve_hosts_in_ha_cluster" 8 60
            fi
            ;;
        1)  # Back
            ;;
    esac
    
    return 0
}

# Display setup dialog for OODP configuration
setup_oodp_dialog() {
    local last_selected_option="1"  # Default to first option initially
    
    while true; do
        local dialog_selected=$(mktemp)
        
        cmd=(dialog --keep-tite --title "OODP Configuration"
             --ok-label "Select" --cancel-label "Back" 
             --default-item "$last_selected_option"
             --menu "Choose OODP setting to configure:" 14 70 3)
        options=("1" "Backup Destination Port (Current: ${oodp_backup_destination_port:-40000})"
                 "2" "Source Default Plan"
                 "3" "Destination Default Plan")
        
        dialog_menu
        rm -f "$dialog_selected"
        
        case $dialog_exit_code in
            0)  # Select
                last_selected_option="$selected_option"  # Remember the selected option
                case "$selected_option" in
                    1)  # Backup Destination Port
                        setup_oodp_port_dialog
                        ;;
                    2)  # Source Default Plan
                        setup_oodp_src_plan_dialog
                        ;;
                    3)  # Destination Default Plan
                        setup_oodp_dst_plan_dialog
                        ;;
                esac
                # After any OODP setup function completes, loop back to OODP setup menu with last option highlighted
                ;;
            1)  # Back
                return 0
                ;;
        esac
    done
}

# Setup OODP backup destination port
setup_oodp_port_dialog() {
    while true; do
        local temp_input=$(mktemp)
        local current_port="${oodp_backup_destination_port:-40000}"
        
        dialog --keep-tite --title "OODP Backup Destination Port" \
               --ok-label "Save" --cancel-label "Back" \
               --extra-button --extra-label "Restore to Factory" \
               --inputbox "Enter the OODP backup destination port:\n\nThis port is used for communication with backup/DR nodes.\n\nFactory Default: 40000\nCurrent: $current_port" 14 70 "$current_port" 2>"$temp_input"
        
        local result=$?
        local new_port=$(< "$temp_input")
        rm -f "$temp_input"
        
        case $result in
            0)  # Save
                # Validate port number
                if [[ "$new_port" =~ ^[0-9]+$ ]] && [[ $new_port -ge 1 && $new_port -le 65535 ]]; then
                    oodp_backup_destination_port="$new_port"
                    log_info "User changed OODP backup destination port to: $oodp_backup_destination_port"
                    save_config_file
                    dialog --msgbox "OODP backup destination port saved successfully!\n\nPort: $oodp_backup_destination_port" 8 60
                    return 0
                else
                    dialog --msgbox "Invalid port number!\n\nPort must be between 1 and 65535." 8 60
                fi
                ;;
            1)  # Back
                return 0
                ;;
            3)  # Restore to Factory
                dialog --keep-tite --title "Restore to Factory" \
                       --yes-label "Restore" --no-label "Cancel" \
                       --yesno "This will restore the OODP backup destination port to factory default.\n\nFactory Default: 40000\nCurrent: $current_port\n\nContinue?" 12 60
                
                if [[ $? -eq 0 ]]; then
                    oodp_backup_destination_port="40000"
                    log_info "User restored OODP backup destination port to factory default: $oodp_backup_destination_port"
                    save_config_file
                    dialog --msgbox "OODP backup destination port restored to factory default!\n\nPort: $oodp_backup_destination_port" 8 60
                fi
                # Stay in dialog - continue the loop
                ;;
        esac
    done
}

# Setup OODP source default plan
setup_oodp_src_plan_dialog() {
    while true; do
        local temp_input=$(mktemp)
        local current_plan="$oodp_src_default_plan"
        
        dialog --keep-tite --title "OODP Source Default Plan" \
               --ok-label "Save" --cancel-label "Back" \
               --extra-button --extra-label "Restore to Factory" \
               --inputbox "Enter the OODP source default plan:\n\nFormat: retention=>interval,retention=>interval,...\nExample: 1hour=>1minute,3days=>15minutes,2weeks=>1hour\n\nThis defines the snapshot schedule for source datasets.\n\nFactory Default: 1hour=>1minute,3days=>15minutes,2weeks=>1hour" 16 75 "$current_plan" 2>"$temp_input"
        
        local result=$?
        local new_plan=$(< "$temp_input")
        rm -f "$temp_input"
        
        case $result in
            0)  # Save
                if [[ -n "$new_plan" ]]; then
                    oodp_src_default_plan="$new_plan"
                    log_info "User changed OODP source default plan to: $oodp_src_default_plan"
                    save_config_file
                    dialog --msgbox "OODP source default plan saved successfully!\n\nPlan: $oodp_src_default_plan" 10 75
                    return 0
                else
                    dialog --msgbox "Plan cannot be empty!" 8 60
                fi
                ;;
            1)  # Back
                return 0
                ;;
            3)  # Restore to Factory
                local factory_default="1hour=>1minute,3days=>15minutes,2weeks=>1hour"
                dialog --keep-tite --title "Restore to Factory" \
                       --yes-label "Restore" --no-label "Cancel" \
                       --yesno "This will restore the OODP source default plan to factory default.\n\nFactory Default:\n$factory_default\n\nCurrent:\n$current_plan\n\nContinue?" 14 75
                
                if [[ $? -eq 0 ]]; then
                    oodp_src_default_plan="$factory_default"
                    log_info "User restored OODP source default plan to factory default: $oodp_src_default_plan"
                    save_config_file
                    dialog --msgbox "OODP source default plan restored to factory default!\n\nPlan: $oodp_src_default_plan" 10 75
                fi
                # Stay in dialog - continue the loop
                ;;
        esac
    done
}

# Setup OODP destination default plan
setup_oodp_dst_plan_dialog() {
    while true; do
        local temp_input=$(mktemp)
        local current_plan="$oodp_dst_default_plan"
        
        dialog --keep-tite --title "OODP Destination Default Plan" \
               --ok-label "Save" --cancel-label "Back" \
               --extra-button --extra-label "Restore to Factory" \
               --inputbox "Enter the OODP destination default plan:\n\nFormat: retention=>interval,retention=>interval,...\nExample: 12hours=>5minute,1week=>15minutes,3weeks=>1hour\n\nThis defines the retention policy for backup destinations.\n\nFactory Default: 12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months" 16 85 "$current_plan" 2>"$temp_input"
        
        local result=$?
        local new_plan=$(< "$temp_input")
        rm -f "$temp_input"
        
        case $result in
            0)  # Save
                if [[ -n "$new_plan" ]]; then
                    oodp_dst_default_plan="$new_plan"
                    log_info "User changed OODP destination default plan to: $oodp_dst_default_plan"
                    save_config_file
                    dialog --msgbox "OODP destination default plan saved successfully!\n\nPlan: $oodp_dst_default_plan" 10 85
                    return 0
                else
                    dialog --msgbox "Plan cannot be empty!" 8 60
                fi
                ;;
            1)  # Back
                return 0
                ;;
            3)  # Restore to Factory
                local factory_default="12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months"
                dialog --keep-tite --title "Restore to Factory" \
                       --yes-label "Restore" --no-label "Cancel" \
                       --yesno "This will restore the OODP destination default plan to factory default.\n\nFactory Default:\n$factory_default\n\nCurrent:\n$current_plan\n\nContinue?" 14 85
                
                if [[ $? -eq 0 ]]; then
                    oodp_dst_default_plan="$factory_default"
                    log_info "User restored OODP destination default plan to factory default: $oodp_dst_default_plan"
                    save_config_file
                    dialog --msgbox "OODP destination default plan restored to factory default!\n\nPlan: $oodp_dst_default_plan" 10 85
                fi
                # Stay in dialog - continue the loop
                ;;
        esac
    done
}

# Restore factory defaults for storage VM configuration
restore_factory_defaults_storage_vm() {
    log_info "Restoring factory defaults for storage VM configuration"
    
    if [ ! -f "$FACTORY_DEFAULTS_FILE" ]; then
        log_error "Factory defaults file not found: $FACTORY_DEFAULTS_FILE"
        dialog --title "Error" --msgbox "Factory defaults file not found. Cannot restore defaults." 8 60
        return 1
    fi
    
    # Confirm with user
    dialog --keep-tite --title "Restore Factory Defaults" \
           --yes-label "Restore" --no-label "Cancel" \
           --yesno "This will restore storage VM startup settings to factory defaults.\n\nCurrent settings will be lost. Continue?" 10 60
    
    if [ $? -ne 0 ]; then
        log_info "User canceled factory defaults restore"
        return 1
    fi
    
    # Load factory defaults for storage VM settings only
    local factory_storage_vm_name_contains=""
    local factory_new_NFS_storage_volume_running_number_starts_with=""
    local factory_dataset_auto_prefix=""
    local factory_storage_vm_start_at_boot=""
    local factory_storage_vm_start_shutdown_order=""
    local factory_storage_vm_startup_deleay=""
    local factory_storage_vm_shutdown_timeout=""
    local factory_storage_vm_boot_order=""
    
    # Source factory defaults and extract only storage VM settings
    if [ -f "$FACTORY_DEFAULTS_FILE" ]; then
        while IFS='=' read -r key value; do
            # Skip comments and empty lines
            [[ "$key" =~ ^[[:space:]]*# ]] && continue
            [[ -z "$key" ]] && continue
            
            case "$key" in
                storage_vm_name_contains)
                    factory_storage_vm_name_contains="$value"
                    ;;
                new_NFS_storage_volume_running_number_starts_with)
                    factory_new_NFS_storage_volume_running_number_starts_with="$value"
                    ;;
                dataset_auto_prefix)
                    factory_dataset_auto_prefix="$value"
                    ;;
                storage_vm_start_at_boot)
                    factory_storage_vm_start_at_boot="$value"
                    ;;
                storage_vm_start_shutdown_order)
                    factory_storage_vm_start_shutdown_order="$value"
                    ;;
                storage_vm_startup_deleay)
                    factory_storage_vm_startup_deleay="$value"
                    ;;
                storage_vm_shutdown_timeout)
                    factory_storage_vm_shutdown_timeout="$value"
                    ;;
                storage_vm_boot_order)
                    factory_storage_vm_boot_order="$value"
                    ;;
            esac
        done < "$FACTORY_DEFAULTS_FILE"
        
        # Update current variables with factory defaults
        storage_vm_name_contains="${factory_storage_vm_name_contains:-$(lower "$PRODUCT")}"
        new_NFS_storage_volume_running_number_starts_with="${factory_new_NFS_storage_volume_running_number_starts_with:-00}"
        dataset_auto_prefix="${factory_dataset_auto_prefix:-datastore-pve}"
        storage_vm_start_at_boot="${factory_storage_vm_start_at_boot:-yes}"
        storage_vm_start_shutdown_order="${factory_storage_vm_start_shutdown_order:-1}"
        storage_vm_startup_deleay="${factory_storage_vm_startup_deleay:-300}"
        storage_vm_shutdown_timeout="${factory_storage_vm_shutdown_timeout:-180}"
        storage_vm_boot_order="${factory_storage_vm_boot_order:-scsi0}"
        
        # Save to config file
        save_config_file
        
        log_info "Factory defaults restored for storage VM configuration"
        dialog --msgbox "Factory defaults restored successfully!" 6 50
        return 0
    else
        log_error "Failed to read factory defaults file"
        dialog --title "Error" --msgbox "Failed to read factory defaults file." 8 50
        return 1
    fi
}

# Save current configuration to file
save_config_file() {
    cat > "$CONFIG_FILE" << EOF
rest_api_user=$rest_api_user
rest_api_password='$rest_api_password'
rest_api_port=$rest_api_port
custom_ips=${custom_ips:-}
storage_vm_name_contains=${storage_vm_name_contains:-$(lower "$PRODUCT")}
new_NFS_storage_volume_running_number_starts_with=${new_NFS_storage_volume_running_number_starts_with:-00}
dataset_auto_prefix=${dataset_auto_prefix:-datastore-pve}
storage_vm_start_at_boot=${storage_vm_start_at_boot:-yes}
storage_vm_start_shutdown_order=${storage_vm_start_shutdown_order:-1}
storage_vm_startup_deleay=${storage_vm_startup_deleay:-300}
storage_vm_shutdown_timeout=${storage_vm_shutdown_timeout:-180}
storage_vm_boot_order=${storage_vm_boot_order:-scsi0}
concurrent_vm_disk_move_count=${concurrent_vm_disk_move_count:-2}
number_of_pve_hosts_in_ha_cluster=${number_of_pve_hosts_in_ha_cluster:-2}
oodp_backup_destination_port=${oodp_backup_destination_port:-40000}
oodp_src_default_plan='${oodp_src_default_plan:-1hour=>1minute,3days=>15minutes,2weeks=>1hour}'
oodp_dst_default_plan='${oodp_dst_default_plan:-12hours=>5minute,1week=>15minutes,3weeks=>1hour,12hours=>3months}'
EOF
    log_info "Configuration saved to $CONFIG_FILE"
}

# RESTORE FROM CLONE WIZARD FUNCTIONS

get_ha_nodes() {
    local ha_nodes=""
    local MAJOR
    MAJOR="$(get_pve_major)"
    
    if [ "$MAJOR" -ge 9 ]; then
        # PVE 9+: Extract nodes from node-affinity rule
        log_debug "get_ha_nodes: Using PVE 9+ node-affinity rule method"
        
        # Check if rules.cfg exists and contains ha-nodes rule
        if [ -f /etc/pve/ha/rules.cfg ]; then
            # Extract nodes from the ha-nodes rule
            # Format in file is like:
            # node-affinity: ha-nodes
            #     nodes pve1,pve2  (or nodes pve1:2,pve2:1)
            local nodes_line
            nodes_line=$(sed -n '/^node-affinity: ha-nodes$/,/^[^ \t]/p' /etc/pve/ha/rules.cfg | grep '^\s*nodes\s' | head -1)
            
            if [ -n "$nodes_line" ]; then
                # Extract node names, removing priorities if present (pve1:2 -> pve1)
                ha_nodes=$(echo "$nodes_line" | sed 's/^\s*nodes\s*//' | tr ',' '\n' | sed 's/:.*$//' | sort)
                log_debug "get_ha_nodes: Extracted nodes from rules.cfg: $ha_nodes"
            else
                log_debug "get_ha_nodes: No nodes line found in ha-nodes rule"
            fi
        else
            log_debug "get_ha_nodes: No rules.cfg file found"
        fi
    else
        # PVE 8 and older: Extract from HA groups
        log_debug "get_ha_nodes: Using PVE 8 HA groups method"
        if [ -f /etc/pve/ha/groups.cfg ]; then
            ha_nodes=$(sed -n '/^group: ha-nodes$/,/^group:/p' /etc/pve/ha/groups.cfg | \
                grep -E '\s+nodes\s' | \
                cut -d' ' -f2 | tr ',' '\n' | sed 's/:.*$//' | sort)
            log_debug "get_ha_nodes: Extracted nodes from group: $ha_nodes"
        else
            log_debug "get_ha_nodes: No groups.cfg file found"
        fi
    fi
    
    # If no ha-nodes configuration exists, return all cluster nodes
    if [[ -n "$ha_nodes" ]]; then
        echo "$ha_nodes"
    else
        log_debug "get_ha_nodes: No ha-nodes configuration, returning all cluster nodes"
        pvecm nodes 2>/dev/null | grep -E '\s+[0-9]+\s+[0-9]+\s+\S+' | awk '{print $3}' | sort
    fi
}

# Determine configuration file type (VM or Container)
config_file_type() {
    local config_file="$1"
    
    if [ -z "$config_file" ]; then
        log_error "config_file_type: No file path provided"
        return 1
    fi
    
    if [ ! -f "$config_file" ]; then
        log_error "config_file_type: File not found: $config_file"
        return 1
    fi
    
    # Check for VM-specific configuration
    if grep -q '^vmgenid:' "$config_file"; then
        echo "vm"
    # Check for Container-specific configuration
    elif grep -q '^rootfs:' "$config_file"; then
        echo "ct"
    else
        echo "unknown"
    fi
    
    return 0
}

# List existing VM/LXC IDs from Proxmox VE configuration files
existing_id() {
    log_debug "Getting existing VM/CT IDs from Proxmox VE configuration"
    
    find /etc/pve/nodes/*/{qemu-server,lxc} -name '*.conf' \
        -printf '%f\n' 2>/dev/null | \
    sed 's/\.conf$//' | \
    sort -n | \
    uniq
}

# Check if VM/CT ID exists in PVE repository
check_vmct_exists() {
    local vmid="$1"
    
    if [ -z "$vmid" ]; then
        log_error "check_vmct_exists: No VM/CT ID provided"
        return 1
    fi
    
    log_debug "Checking if VM/CT ID $vmid exists in PVE repository"
    
    # Check if config file exists in any node
    if find /etc/pve/nodes/*/{qemu-server,lxc} -name "${vmid}.conf" 2>/dev/null | grep -q .; then
        log_debug "VM/CT ID $vmid exists in PVE repository"
        return 0
    else
        log_debug "VM/CT ID $vmid does not exist in PVE repository"
        return 1
    fi
}

# Get next available VM/CT ID
get_available_vmct_id() {
    local start_id="${1:-100}"
    local current_id="$start_id"
    
    log_debug "Finding next available VM/CT ID starting from $start_id"
    
    # Get list of existing IDs
    local existing_ids
    existing_ids=$(existing_id)
    
    # Find first available ID
    while check_vmct_exists "$current_id"; do
        ((current_id++))
    done
    
    log_debug "Next available VM/CT ID: $current_id"
    echo "$current_id"
    return 0
}

# List all configuration files in mounted PVE storage
all_mnt_confs() {
    log_debug "Getting all mounted configuration files"
    
    find /mnt/pve/*/images/* -maxdepth 1 -name '*.conf' 2>/dev/null | sort
    
    return 0
}


get_id_from_conf_file() {
  local conf="$1"
  awk -F/ '{ sub(/\.conf$/, "", $NF); print $NF }' <<< "$conf"
}

get_storage_name_from_mnt_conf_file() {
  local conf="$1"
  # the storage name is the 3rd path component (after “mnt” and “pve”), which is $4 when splitting on “/”
  awk -F/ '{ print $4 }' <<< "$conf"
}

get_etc_conf_file_for_id() {
    local vmid="$1"
    local conf

    # Check in both qemu-server and lxc dirs
    for conf in /etc/pve/nodes/*/qemu-server/"${vmid}".conf /etc/pve/nodes/*/lxc/"${vmid}".conf; do
        if [[ -f "$conf" ]]; then
            echo "$conf"
            return 0
        fi
    done

    # If we get here, no file was found
    log_error "get_etc_conf_file_for_id: no config file found for VMID ${vmid}"
    return 1
}

get_storage_name_of_id() {
    local id="$1"
    local etc_conf conf_type storage_name

    # 1) locate the /etc/pve config file
    etc_conf=$(get_etc_conf_file_for_id "$id") || return 1

    # 2) determine if it's a CT or VM
    conf_type=$(config_file_type "$etc_conf") || return 1

    case "$conf_type" in
        ct)
            # parse the storage name from rootfs:
            storage_name=$(
                awk -F: '/^rootfs:/ {print $2}' "$etc_conf" \
                | awk -F: '{print $1}' \
                | xargs
            )
            echo "$storage_name"
            ;;
        vm)
            # use the comprehensive device pattern to find the first VM disk line
            storage_name=$(
                grep -P "^(?:scsi|ide|sata|virtio|efidisk|tpmstate|nvme)[0-9]+:[^:]+:[0-9]+/vm" "$etc_conf" \
                | head -n1 \
                | awk -F: '{print $2}' \
                | xargs
            )
            echo "$storage_name"
            ;;
        *)
            log_error "get_storage_name_of_id: unknown config type '$conf_type' in $etc_conf"
            return 1
            ;;
    esac

    return 0
}

is_mnt_conf_file_form_existing_vmct() {
    local mnt_conf_file="$1"
    local vmid etc_conf storage_etc storage_mnt

    # 1) sanity check
    if [[ -z "$mnt_conf_file" ]]; then
        log_error "get_storage_name_from_mnt_conf_file: Usage: $FUNCNAME /mnt/pve/.../<vmid>.conf"
        return 2
    fi
    if [[ ! -f "$mnt_conf_file" ]]; then
        log_error "get_storage_name_from_mnt_conf_file: File not found: $mnt_conf_file"
        return 1
    fi

    # 2) extract ID
    vmid=$(get_id_from_conf_file "$mnt_conf_file")

    # 3) find /etc/pve conf for that ID
    etc_conf=$(get_etc_conf_file_for_id "$vmid") || return 1

    # 4) get the storage name from the /etc/pve config
    storage_etc=$(get_storage_name_of_id "$vmid") || return 1

    # 5) get the storage name from the mounted .conf path
    storage_mnt=$(get_storage_name_from_mnt_conf_file "$mnt_conf_file")

    # 6) compare and return
    if [[ "$storage_etc" == "$storage_mnt" ]]; then
        return 0
    else
        return 1
    fi
}


# this is NEW get to restore func
get_cloned_config_file_list() {
    local config_files=()
    
    log_debug "get_cloned_config_file_list: Starting config file discovery"
    
    for storage in $(get_cloned_storage_list); do
        log_debug "get_cloned_config_file_list: Processing storage '$storage'"
        
        # Find VM disk files and extract VM IDs
        for disk_file in /mnt/pve/"$storage"/images/*/vm-[0-9]*-disk-*; do
            log_debug "get_cloned_config_file_list: Checking disk file '$disk_file'"
            if [[ -f "$disk_file" ]]; then
                log_debug "get_cloned_config_file_list: Found valid disk file '$disk_file'"
                # Extract VM ID from disk filename (vm-201-disk-0.qcow2 -> 201)
                local vm_id=$(basename "$disk_file" | sed -n 's/vm-\([0-9]\+\)-disk-.*/\1/p')
                log_debug "get_cloned_config_file_list: Extracted VM ID '$vm_id' from disk file"
                if [[ -n "$vm_id" ]]; then
                    # Look for corresponding config file in the same directory
                    local vm_dir=$(dirname "$disk_file")
                    local config_file="$vm_dir/$vm_id.conf"
                    log_debug "get_cloned_config_file_list: Looking for config file '$config_file'"
                    if [[ -f "$config_file" ]]; then
                        log_debug "get_cloned_config_file_list: Adding config file '$config_file' to list"
                        config_files+=("$config_file")
                    else
                        log_debug "get_cloned_config_file_list: Config file '$config_file' not found"
                    fi
                else
                    log_debug "get_cloned_config_file_list: Could not extract VM ID from '$disk_file'"
                fi
            else
                log_debug "get_cloned_config_file_list: Disk file '$disk_file' does not exist or is not a regular file"
            fi
        done
    done
    
    log_debug "get_cloned_config_file_list: Found ${#config_files[@]} config files before deduplication"
    
    # Remove duplicates and sort
    printf '%s\n' "${config_files[@]}" | sort | uniq
}


# Function to get VM/CT IDs that are using cloned NFS storage
get_vmid_ctid_using_cloned_nfs_storage_list() {
    local vmct_list=()
    local vmct_info
    
    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Starting VM/CT scan for cloned storage usage"
    
    # Get all VM/CT from cluster resources using optimized function
    local vm_list
    vm_list=$(get_all_vm_ct_info | awk -F'|' '{print $1"|"$2"|"$3}')
    
    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: VM list: $vm_list"
    
    # Process each VM/CT to check if it uses cloned NFS storage
    while IFS='|' read -r vmid name vm_type; do
        if [[ -n "$vmid" ]]; then
            log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Processing VM/CT $vmid ($vm_type)"
            
            # Get storage name used by this VM/CT
            local storage_name
            storage_name=$(get_storage_name_of_id "$vmid" 2>/dev/null)
            
            log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: VM/CT $vmid uses storage: '$storage_name'"
            
            if [[ -n "$storage_name" ]]; then
                # Check if this storage is NFS
                if grep -q "^nfs:[[:space:]]*$storage_name" /etc/pve/storage.cfg 2>/dev/null; then
                    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Storage $storage_name is NFS"
                    
                    # Get NFS server IP for this storage
                    local nfs_ip
                    nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
                    
                    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: NFS IP for $storage_name: '$nfs_ip'"
                    
                    if [[ -n "$nfs_ip" ]]; then
                        # Get dataset pool name for is_clone check (using cached data)
                        local pool_name
                        pool_name=$(get_dataset_pool_name_cached "$storage_name" "$nfs_ip" 2>/dev/null)
                        
                        log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Pool name for $storage_name: '$pool_name'"
                        
                        if [[ -n "$pool_name" ]]; then
                            local volume_name="$pool_name/$storage_name"
                            
                            # Check if this storage is a clone (using cached data)
                            if is_clone_cached "$volume_name" "$nfs_ip"; then
                                vmct_list+=("$vmid|$vm_type|$name|$storage_name")
                                log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Found VM/CT $vmid ($vm_type) using cloned storage $storage_name"
                            else
                                log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Storage $storage_name is not a clone"
                            fi
                        else
                            log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Could not get pool name for $storage_name"
                        fi
                    else
                        log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Could not get NFS IP for $storage_name"
                    fi
                else
                    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Storage $storage_name is not NFS"
                fi
            else
                log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Could not get storage name for VM/CT $vmid"
            fi
        fi
    done <<< "$vm_list"
    
    log_debug "get_vmid_ctid_using_cloned_nfs_storage_list: Found ${#vmct_list[@]} VM/CTs using cloned storage"
    
    # Output the list
    printf '%s\n' "${vmct_list[@]}"
}

# Function to get non-cloned NFS storage list
get_non_cloned_nfs_storage() {
    local non_cloned_storages=()
    
    # Get all NFS storages
    local nfs_storages
    nfs_storages=$(get_nfs_storages)
    
    # First, get list of all cloned storages for comparison
    local cloned_storages
    cloned_storages=$(get_cloned_storage_list)
    
    # Check each NFS storage
    while read -r storage_name; do
        if [[ -n "$storage_name" ]]; then
            # Get NFS server IP for this storage
            local nfs_ip
            nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
            
            if [[ -n "$nfs_ip" ]]; then
                # Check if this storage is in the cloned storage list
                local is_in_cloned_list=false
                while read -r cloned_storage; do
                    if [[ "$cloned_storage" == "$storage_name" ]]; then
                        is_in_cloned_list=true
                        break
                    fi
                done <<< "$cloned_storages"
                
                if [[ "$is_in_cloned_list" == false ]]; then
                    # This storage is not in the cloned list, so it's non-cloned
                    non_cloned_storages+=("$storage_name")
                    log_debug "Found non-cloned NFS storage: $storage_name"
                else
                    log_debug "Skipped cloned NFS storage: $storage_name"
                fi
            else
                log_debug "Could not get NFS IP for storage: $storage_name"
            fi
        fi
    done <<< "$nfs_storages"
    
    # Output the list
    printf '%s\n' "${non_cloned_storages[@]}"
}

# Step 1: Select configuration files for restoration
step1_select_config() {
    local line confs=() menu_items=() vmids=()

    # Use cached config list if available, otherwise get fresh list
    if [ "${#cached_config_list[@]}" -eq 0 ]; then
        while IFS= read -r line; do
            if [[ -n "$line" ]]; then
                cached_config_list+=("$line")
                log_debug "Added config file to cache: $line"
            fi
        done < <(get_cloned_config_file_list)
    fi
    
    # Copy cached list to working array
    confs=("${cached_config_list[@]}")

    if [ "${#confs[@]}" -eq 0 ]; then
        dialog --msgbox "No VMs/CTs found to restore.\nPlease add cloned NFS storage first." 10 50
        return 1
    fi

    # First pass: collect all storage names, VM/CT names, and pool names to calculate max lengths
    local storage_names=() vmct_names=() pool_names=()
    for ((i=0; i<${#confs[@]}; i++)); do
        local storage_name=$(get_storage_name_from_mnt_conf_file "${confs[i]}")
        storage_names+=("$storage_name")
        
        # Get VM/CT name from config file
        # For VMs (QEMU), use 'name:' field; for containers (LXC), use 'hostname:' field
        local vmct_name
        vmct_name=$(grep -E "^name[[:space:]]*[:=]" "${confs[i]}" 2>/dev/null | sed -E 's/^name[[:space:]]*[:=][[:space:]]*//' | head -1)
        if [[ -z "$vmct_name" ]]; then
            # Try hostname field for LXC containers
            vmct_name=$(grep -E "^hostname[[:space:]]*[:=]" "${confs[i]}" 2>/dev/null | sed -E 's/^hostname[[:space:]]*[:=][[:space:]]*//' | head -1)
        fi
        if [[ -z "$vmct_name" ]]; then
            vmct_name="(no name)"
        fi
        vmct_names+=("$vmct_name")
        
        # Get NFS IP and pool name
        local nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
        local pool_name=$(get_dataset_pool_name "$storage_name" "$nfs_ip" 2>/dev/null)
        if [[ -z "$pool_name" ]]; then
            pool_name="(unknown)"
        fi
        pool_names+=("$pool_name")
    done

    # Calculate maximum storage name length for alignment
    local max_storage_len=0
    for storage_name in "${storage_names[@]}"; do
        if [ ${#storage_name} -gt $max_storage_len ]; then
            max_storage_len=${#storage_name}
        fi
    done
    
    # Calculate maximum VM/CT name length for alignment
    local max_vmct_name_len=0
    for vmct_name in "${vmct_names[@]}"; do
        if [ ${#vmct_name} -gt $max_vmct_name_len ]; then
            max_vmct_name_len=${#vmct_name}
        fi
    done
    
    # Calculate maximum pool name length for alignment
    local max_pool_len=0
    for pool_name in "${pool_names[@]}"; do
        if [ ${#pool_name} -gt $max_pool_len ]; then
            max_pool_len=${#pool_name}
        fi
    done

    local i vmid vmid_to_conf_map=()
    for ((i=0; i<${#confs[@]}; i++)); do
        # Extract VM/CT ID from path like /mnt/pve/storage/images/103/103.conf
        local dir_name="$(dirname "${confs[i]}")"
        vmid=$(basename "$dir_name")
        
        vmids+=("$vmid")
        # Map vmid to original config file path
        vmid_to_conf_map+=("${confs[i]}")

        # Get VM/CT type
        local vm_type=$(config_file_type "${confs[i]}")
        
        # Get VM/CT name from pre-calculated array
        local vmct_name="${vmct_names[i]}"
        
        # Get storage name
        local storage_name="${storage_names[i]}"
        
        # Get pool name from pre-calculated array
        local pool_name="${pool_names[i]}"
        
        # Get NFS IP address
        local nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
        
        # Format storage name with proper alignment
        local formatted_storage=$(printf "%-${max_storage_len}s" "$storage_name")
        
        # Format pool name with proper alignment
        local formatted_pool=$(printf "%-${max_pool_len}s" "$pool_name")
        
        # Calculate spacing after parentheses to align columns using non-breaking spaces
        local nbsp=$'\u00A0'  # non-breaking space for dialog alignment
        local name_with_parens="($vmct_name)"
        local total_name_width=$((max_vmct_name_len + 2))  # +2 for parentheses
        local padding_needed=$((total_name_width - ${#name_with_parens}))
        local padding=""
        for ((j=0; j<padding_needed; j++)); do
            padding="${padding}${nbsp}"
        done
        
        # Create display string with parentheses aligned: "vm/ct (vmct_name)  storage_name  nfs_ip  pool_name"
        local display_string=$(printf "%-2s %s%s  %s  %-16s  %s" "$vm_type" "$name_with_parens" "$padding" "$formatted_storage" "$nfs_ip" "$formatted_pool")

        # Check if this config was previously selected
        local status="off"
        for selected_conf in "${selected_confs[@]}"; do
            if [[ "$selected_conf" == "${confs[i]}" ]]; then
                status="on"
                break
            fi
        done

        menu_items+=("$vmid" "$display_string" "$status")
    done

    local dialog_selected=$(mktemp)
    local select_label="Select All"
    if [[ "$select_all_mode" == true ]]; then
        select_label="Deselect All"
    fi

    cmd=(dialog --keep-tite --title "Step 1/$total_steps: Select VM/CT for Restore"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "$select_label"
         --checklist "Choose VM/CT for restore (use SPACE to select/deselect):" 20 120 10)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select at least one VM/CT to restore." 8 50
                return 2  # Stay on same step
            fi

            # Parse selected VM IDs and find corresponding config files
            selected_confs=()
            IFS=' ' read -ra selected_vmids <<< "$selected_option"
            for selected_vmid in "${selected_vmids[@]}"; do
                # Remove quotes if present
                selected_vmid=$(echo "$selected_vmid" | tr -d '"')
                for ((i=0; i<${#vmids[@]}; i++)); do
                    if [[ "${vmids[i]}" == "$selected_vmid" ]]; then
                        selected_confs+=("${vmid_to_conf_map[i]}")
                        break
                    fi
                done
            done
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Select All / Deselect All
            if [[ "$select_all_mode" == false ]]; then
                # Select all
                selected_confs=("${confs[@]}")
                select_all_mode=true
            else
                # Deselect all
                selected_confs=()
                select_all_mode=false
            fi
            return 2  # Stay on same step
            ;;
    esac
}

step2_select_host() {
    local line hosts=() menu_items=()
    while IFS= read -r line; do
        hosts+=("$line")
    done < <(get_ha_nodes)

    if [ "${#hosts[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE hosts found." 8 40
        return 1
    fi

    local i
    for ((i=0; i<${#hosts[@]}; i++)); do
        menu_items+=("$i" "${hosts[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_host_index" ]]; then
        extra_button="--default-item $selected_host_index"
    fi

    cmd=(dialog --keep-tite --title "Step 2/$total_steps: Select Target Host"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose PVE host to restore to:" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_host_index="$selected_option"
            selected_host="${hosts[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

step3_summary() {
    local summary_text="Summary of selections:\n\n"
    summary_text+="Target host: $selected_host\n"
    summary_text+="Selected VM/CT configs (${#selected_confs[@]} total):\n\n"

    local i=1
    for conf in "${selected_confs[@]}"; do
        local dir_name="$(dirname "$conf")"
        local vmid=$(basename "$dir_name")
        local config_type=$(config_file_type "$conf")
        summary_text+="  $i) VM/CT ID: $vmid ($config_type)\n"
        summary_text+="     Config: $conf\n"
        ((i++))
    done

    summary_text+="\nClick 'Apply' to proceed with restoration or 'Back' to modify selections."

    dialog --keep-tite --title "Step 3/$total_steps: Confirmation" \
           --ok-label "Apply" --cancel-label "Back to Main" --extra-button --extra-label "Back" \
           --msgbox "$summary_text" 25 100

    case $? in
        0)  # Apply
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

restore_ct() {
    local src_conf="$1"
    local host_name="$2"
    local new_ctid="$3"  # Optional new CT ID
    local storage_name
    storage_name="$(echo "$src_conf" | awk -F'/' '{print $(NF-3)}')"
    local ctid
    
    # Use new ID if provided, otherwise use original ID from config path
    if [ -n "$new_ctid" ]; then
        ctid="$new_ctid"
        log_info "Using new CT ID: $ctid for restoration"
    else
        ctid="$(echo "$src_conf" | awk -F'/' '{print $(NF-1)}')"
        log_info "Using original CT ID: $ctid for restoration"
    fi
    
    local dst_conf="/etc/pve/nodes/${host_name}/lxc/${ctid}.conf"
    
    log_info "Restoring CT $ctid from $src_conf to $dst_conf"
    
    # Copy configuration file
    if ! cp "$src_conf" "$dst_conf"; then
        log_error "Failed to copy CT configuration from $src_conf to $dst_conf"
        return 1
    fi
    
    # Update storage reference in config
    local current_storage_name
    current_storage_name=$(awk -F: '/^rootfs:/ {print $2}' "$dst_conf" | awk -F: '{print $1}' | awk '{$1=$1};1')
    
    if [ -n "$current_storage_name" ]; then
        sed -i "s|^rootfs: *${current_storage_name}:|rootfs: ${storage_name}:|" "$dst_conf"
        log_info "Updated storage reference from $current_storage_name to $storage_name in CT $ctid config"
    else
        log_error "Could not find rootfs storage reference in CT $ctid config"
        return 1
    fi
    
    # Start the start-and-stop sequence in background using SSH
    # Use nohup to fully detach the job
    local host_ip
    host_ip=$(host_ip "$host_name")
    if [[ -z "$host_ip" ]]; then
        log_error "Cannot resolve IP for host '$host_name'"
        return 1
    fi
    
    nohup bash -c "ssh root@$host_ip 'pct start $ctid; pct stop $ctid'" > "$LOGS_DIR/restore_ct_${ctid}.log" 2>&1 < /dev/null &
    
    log_info "CT $ctid restored"
    return 0
}


# Regenerate MAC addresses for network interfaces in VM config
regenerate_mac_addresses() {
    local config_file="$1"
    
    if [ ! -f "$config_file" ]; then
        log_error "regenerate_mac_addresses: Config file not found: $config_file"
        return 1
    fi
    
    log_info "Regenerating MAC addresses for VM config: $config_file"
    
    # Find all network interface lines (net0, net1, etc.) with MAC addresses
    # Handle both formats: macaddr=XX:XX:XX:XX:XX:XX and hwaddr=XX:XX:XX:XX:XX:XX
    local mac_count=0
    
    # Process each network interface line
    while IFS= read -r line; do
        if [[ "$line" =~ ^net[0-9]+: ]]; then
            local net_num=$(echo "$line" | grep -oE "^net[0-9]+" | grep -oE "[0-9]+")
            local old_mac=""
            local mac_param=""
            
            # Check for virtio=MAC pattern (Proxmox VE format)
            if [[ "$line" =~ virtio=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="virtio"
            # Check for e1000=MAC pattern
            elif [[ "$line" =~ e1000=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="e1000"
            # Check for rtl8139=MAC pattern
            elif [[ "$line" =~ rtl8139=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="rtl8139"
            # Check for macaddr= pattern (alternative format)
            elif [[ "$line" =~ macaddr=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="macaddr"
            fi
            
            if [[ -n "$old_mac" ]]; then
                # Generate new random MAC address (using Proxmox's range)
                # Proxmox VE uses prefix BC:24:11 for auto-generated MACs
                local new_mac=$(printf "BC:24:11:%02X:%02X:%02X" $((RANDOM % 256)) $((RANDOM % 256)) $((RANDOM % 256)))
                
                # Replace the MAC address in the config file
                sed -i "s/${mac_param}=${old_mac}/${mac_param}=${new_mac}/g" "$config_file"
                
                log_info "Regenerated MAC address for net${net_num}: ${old_mac} -> ${new_mac}"
                ((mac_count++))
            fi
        fi
    done < "$config_file"
    
    log_info "Total MAC addresses regenerated: $mac_count"
    
    return 0
}

restore_vm() {
    local src_conf="$1"
    local host_name="$2"
    local new_vmid="$3"  # Optional new VM ID
    local storage_name
    storage_name="$(echo "$src_conf" | awk -F'/' '{print $(NF-3)}')"
    local vmid
    
    # Use new ID if provided, otherwise use original ID from config path
    if [ -n "$new_vmid" ]; then
        vmid="$new_vmid"
        log_info "Using new VM ID: $vmid for restoration"
    else
        vmid="$(echo "$src_conf" | awk -F'/' '{print $(NF-1)}')"
        log_info "Using original VM ID: $vmid for restoration"
    fi
    
    local dst_conf="/etc/pve/nodes/${host_name}/qemu-server/${vmid}.conf"
    
    log_info "Restoring VM $vmid from $src_conf to $dst_conf"
    
    # Copy configuration file
    if ! cp "$src_conf" "$dst_conf"; then
        log_error "Failed to copy VM configuration from $src_conf to $dst_conf"
        return 1
    fi

    # Update storage references in config using comprehensive device pattern
    local current_storage_name
    current_storage_name="$(grep -P "^(?:scsi|ide|sata|virtio|efidisk|tpmstate|nvme)[0-9]+:\s+[^:]+:[0-9]+/vm" "$dst_conf" | head -1 | awk -F':' '{print $2}' | awk '{$1=$1};1')"

    if [[ -n "$current_storage_name" ]]; then
        # Update storage references for disk devices
        sed -i -E "s@^((scsi|ide|sata|virtio|efidisk|tpmstate|nvme)[0-9]+:) *${current_storage_name}:@\1 ${storage_name}:@g" "$dst_conf"
        log_info "Updated storage reference from $current_storage_name to $storage_name in VM $vmid config"
        
        # Update storage references for vmstate files (snapshots)
        if grep -q "vmstate: ${current_storage_name}:" "$dst_conf"; then
            sed -i -E "s@^(vmstate:) *${current_storage_name}:@\1 ${storage_name}:@g" "$dst_conf"
            log_info "Updated vmstate storage reference from $current_storage_name to $storage_name in VM $vmid config"
        fi
    else
        log_error "Could not find storage reference in VM $vmid config"
        return 1
    fi
    
    # Regenerate MAC addresses for network interfaces
    regenerate_mac_addresses "$dst_conf"
    
    log_info "VM $vmid restored"
    return 0
}


perform_restore() {
    local total=${#selected_confs[@]}
    local current=0
    local failed=0
    local success=0
    local skipped=0
    local any_new_ids_used=false

    for conf in "${selected_confs[@]}"; do
        ((current++))
        local dir_name="$(dirname "$conf")"
        local vmid=$(basename "$dir_name")
        local type=$(config_file_type "$conf")

        # Show progress
        local type_upper=$(upper "$type")
        dialog --title "Restoring..." --infobox "Processing $type_upper $vmid ($current/$total)\nType: $type\nFile: $conf" 8 100
        sleep 1

        log_info "Processing $type_upper $vmid ($current/$total) - Type: $type"

        # Check if VM/CT already exists in PVE repository
        if check_vmct_exists "$vmid"; then
            log_info "VM/CT $vmid already exists - initiating conflict resolution"
            
            # Reset conflict resolution variables
            new_vmct_id=""
            conflict_selected_host=""
            
            # Show conflict resolution dialog
            if resolve_vmct_conflict "$vmid" "$type" "$conf"; then
                # User provided new ID, now ask for host selection
                if select_host_for_conflict "$new_vmct_id" "$type"; then
                    # User selected host, proceed with restore using new ID and host
                    log_info "Restoring VM/CT $vmid as $new_vmct_id to host $conflict_selected_host"
                    
                    case "$type" in
                        ct)
                            if restore_ct "$conf" "$conflict_selected_host" "$new_vmct_id"; then
                                ((success++))
                                any_new_ids_used=true
                                log_info "Successfully restored CT $vmid as $new_vmct_id"
                            else
                                ((failed++))
                                log_error "Failed to restore CT $vmid as $new_vmct_id"
                            fi
                            ;;
                        vm)
                            if restore_vm "$conf" "$conflict_selected_host" "$new_vmct_id"; then
                                ((success++))
                                any_new_ids_used=true
                                log_info "Successfully restored VM $vmid as $new_vmct_id"
                            else
                                ((failed++))
                                log_error "Failed to restore VM $vmid as $new_vmct_id"
                            fi
                            ;;
                        *)
                            ((failed++))
                            log_error "Unknown VM/CT type: $type"
                            ;;
                    esac
                else
                    # User cancelled host selection
                    ((skipped++))
                fi
            else
                # User cancelled conflict resolution
                ((skipped++))
            fi
        else
            # VM/CT doesn't exist, proceed with normal restore
            log_info "VM/CT $vmid does not exist - proceeding with normal restore"
            
            case "$type" in
                ct)
                    if restore_ct "$conf" "$selected_host"; then
                        ((success++))
                        log_info "Successfully restored CT $vmid"
                    else
                        ((failed++))
                        log_error "Failed to restore CT $vmid"
                    fi
                    ;;
                vm)
                    if restore_vm "$conf" "$selected_host"; then
                        ((success++))
                        log_info "Successfully restored VM $vmid"
                    else
                        ((failed++))
                        log_error "Failed to restore VM $vmid"
                    fi
                    ;;
                *)
                    ((failed++))
                    log_error "Unknown VM/CT type: $type"
                    ;;
            esac
        fi
    done

    # Need to initiate fresh config files backup using pve-config-backup daemon
    log_info "Stopping pve-config-backup daemon"
    pve-config-backup stop >/dev/null 2>&1
    log_info "Starting pve-config-backup daemon"
    pve-config-backup start >/dev/null 2>&1

    # Show final result
    local result_text="Restoration completed!\n\n"
    result_text+="Total processed: $total\n"
    result_text+="Successful: $success\n"
    result_text+="Failed: $failed\n"
    result_text+="Skipped: $skipped\n"
    result_text+="Target host: $selected_host"

    log_info "Restoration completed - Total: $total, Success: $success, Failed: $failed, Skipped: $skipped"
    dialog --msgbox "$result_text" 15 50
    
    # Show static IP address warning only if any VMs were restored with new IDs (due to conflicts)
    if [ $success -gt 0 ] && [ "$any_new_ids_used" = true ]; then
        local warning_text="IMPORTANT: Static IP Address Configuration\n\n"
        warning_text+="If the restored VM use static IP addresses, you must:\n\n"
        warning_text+="1. Start the VM with network isolated/disconnected\n"
        warning_text+="2. Access the VM console\n"
        warning_text+="3. Change static IP addresses to unique values\n"
        warning_text+="4. Reconnect network after IP configuration\n\n"
        warning_text+="This is not required if the VM use DHCP for IP assignment.\n\n"
        warning_text+="To confirm you have read this important information,\ntype 'noted':"
        
        local confirm_input=$(mktemp)
        dialog --ok-label "OK" --no-cancel \
               --inputbox "$warning_text" 22 70 2> "$confirm_input"
        local confirmation=$(cat "$confirm_input" | tr -d ' \t\n\r')
        rm -f "$confirm_input"
        
        # Keep showing dialog until user types "noted"
        while [[ "$confirmation" != "noted" ]]; do
            dialog --ok-label "OK" --no-cancel \
                   --inputbox "$warning_text" 20 70 2> "$confirm_input"
            confirmation=$(cat "$confirm_input" | tr -d ' \t\n\r')
            rm -f "$confirm_input"
        done
    fi
}

restore_from_clone_wizard() {
    # Reset variables for restore wizard
    selected_confs=()
    selected_host=""
    selected_host_index=""
    current_step=1
    select_all_mode=false
    storage_discovery_done=false
    cached_config_list=()  # Clear cached config list
    
    # Reset conflict resolution variables
    new_vmct_id=""
    conflict_selected_host=""
    
    # Ensure storage configuration is synchronized (only once)
    if [[ "$storage_discovery_done" == false ]]; then
        dialog --title "Please wait..." --infobox "Synchronizing storage discovery..." 5 50
        sync_storage_discovery
        storage_discovery_done=true
    fi

    while true; do
        case $current_step in
            1)
                step1_select_config
                case $? in
                    0)  # Next
                        current_step=2
                        ;;
                    1)  # Back to Main
                        return 0
                        ;;
                    2)  # Stay on same step (retry)
                        ;;
                esac
                ;;
            2)
                step2_select_host
                case $? in
                    0)  # Next
                        current_step=3
                        ;;
                    1)  # Back to Main
                        return 0
                        ;;
                    2)  # Back
                        current_step=1
                        ;;
                esac
                ;;
            3)
                step3_summary
                case $? in
                    0)  # Apply
                        perform_restore
                        return 0
                        ;;
                    1)  # Back to Main
                        return 0
                        ;;
                    2)  # Back
                        current_step=2
                        ;;
                esac
                ;;
        esac
    done
}

# MOVE STORAGE FOR VM/CT USING CLONED NFS STORAGE

# Variables for move storage wizard
selected_vmcts=()
selected_target_storage=""
move_storage_current_step=0
move_select_all_mode=false
move_storage_mode=""
selected_migration_type=""  # "vm", "ct", or "" for single-type systems

# Detect available VM/CT types in the system
detect_available_vm_ct_types() {
    local has_vms=false has_cts=false
    local vmct_raw_list
    
    # Get list of VM/CT using cloned NFS storage
    vmct_raw_list=$(get_vmid_ctid_using_cloned_nfs_storage_list)
    
    log_debug "detect_available_vm_ct_types: Raw list result: '$vmct_raw_list'"
    
    if [[ -z "$vmct_raw_list" ]]; then
        log_debug "detect_available_vm_ct_types: No VM/CT found via get_vmid_ctid_using_cloned_nfs_storage_list, trying fallback method"
        
        # Fallback: Check VM/CT configs directly for cloned storage patterns
        local fallback_found=false
        
        # Check VMs
        for conf_file in /etc/pve/nodes/*/qemu-server/*.conf; do
            [[ -f "$conf_file" ]] || continue
            if grep -q "clone.*2025-08" "$conf_file" 2>/dev/null; then
                has_vms=true
                fallback_found=true
                log_debug "detect_available_vm_ct_types: Found VM with cloned storage via fallback: $conf_file"
            fi
        done
        
        # Check CTs  
        for conf_file in /etc/pve/nodes/*/lxc/*.conf; do
            [[ -f "$conf_file" ]] || continue
            if grep -q "clone.*2025-08" "$conf_file" 2>/dev/null; then
                has_cts=true
                fallback_found=true
                log_debug "detect_available_vm_ct_types: Found CT with cloned storage via fallback: $conf_file"
            fi
        done
        
        if [[ "$fallback_found" == false ]]; then
            log_debug "detect_available_vm_ct_types: No cloned storage found in VM/CT configs"
            echo "none"
            return 0
        fi
    else
        # Analyze what types are available from the function result
        while IFS='|' read -r vmid vm_type name storage_name; do
            if [[ -n "$vmid" ]]; then
                log_debug "detect_available_vm_ct_types: Found $vm_type $vmid on storage $storage_name"
                if [[ "$vm_type" == "vm" ]]; then
                    has_vms=true
                elif [[ "$vm_type" == "ct" ]]; then
                    has_cts=true
                fi
            fi
        done <<< "$vmct_raw_list"
    fi
    
    # Return the result
    if [[ "$has_vms" == true && "$has_cts" == true ]]; then
        log_debug "detect_available_vm_ct_types: Result = mixed (VMs and CTs found)"
        echo "mixed"
    elif [[ "$has_vms" == true ]]; then
        log_debug "detect_available_vm_ct_types: Result = vm-only"
        echo "vm-only"
    elif [[ "$has_cts" == true ]]; then
        log_debug "detect_available_vm_ct_types: Result = ct-only"
        echo "ct-only"
    else
        log_debug "detect_available_vm_ct_types: Result = none"
        echo "none"
    fi
}

# Step 0: Select migration type (only for mixed environments)
move_storage_step0_select_type() {
    local dialog_selected=$(mktemp)
    
    cmd=(dialog --keep-tite --title "Step 1/5: Select Migration Type"
         --ok-label "Next" --cancel-label "Back"
         --menu "Both Virtual Machines and Containers use cloned storage.\n\nChoose which type to migrate in this session:" 14 75 2)
    
    options=("vm" "Migrate Virtual Machines (VMs)"
             "ct" "Migrate Containers (CTs)")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            case "$selected_option" in
                vm)
                    selected_migration_type="vm"
                    log_info "User selected VM migration type"
                    ;;
                ct)
                    selected_migration_type="ct"
                    log_info "User selected CT migration type"
                    ;;
            esac
            return 0
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Main wizard function for moving storage
move_storage_cloned_nfs() {
    # Reset variables for move storage wizard
    selected_vmcts=()
    selected_target_storage=""
    move_storage_current_step=0
    move_select_all_mode=false
    move_storage_mode=""
    selected_migration_type=""
    
    # Step -1: Detect available types and set initial step
    local available_types=$(detect_available_vm_ct_types)
    log_info "Detected available types: $available_types"
    
    case "$available_types" in
        "none")
            dialog --msgbox "No VM/CT found using cloned NFS storage.\nPlease ensure you have VM/CT using cloned NFS storage." 10 60
            return 0
            ;;
        "vm-only")
            selected_migration_type="vm"
            move_storage_current_step=1  # Skip type selection, go to VM selection
            ;;
        "ct-only")
            selected_migration_type="ct"
            move_storage_mode="standard"  # Auto-select standard mode for CTs
            move_storage_current_step=1  # Skip type selection, go to CT selection
            log_info "Auto-selected standard mode for CT-only environment"
            ;;
        "mixed")
            move_storage_current_step=0  # Start with type selection
            ;;
    esac
    
    while true; do
        case $move_storage_current_step in
            0)
                # Step 0: Select migration type (only for mixed environments)
                move_storage_step0_select_type
                case $? in
                    0)  # Next
                        move_storage_current_step=1
                        ;;
                    1)  # Back to Main
                        return 0
                        ;;
                esac
                ;;
            1)
                # Step 1: Select VM/CT based on type
                move_storage_step1_select_vmct "$available_types"
                case $? in
                    0)  # Next
                        if [[ "$selected_migration_type" == "ct" ]]; then
                            # Skip mode selection for CT migrations (auto-select standard mode)
                            move_storage_mode="standard"
                            log_info "Auto-selected standard mode for CT migration"
                            move_storage_current_step=3
                        else
                            # VM migration - proceed to mode selection
                            move_storage_current_step=2
                        fi
                        ;;
                    1)  # Back
                        if [[ "$available_types" == "mixed" ]]; then
                            move_storage_current_step=0  # Back to type selection
                        else
                            return 0  # Back to main (single-type systems)
                        fi
                        ;;
                    2)  # Stay on same step (retry)
                        ;;
                esac
                ;;
            2)
                # Step 2: Select mode based on type
                move_storage_step2_select_mode "$available_types"
                case $? in
                    0)  # Next
                        move_storage_current_step=3
                        ;;
                    1)  # Back
                        move_storage_current_step=1
                        ;;
                    2)  # Stay on same step (retry)
                        ;;
                esac
                ;;
            3)
                # Step 3: Select target storage
                move_storage_step3_select_target_storage
                case $? in
                    0)  # Next - execute migration
                        move_storage_step4_execute_migration
                        return 0
                        ;;
                    1)  # Back to Main
                        return 0
                        ;;
                    2)  # Back
                        if [[ "$selected_migration_type" == "ct" ]]; then
                            # For CT migrations, go back to VM/CT selection (step 1)
                            move_storage_current_step=1
                        else
                            # For VM migrations, go back to mode selection (step 2)
                            move_storage_current_step=2
                        fi
                        ;;
                    3)  # Stay on same step (retry)
                        ;;
                esac
                ;;
        esac
    done
}

# Helper functions for mode-specific confirmations

# Confirm VM snapshot deletion for standard mode
confirm_vm_snapshot_deletion() {
    local vm_list=""
    local vm_count=0
    
    # Build list of VMs that will lose snapshots
    for vmct_data in "${selected_vmcts[@]}"; do
        IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
        if [[ "$vm_type" == "vm" ]]; then
            vm_list+="- VM $vmid ($name)\n"
            ((vm_count++))
        fi
    done
    
    local confirm_input=$(mktemp)
    dialog --inputbox "WARNING: VM Standard mode will DELETE all local snapshots (qcow2 based) for the following VMs before Disk Move:\n\n$vm_list\nThis operation is irreversible!\n\nTo confirm and proceed with VM standard migration mode, type 'delete snapshots':" 18 85 2> "$confirm_input"
    local exit_code=$?
    local confirmation=$(trimq "$(cat "$confirm_input")")
    rm -f "$confirm_input"
    
    # Check if user input contains both "delete" and "snapshots" (case-insensitive)
    if [[ $exit_code -ne 0 ]] || ! echo "$confirmation" | grep -qi "delete" || ! echo "$confirmation" | grep -qi "snapshots"; then
        dialog --msgbox "VM Standard migration mode cancelled." 8 40
        return 1
    fi
    
    log_info "User confirmed VM standard mode with snapshot deletion for $vm_count VMs"
    return 0
}

# Confirm container stop for CT standard mode
confirm_container_stop() {
    local running_cts=""
    local running_count=0
    local total_count=0
    
    # Build list of containers and check which are running
    for vmct_data in "${selected_vmcts[@]}"; do
        IFS='|' read -r ctid vm_type name storage_name <<< "$vmct_data"
        if [[ "$vm_type" == "ct" ]]; then
            ((total_count++))
            if is_power_on "$ctid"; then
                running_cts+="- CT $ctid ($name) - Currently RUNNING\n"
                ((running_count++))
            else
                running_cts+="- CT $ctid ($name) - Currently stopped\n"
            fi
        fi
    done
    
    local message
    if [[ $running_count -gt 0 ]]; then
        message="NOTICE: Container migration requires shutting down running containers temporarily.\n\nThe following containers will be affected:\n\n$running_cts\nRunning containers ($running_count) will be SHUTDOWN during migration and automatically RESTARTED when complete.\n\nTo confirm and proceed with container migration, type 'shutdown':"
    else
        message="Container migration will process the following containers:\n\n$running_cts\nAll containers are already stopped.\n\nTo confirm and proceed with container migration, type 'proceed':"
    fi
    
    local confirm_input=$(mktemp)
    local expected_response="shutdown"
    if [[ $running_count -eq 0 ]]; then
        expected_response="proceed"
    fi
    
    dialog --inputbox "$message" 20 85 2> "$confirm_input"
    local exit_code=$?
    local confirmation=$(trimq "$(cat "$confirm_input")")
    rm -f "$confirm_input"
    
    if [[ $exit_code -ne 0 || "$confirmation" != "$expected_response" ]]; then
        dialog --msgbox "Container migration cancelled." 8 40
        return 1
    fi
    
    log_info "User confirmed CT standard mode for $total_count containers ($running_count running)"
    return 0
}

# Validate VM power states for preserve mode
validate_vm_preserve_mode() {
    local running_vms=""
    local running_count=0
    local total_count=0
    
    # Check power states of selected VMs
    for vmct_data in "${selected_vmcts[@]}"; do
        IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
        if [[ "$vm_type" == "vm" ]]; then
            ((total_count++))
            if is_power_on "$vmid"; then
                running_vms+="- VM $vmid ($name)\n"
                ((running_count++))
            fi
        fi
    done
    
    if [[ $running_count -gt 0 ]]; then
        dialog --msgbox "ERROR: VM Preserve mode requires all VMs to be powered OFF.\n\nThe following VMs are currently running:\n\n$running_vms\nPlease power off these VMs and try again, or use VM Standard mode instead." 16 70
        return 1
    fi
    
    log_info "VM preserve mode validated: all $total_count VMs are powered off"
    return 0
}

# Step 2: Select mode based on type (simplified - no mixed-type complexity)
move_storage_step2_select_mode() {
    local available_types="$1"
    local dialog_selected=$(mktemp)
    local menu_options=()
    local step_number=""
    
    # Determine step number based on environment type
    if [[ "$available_types" == "mixed" ]]; then
        step_number="Step 3/5"
    else
        step_number="Step 2/4"
    fi
    
    # Build menu options based on selected migration type (no mixed complexity!)
    if [[ "$selected_migration_type" == "vm" ]]; then
        # VM migration modes
        menu_options+=(
            "standard" "Delete VM snapshots (VMs can be ON or OFF)"
            "preserve" "Preserve VM snapshots (VMs must be powered OFF)"
        )
    elif [[ "$selected_migration_type" == "ct" ]]; then
        # CT migration modes (only one option)
        menu_options+=(
            "standard" "Migrate containers (will shutdown running containers temporarily)"
        )
    fi
    
    cmd=(dialog --keep-tite --title "$step_number: Select Migration Mode"
         --ok-label "Next" --cancel-label "Back"
         --menu "Select migration mode for $selected_migration_type migration:" 14 85 2)
    options=("${menu_options[@]}")
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            case "$selected_option" in
                preserve)
                    if validate_vm_preserve_mode; then
                        move_storage_mode="preserve-snapshots"
                        log_info "User selected VM preserve snapshots mode"
                        return 0
                    else
                        return 2  # Stay on same step - validation failed
                    fi
                    ;;
                standard)
                    if [[ "$selected_migration_type" == "vm" ]]; then
                        if confirm_vm_snapshot_deletion; then
                            move_storage_mode="standard"
                            log_info "User selected VM standard mode (no snapshot preservation)"
                            return 0
                        else
                            return 2  # Stay on same step - confirmation cancelled
                        fi
                    elif [[ "$selected_migration_type" == "ct" ]]; then
                        if confirm_container_stop; then
                            move_storage_mode="standard"
                            log_info "User selected CT standard mode"
                            return 0
                        else
                            return 2  # Stay on same step - confirmation cancelled
                        fi
                    fi
                    ;;
            esac
            ;;
        1)  # Back
            return 1
            ;;
    esac
}


# VM/CT power state checking functions for disk migration
vm_type() {
    local vmid="$1"

    # Check if LXC config exists and contains container-specific entries
    if [[ -d "/etc/pve/nodes" ]] && grep -Eq '(rootfs|mp[0-9]+)' /etc/pve/nodes/*/lxc/${vmid}.conf 2>/dev/null; then
        echo "ct"
    else
        echo "vm"
    fi
}

# Optimized function to get all VM/CT information using hybrid approach
# Returns: vmid|name|type|status|node|tags|lock
# Performance: Fast file parsing + single cluster API call for status
get_all_vm_ct_info() {
    local vm_ct_list=()
    declare -A vm_details       # vmid -> "name|tags|lock|node"
    declare -A ct_details       # ctid -> "name|tags|lock|node"
    declare -A status_map       # vmid -> "status"
    
    # Get all VM/CT status in one cluster API call (fast!)
    local status_data
    if status_data=$(pvesh get /cluster/resources --type vm --output-format json 2>/dev/null | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    for item in data:
        if 'vmid' in item and 'status' in item:
            print(f\"{item['vmid']}:{item['status']}\")
except: pass
" 2>/dev/null); then
        # Parse cluster status into associative array
        while IFS=':' read -r vmid status; do
            [[ -n "$vmid" && -n "$status" ]] && status_map["$vmid"]="$status"
        done <<< "$status_data"
    fi
    
    # Parse VM configs for detailed info
    for conf_file in /etc/pve/nodes/*/qemu-server/*.conf; do
        [[ -f "$conf_file" ]] || continue
        
        local vmid=$(basename "$conf_file" .conf)
        local node=$(echo "$conf_file" | cut -d'/' -f5)
        local name="unnamed"
        local tags=""
        local lock=""
        
        # Parse config file for details
        while IFS=': ' read -r key value; do
            case "$key" in
                "name") name="$value" ;;
                "tags") tags="$value" ;;
                "lock") lock="$value" ;;
            esac
        done < "$conf_file"
        
        # Get status from cluster data or fallback
        local status="${status_map[$vmid]:-unknown}"
        
        # Handle suspended/paused status
        if [[ "$lock" == "suspended" ]]; then
            status="suspended"
        fi
        
        vm_ct_list+=("$vmid|$name|vm|$status|$node|$tags|$lock")
    done
    
    # Parse CT configs for detailed info
    for conf_file in /etc/pve/nodes/*/lxc/*.conf; do
        [[ -f "$conf_file" ]] || continue
        
        local ctid=$(basename "$conf_file" .conf)
        local node=$(echo "$conf_file" | cut -d'/' -f5)
        local name="unnamed"
        local tags=""
        local lock=""
        
        # Parse config file for details
        while IFS=': ' read -r key value; do
            case "$key" in
                "hostname") name="$value" ;;
                "tags") tags="$value" ;;
                "lock") lock="$value" ;;
            esac
        done < "$conf_file"
        
        # Get status from cluster data or fallback
        local status="${status_map[$ctid]:-unknown}"
        
        # Handle suspended/paused status
        if [[ "$lock" == "suspended" ]]; then
            status="suspended"
        fi
        
        vm_ct_list+=("$ctid|$name|ct|$status|$node|$tags|$lock")
    done
    
    # Output all results
    printf '%s\n' "${vm_ct_list[@]}" | sort -t'|' -k1,1n
}

is_power_on() {
    local vmid="$1"
    local type status vm_node vm_node_ip

    # Determine if this is a VM or CT and find which node it's on
    type=$(vm_type "$vmid")
    vm_node=$(on_which_host_vmct_is "$vmid")
    
    if [[ -z "$vm_node" ]]; then
        log_error "Cannot find node for VM/CT $vmid"
        return 1
    fi
    
    # Get IP for the node
    vm_node_ip=$(host_ip "$vm_node")
    if [[ -z "$vm_node_ip" ]]; then
        log_error "Cannot resolve IP for node '$vm_node'"
        return 1
    fi

    # Get status using SSH to execute command on the correct node
    if [[ "$type" == "ct" ]]; then
        status=$(ssh root@"$vm_node_ip" "pct status '$vmid'" 2>/dev/null < /dev/null | awk '{print $2}')
    else
        status=$(ssh root@"$vm_node_ip" "qm status '$vmid'" 2>/dev/null < /dev/null | awk '{print $2}')
    fi
    
    if [[ -z "$status" ]]; then
        log_error "Failed to get status for VM/CT $vmid on node $vm_node"
        return 1
    fi

    # Check power status
    if [[ "$status" == "running" ]]; then
        log_debug "VM/CT $vmid is running"
        return 0
    else
        log_debug "VM/CT $vmid is not running (status: $status)"
        return 1
    fi
}

# Step 1: Select VM/CT using cloned NFS storage (filtered by type)
move_storage_step1_select_vmct() {
    local available_types="$1"
    local vmct_data menu_items=() vmct_list=()
    
    # Initialize API cache for optimal performance
    log_debug "Initializing API cache for storage migration dialog"
    if ! init_api_cache; then
        dialog --msgbox "Failed to initialize API cache.\nStorage migration may be slower than expected." 10 60
    fi
    
    # Get list of VM/CT using cloned NFS storage
    local vmct_raw_list
    vmct_raw_list=$(get_vmid_ctid_using_cloned_nfs_storage_list)
    
    if [[ -z "$vmct_raw_list" ]]; then
        dialog --msgbox "No VM/CT found using cloned NFS storage.\nPlease ensure you have VM/CT running on cloned NFS storage." 10 60
        return 1
    fi
    
    # Process the list into arrays - filter by selected migration type
    while IFS='|' read -r vmid vm_type name storage_name; do
        if [[ -n "$vmid" && "$vm_type" == "$selected_migration_type" ]]; then
            log_debug "Including $vm_type $vmid for $selected_migration_type migration"
            vmct_list+=("$vmid|$vm_type|$name|$storage_name")
        fi
    done <<< "$vmct_raw_list"
    
    # Check if list is empty
    if [[ ${#vmct_list[@]} -eq 0 ]]; then
        dialog --msgbox "No VM/CT found using cloned NFS storage.\nPlease ensure you have VM/CT using cloned NFS storage." 10 60
        return 1
    fi
    
    # Calculate maximum field lengths for alignment (including parentheses for names)
    local max_name_len=0 max_storage_len=0
    for vmct_data in "${vmct_list[@]}"; do
        IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
        # Add 2 for parentheses around name: (name)
        local name_with_parens_len=$((${#name} + 2))
        if [[ $name_with_parens_len -gt $max_name_len ]]; then
            max_name_len=$name_with_parens_len
        fi
        if [[ ${#storage_name} -gt $max_storage_len ]]; then
            max_storage_len=${#storage_name}
        fi
    done
    
    # Build menu items
    for vmct_data in "${vmct_list[@]}"; do
        IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
        
        # Check power state for display
        local power_state="OFF"
        if is_power_on "$vmid"; then
            power_state="ON "
        fi
        
        # Format the display string with proper alignment including power state
        local name_with_parens="($name)"
        local formatted_name=$(printf "%-${max_name_len}s" "$name_with_parens")
        local display_string=$(printf "%-2s %s  %s  [%s]" "$vm_type" "$formatted_name" "$storage_name" "$power_state")
        
        # Check if this VM/CT was previously selected by checking if vmid is in selected list
        local status="off"
        for selected_vmct_data in "${selected_vmcts[@]}"; do
            IFS='|' read -r selected_vmid _ _ _ <<< "$selected_vmct_data"
            if [[ "$selected_vmid" == "$vmid" ]]; then
                status="on"
                break
            fi
        done
        
        menu_items+=("$vmid" "$display_string" "$status")
    done
    
    local dialog_selected=$(mktemp)
    local select_label="Select All"
    if [[ "$move_select_all_mode" == true ]]; then
        select_label="Deselect All"
    fi
    
    # Build type-specific dialog title and description
    local dialog_title=""
    local dialog_description=""
    local step_number=""
    
    if [[ "$available_types" == "mixed" ]]; then
        if [[ "$selected_migration_type" == "ct" ]]; then
            step_number="Step 2/4"  # Mixed environment, CT selected (shorter workflow)
        else
            step_number="Step 2/5"  # Mixed environment, VM selected (full workflow)
        fi
    elif [[ "$available_types" == "ct-only" ]]; then
        step_number="Step 1/3"  # CT-only workflow is shorter (no mode selection)
    else
        step_number="Step 1/4"  # VM-only workflow
    fi
    
    if [[ "$selected_migration_type" == "vm" ]]; then
        dialog_title="$step_number: Select Virtual Machines for Storage Migration"
        dialog_description="Choose VMs to migrate storage (use SPACE to select/deselect):\n\nFormat: [vm] (Name) Storage [Power State]"
    elif [[ "$selected_migration_type" == "ct" ]]; then
        dialog_title="$step_number: Select Containers for Storage Migration"
        dialog_description="Choose containers to migrate storage (use SPACE to select/deselect):\n\nFormat: [ct] (Name) Storage [Power State]"
    else
        dialog_title="$step_number: Select VM/CT for Storage Migration"
        dialog_description="Choose VM/CT to migrate storage (use SPACE to select/deselect):\n\nFormat: [Type] (Name) Storage [Power State]"
    fi
    
    # Display dialog
    cmd=(dialog --keep-tite --title "$dialog_title"
         --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
         --checklist "$dialog_description" 20 110 10)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select at least one VM/CT to migrate." 8 50
                return 2  # Stay on same step
            fi
            
            # Parse selected VM/CT data - convert vmids back to full data
            selected_vmcts=()
            IFS=' ' read -ra selected_vmids <<< "$selected_option" 
            for selected_vmid in "${selected_vmids[@]}"; do
                # Remove quotes if present
                selected_vmid=$(echo "$selected_vmid" | tr -d '"')
                # Find the corresponding full data for this vmid
                for vmct_data in "${vmct_list[@]}"; do
                    IFS='|' read -r vmid _ _ _ <<< "$vmct_data"
                    if [[ "$vmid" == "$selected_vmid" ]]; then
                        selected_vmcts+=("$vmct_data")
                        break
                    fi
                done
            done
            
            log_info "Selected ${#selected_vmcts[@]} VM/CT for storage migration"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Select/Deselect All
            if [[ "$move_select_all_mode" == false ]]; then
                # Select all
                selected_vmcts=("${vmct_list[@]}")
                move_select_all_mode=true
                log_info "Selected all VM/CT for migration"
            else
                # Deselect all
                selected_vmcts=()
                move_select_all_mode=false
                log_info "Deselected all VM/CT"
            fi
            return 2  # Stay on same step
            ;;
    esac
}

# Step 2: Select target non-cloned NFS storage
move_storage_step3_select_target_storage() {
    local storage_list menu_items=()
    local step_number=""
    
    # Determine step number based on selected migration type
    local available_types=$(detect_available_vm_ct_types)
    if [[ "$selected_migration_type" == "ct" ]]; then
        if [[ "$available_types" == "ct-only" ]]; then
            step_number="Step 2/3"  # CT-only workflow
        else
            step_number="Step 3/4"  # Mixed environment, CT selected
        fi
    else
        # VM migration workflow
        if [[ "$available_types" == "mixed" ]]; then
            step_number="Step 4/5"  # Mixed environment, VM selected
        else
            step_number="Step 3/4"  # VM-only workflow
        fi
    fi
    
    # Get list of non-cloned NFS storage
    local non_cloned_storages
    non_cloned_storages=$(get_non_cloned_nfs_storage)
    
    if [[ -z "$non_cloned_storages" ]]; then
        dialog --msgbox "No non-cloned NFS storage found.\nPlease add non-cloned NFS storage first." 10 50
        return 1
    fi
    
    # Process the list
    while read -r storage_name; do
        if [[ -n "$storage_name" ]]; then
            # Get NFS server IP for display
            local nfs_ip
            nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
            
            local display_string="$storage_name (NFS Server: ${nfs_ip:-unknown})"
            
            # Check if this storage was previously selected
            local status="off"
            if [[ "$selected_target_storage" == "$storage_name" ]]; then
                status="on"
            fi
            
            menu_items+=("$storage_name" "$display_string" "$status")
        fi
    done <<< "$non_cloned_storages"
    
    local dialog_selected=$(mktemp)
    
    # Display dialog
    cmd=(dialog --keep-tite --title "$step_number: Select Target Storage"
         --ok-label "Migrate" --cancel-label "Back to Main" --extra-button --extra-label "Back"
         --radiolist "Choose target non-cloned NFS storage:" 15 80 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Migrate
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select target storage." 8 40
                return 3  # Stay on same step
            fi
            
            selected_target_storage="$selected_option"
            log_info "Selected target storage: $selected_target_storage"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

# Disk migration helper functions (from disk-move)
gather_disks() {
    local vmid="$1"
    local SRC="$2"
    
    # Detect type for proper logging
    local type=$(vm_type "$vmid")

    log_debug "Gathering disks on $SRC for $type $vmid"

    # Parse VM/CT config to find disks on source storage
    # Format: "device: storage:vm-ID-disk-N,options..." (VM) or "device: storage:..." (CT)
    # Extract device name (before first colon)
    log_debug "Querying $type config for disks..."
    if ! mapfile -t disks < <(pve_vm_ct_conf_file "$vmid" | grep "$SRC" | awk -F: '{print $1}'); then
        log_error "Failed to query $type config for $type $vmid"
        return 1
    fi

    # Check if any disks were found
    if [[ ${#disks[@]} -eq 0 ]]; then
        log_debug "No disks found on source storage $SRC for $type $vmid"
        return 1
    else
        log_debug "Found disks for $type $vmid: ${disks[*]}"
        return 0
    fi
}

launch_disk_move() {
    local vmid="$1"
    local SRC="$2"
    local DST="$3"
    
    # Detect VM/CT type and find which node it's on
    local type=$(vm_type "$vmid")
    local vm_node=$(on_which_host_vmct_is "$vmid")
    local vm_node_ip
    
    if [[ -z "$vm_node" ]]; then
        log_error "Cannot find node for VM/CT $vmid"
        return 1
    fi
    
    # Get IP for the node
    vm_node_ip=$(host_ip "$vm_node")
    if [[ -z "$vm_node_ip" ]]; then
        log_error "Cannot resolve IP for node '$vm_node'"
        return 1
    fi
    
    local CONFIG_DIR
    local move_cmd
    local cleanup_cmd
    
    if [[ "$type" == "ct" ]]; then
        CONFIG_DIR="/etc/pve/nodes/$vm_node/lxc"
        log_info "Launching disk move operations for CT $vmid from $SRC to $DST on node $vm_node"
    else
        CONFIG_DIR="/etc/pve/nodes/$vm_node/qemu-server"  
        log_info "Launching disk move operations for VM $vmid from $SRC to $DST on node $vm_node"
    fi
    
    log_info "Using nohup pattern - processes will survive shell exit"
    
    # Initialize disk move PIDs array for this VM/CT
    disk_move_pids=()

    # Process each disk found by gather_disks
    for disk in "${disks[@]}"; do
        log_info "Moving disk $disk for $type $vmid from $SRC to $DST"

        # Construct the appropriate move command based on type (using SSH)
        if [[ "$type" == "ct" ]]; then
            # For containers, use pct move-volume via SSH
            move_cmd="ssh root@$vm_node_ip 'pct move-volume $vmid $disk $DST --delete 0'"
            cleanup_cmd="ssh root@$vm_node_ip \"sed -i '/^unused[0-9]\\\\+:/d' '$CONFIG_DIR/$vmid.conf'\""
        else
            # For VMs, use qm move_disk via SSH
            move_cmd="ssh root@$vm_node_ip 'qm move_disk $vmid $disk $DST --delete 0'"
            cleanup_cmd="ssh root@$vm_node_ip \"sed -i '/^unused[0-9]\\\\+:/d' '$CONFIG_DIR/$vmid.conf'\""
        fi

        # Launch disk move in background with nohup (survives shell exit)
        # Use the pattern from user's research: nohup + PID capture + wait monitoring
        nohup bash -c "
            if $move_cmd >> '$LOG_FILE' 2>&1; then
                echo \"[\$(date '+%F %T')] INFO: Successfully moved disk $disk for $type $vmid\" >> '$LOG_FILE'

                # Clean up unused entries that may be created during move
                $cleanup_cmd >> '$LOG_FILE' 2>&1
                echo \"[\$(date '+%F %T')] INFO: Cleaned unused entries for $type $vmid\" >> '$LOG_FILE'
            else
                echo \"[\$(date '+%F %T')] ERROR: Failed to move disk $disk for $type $vmid\" >> '$LOG_FILE'
                exit 1
            fi
        " >> "$LOG_FILE" 2>&1 &

        # Capture PID right after launch (user's research pattern)
        local pid=$!
        disk_move_pids+=("$pid")
        log_info "Started nohup disk move process for $disk with PID: $pid (survives shell exit)"
    done
}

wait_for_disk_migration_completion() {
    local vmid="$1"
    
    # Detect type for proper logging
    local type=$(vm_type "$vmid")

    log_info "Waiting for disk migration to complete for $type $vmid"
    log_info "Waiting for ${#disk_move_pids[@]} nohup disk move processes: ${disk_move_pids[*]}"
    log_info "These processes will survive shell exit and continue running"

    # Use polling pattern from user's research for nohup processes
    local completed=0
    local failed=0
    
    for pid in "${disk_move_pids[@]}"; do
        log_info "Monitoring nohup process PID: $pid using kill -0 polling"
        
        # Polling loop (from user's research)
        while kill -0 "$pid" 2>/dev/null; do
            log_info "PID $pid still running... ($(date '+%F %T'))"
            sleep 5
        done
        
        # Reap it and fetch the real exit status (from user's research)
        if wait "$pid" 2>/dev/null; then
            log_info "Nohup disk move process $pid completed successfully"
            ((completed++))
        else
            local exit_code=$?
            log_error "Nohup disk move process $pid failed with exit code: $exit_code"
            ((failed++))
        fi
    done

    log_info "Disk migration completed for $type $vmid: $completed successful, $failed failed"
    log_info "All nohup processes monitored and reaped successfully"
    
    # Reset the PIDs array for next VM/CT
    disk_move_pids=()
    
    # Return failure if any disk moves failed
    if [[ $failed -gt 0 ]]; then
        return 1
    fi
    return 0
}

delete_snapshots() {
    local vmid="$1"
    local vm_node vm_node_ip

    log_info "Deleting snapshots for VM $vmid (standard mode)"

    # Find which node the VM is on
    vm_node=$(on_which_host_vmct_is "$vmid")
    if [[ -z "$vm_node" ]]; then
        log_error "Cannot find node for VM $vmid"
        return 1
    fi
    
    # Get IP for the node
    vm_node_ip=$(host_ip "$vm_node")
    if [[ -z "$vm_node_ip" ]]; then
        log_error "Cannot resolve IP for node '$vm_node'"
        return 1
    fi

    # Get array of snapshot names using SSH
    local snaps=()
    mapfile -t snaps < <(ssh root@"$vm_node_ip" "qm listsnapshot '$vmid'" < /dev/null | awk '/`->/ {print $2}' | grep -v '^current$' || true)

    # Process each snapshot
    for snapshot in "${snaps[@]}"; do
        log_info "Deleting snapshot: $snapshot for VM $vmid on node $vm_node"

        # Delete snapshot with force flag (ignores locks) using SSH
        ssh root@"$vm_node_ip" "qm delsnapshot '$vmid' '$snapshot' --force" < /dev/null &>> "$LOG_FILE"

        # Wait for deletion to complete by polling until snapshot is gone
        until ! ssh root@"$vm_node_ip" "qm listsnapshot '$vmid'" < /dev/null | grep -q "^$snapshot"; do
            sleep 2
        done
    done
}

# Step 3: Execute migration
move_storage_step4_execute_migration() {
    local migration_summary=""
    migration_summary="Migration Summary:\n\n"
    migration_summary+="Migration Mode: $move_storage_mode\n"
    migration_summary+="Target Storage: $selected_target_storage\n"
    migration_summary+="VM/CT to migrate: ${#selected_vmcts[@]}\n"
    migration_summary+="Concurrent VM count: $concurrent_vm_disk_move_count\n\n"
    
    for vmct_data in "${selected_vmcts[@]}"; do
        IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
        migration_summary+="- $vm_type $vmid ($name) from $storage_name\n"
    done
    
    migration_summary+="\nProceed with migration?"
    
    # Confirm migration
    if ! dialog --yesno "$migration_summary" 20 80; then
        log_info "User cancelled storage migration"
        return 1
    fi
    
    # For CT migrations, show container shutdown confirmation
    if [[ "$selected_migration_type" == "ct" ]]; then
        if ! confirm_container_stop; then
            log_info "User cancelled container migration during shutdown confirmation"
            return 1
        fi
    fi
    
    log_info "Starting storage migration for ${#selected_vmcts[@]} VM/CT in $move_storage_mode mode"
    
    # Initialize progress tracking
    local completed_vms=0
    local failed_vms=0
    local total_vms=${#selected_vmcts[@]}
    local processing_vms=()
    local pids=()
    
    # Show initial progress
    dialog --infobox "Starting migration of $total_vms VM/CT...\nMode: $move_storage_mode\nConcurrent limit: $concurrent_vm_disk_move_count" 8 60
    sleep 2
    
    # Process VMs in batches based on concurrent limit
    local batch_start=0
    while [[ $batch_start -lt $total_vms ]]; do
        local batch_end=$((batch_start + concurrent_vm_disk_move_count))
        if [[ $batch_end -gt $total_vms ]]; then
            batch_end=$total_vms
        fi
        
        local batch_size=$((batch_end - batch_start))
        local batch_num=$((batch_start / concurrent_vm_disk_move_count + 1))
        log_info "=== BATCH $batch_num DEBUG START ==="
        log_info "Processing batch $batch_num: VMs $((batch_start + 1))-$batch_end of $total_vms (batch size: $batch_size)"
        log_info "Concurrent limit: $concurrent_vm_disk_move_count"
        log_info "Mode: $move_storage_mode"
        
        # Reset batch tracking arrays
        pids=()
        processing_vms=()
        log_info "Reset pids and processing_vms arrays for batch $batch_num"
        
        # Start migrations for current batch
        for ((i = batch_start; i < batch_end; i++)); do
            local vmct_data="${selected_vmcts[$i]}"
            IFS='|' read -r vmid vm_type name storage_name <<< "$vmct_data"
            
            log_info "--- Starting VM $((i + 1))/$total_vms in batch $batch_num ---"
            log_info "Starting migration for $vm_type $vmid ($name) from $storage_name to $selected_target_storage"
            
            # Process single VM migration in background
            (
                # Ignore SIGHUP to survive shell exit
                trap '' HUP
                local vm_success=true
                
                if [[ "$move_storage_mode" == "preserve-snapshots" ]]; then
                    # Preserve snapshots mode - use disk-move functionality
                    log_info "Using preserve-snapshots mode for VM $vmid"
                    
                    # Call the actual disk-move script with preserve-snapshots mode
                    # First, check if VM is powered off (required for this mode)
                    if is_power_on "$vmid"; then
                        log_error "VM $vmid is powered on - preserve-snapshots mode requires powered off VMs"
                        vm_success=false
                    else
                        # Use PVE Perl module for preserve-snapshots migration (shows in GUI)
                        log_info "VM $vmid is powered off - starting preserve-snapshots migration using PVE API"
                        log_info "Migration: $storage_name -> $selected_target_storage (preserving snapshots)"
                        
                        # Call PVE API module for migration (creates GUI-visible task)
                        # Use nohup to ensure task survives shell exit (same as disk-move script)
                        local api_result upid
                        log_info "Starting background migration task (survives shell exit)..."
                        # Call the Perl module directly since pvesh might not recognize our custom endpoint
                        log_info "Calling PveToolsDiskMigration module directly"
                        
                        # Create a temporary Perl script to call our module
                        local perl_script=$(mktemp /tmp/pve_migrate_XXXXXX.pl)
                        cat > "$perl_script" << 'EOF'
#!/usr/bin/perl
use strict;
use warnings;
use lib '/usr/share/perl5';
use PVE::API2::PveToolsDiskMigration;
use PVE::RPCEnvironment;
use PVE::INotify;

# Initialize PVE environment
PVE::INotify::inotify_init();
my $rpcenv = PVE::RPCEnvironment->init('cli');
$rpcenv->init_request();
$rpcenv->set_language($ENV{LANG});
$rpcenv->set_user('root@pam');

# Create migration instance and execute workflow
my $log_dir = '/var/log/pve-disk-migration';
make_path($log_dir) unless -d $log_dir;
my $debug_logfile = "$log_dir/migration-" . time() . ".log";

my $migration = PVE::API2::PveToolsDiskMigration->new(
    vmid          => $ARGV[0],
    src_storage   => $ARGV[1], 
    dst_storage   => $ARGV[2],
    mode          => $ARGV[3],
    debug_level   => $ARGV[4],
    debug_logfile => $debug_logfile,
    authuser      => 'root@pam'
);

# Execute the migration workflow
my $result = $migration->execute_migration_workflow();

# Don't print to stdout - let the script handle logging
EOF
                        chmod +x "$perl_script"
                        
                        # Execute the Perl script
                        api_result=$(perl "$perl_script" "$vmid" "$storage_name" "$selected_target_storage" "preserve-snapshots" "2" 2>&1)
                        local api_success=false
                        
                        if [[ "$api_result" =~ ^UPID: ]] || [[ "$api_result" =~ "Migration completed successfully" ]]; then
                            api_success=true
                            if [[ "$api_result" =~ ^UPID: ]]; then
                                local upid="$api_result"
                                log_info "Migration task started successfully with UPID: $upid"
                            else
                                log_info "Direct migration completed successfully"
                            fi
                        else
                            log_error "Failed to start migration task: $api_result"
                        fi
                        
                        # Clean up temporary script
                        rm -f "$perl_script"
                        
                        if [[ "$api_success" == true ]]; then
                            if [[ "$api_result" =~ ^UPID: ]]; then
                                log_info "Migration task started successfully"
                                log_info "Task UPID: $upid"
                                
                                # Wait for the PVE task to complete
                                log_info "Waiting for migration task to complete for VM $vmid..."
                            else
                                # Direct execution - migration is already complete
                                log_info "Direct migration execution completed for VM $vmid"
                                vm_success=true
                                # Skip task monitoring since migration is already done
                            fi
                            
                            # Only do task monitoring if we have a UPID (API-based task)
                            if [[ "$api_result" =~ ^UPID: ]]; then
                                local task_status=""
                                local task_wait_count=0
                                local max_wait_time=7200  # 2 hours max
                                local check_interval=5    # Check every 5 seconds
                                
                                while [[ $task_wait_count -lt $max_wait_time ]]; do
                                # Check task status using pvesh
                                # Try to get the full task status JSON
                                local task_json
                                task_json=$(pvesh get /nodes/"$(hostname)"/tasks/$upid/status --output-format json 2>/dev/null)
                                
                                if [[ -n "$task_json" ]]; then
                                    # Extract status from JSON
                                    task_status=$(echo "$task_json" | grep -Po '"status"\s*:\s*"\K[^"]+' | head -1)
                                    log_debug "Task monitoring for VM $vmid: status=$task_status, JSON=$task_json"
                                    
                                    case "$task_status" in
                                        "stopped")
                                            # Task completed, check exit status
                                            local exit_status=$(echo "$task_json" | grep -Po '"exitstatus"\s*:\s*"\K[^"]+' | head -1)
                                            if [[ "$exit_status" == "OK" ]]; then
                                                log_info "Migration task completed successfully for VM $vmid (waited ${task_wait_count}s)"
                                                vm_success=true
                                            else
                                                log_error "Migration task failed for VM $vmid with status: $exit_status (waited ${task_wait_count}s)"
                                                vm_success=false
                                            fi
                                            break
                                            ;;
                                        "running")
                                            # Task still running - log progress occasionally
                                            if [[ $((task_wait_count % 30)) -eq 0 ]]; then
                                                log_info "Migration still in progress for VM $vmid (${task_wait_count}s elapsed)..."
                                            fi
                                            ;;
                                        *)
                                            # Unknown status
                                            log_debug "Task status for VM $vmid: $task_status"
                                            ;;
                                    esac
                                else
                                    # Couldn't get task status - may have completed very quickly
                                    # Try alternate method using pvenode
                                    if pvenode task status $upid 2>/dev/null | grep -q "no such task" 2>/dev/null; then
                                        # Task no longer exists - likely completed
                                        log_info "Task no longer exists - checking if migration completed for VM $vmid"
                                        # Check if disks exist in destination storage
                                        if pvesm list "$selected_target_storage" 2>/dev/null | grep -q "vm-$vmid-disk"; then
                                            log_info "Migration appears successful - disks found in destination storage"
                                            vm_success=true
                                        else
                                            log_error "Migration may have failed - no disks found in destination storage"
                                            vm_success=false
                                        fi
                                        break
                                    fi
                                fi
                                
                                # Wait before next check
                                sleep $check_interval
                                ((task_wait_count += check_interval))
                            done
                            
                                if [[ $task_wait_count -ge $max_wait_time ]]; then
                                    log_error "Migration task timeout for VM $vmid after 2 hours"
                                    vm_success=false
                                fi
                            fi  # End of UPID-based task monitoring
                        else
                            log_error "Failed to start migration task via PVE API"
                            log_error "Check logs for details: tail -f $LOG_FILE"
                            vm_success=false
                        fi
                    fi
                else
                    # Detect type once for all logging
                    local type=$(vm_type "$vmid")
                    
                    # Prepare result file for early exit scenarios
                    local result_file="/tmp/pve_migrate_result_${vmid}.tmp"
                    
                    # Standard mode - delete snapshots and use appropriate move command
                    log_info "Using standard mode for $type $vmid"
                    log_info "*** STANDARD MODE: This will actually wait for completion ***"
                    
                    # Check if container needs to be stopped for migration
                    local was_running=false
                    local ct_node="" ct_node_ip=""  # Make available for restart operations
                    
                    # Get node and IP for any container operations
                    if [[ "$type" == "ct" ]]; then
                        ct_node=$(on_which_host_vmct_is "$vmid")
                        if [[ -z "$ct_node" ]]; then
                            log_error "Cannot find node for container $vmid"
                            vm_success=false
                            echo "FAILED:$vmid" > "$result_file"
                            return 1
                        fi
                        
                        ct_node_ip=$(host_ip "$ct_node")
                        if [[ -z "$ct_node_ip" ]]; then
                            log_error "Cannot resolve IP for node '$ct_node'"
                            vm_success=false
                            echo "FAILED:$vmid" > "$result_file"
                            return 1
                        fi
                    fi
                    
                    if [[ "$type" == "ct" ]] && is_power_on "$vmid"; then
                        log_info "Step 0: Stopping running container $vmid for storage migration"
                        
                        if ssh root@"$ct_node_ip" "pct stop '$vmid'" < /dev/null >> "$LOG_FILE" 2>&1; then
                            log_info "Container $vmid stopped successfully on node $ct_node"
                            was_running=true
                        else
                            log_error "Failed to stop container $vmid on node $ct_node - aborting migration"
                            vm_success=false
                            # Early exit if we can't stop the container
                            echo "FAILED:$vmid" > "$result_file"
                            return 1
                        fi
                    fi
                    
                    # Step 1: Delete snapshots
                    log_info "Step 1: Deleting snapshots for $type $vmid"
                    delete_snapshots "$vmid"
                    
                    # Step 2: Gather disks
                    log_info "Step 2: Gathering disks for $type $vmid"
                    if gather_disks "$vmid" "$storage_name"; then
                        # Step 3: Launch disk moves
                        log_info "Step 3: Launching disk move for $type $vmid"
                        launch_disk_move "$vmid" "$storage_name" "$selected_target_storage"
                        
                        # Step 4: Wait for completion
                        log_info "Step 4: Waiting for disk migration completion for $type $vmid"
                        if wait_for_disk_migration_completion "$vmid"; then
                            log_info "Step 4 completed: $type $vmid migration finished successfully"
                            
                            # Step 5: Restart container if it was running before migration
                            if [[ "$type" == "ct" && "$was_running" == "true" ]]; then
                                log_info "Step 5: Restarting container $vmid (was running before migration)"
                                if ssh root@"$ct_node_ip" "pct start '$vmid'" < /dev/null >> "$LOG_FILE" 2>&1; then
                                    log_info "Container $vmid restarted successfully on node $ct_node"
                                else
                                    log_error "Failed to restart container $vmid on node $ct_node - manual intervention may be required"
                                    # Don't fail the migration for restart issues
                                fi
                            fi
                        else
                            log_error "Step 4 failed: $type $vmid migration had errors"
                            vm_success=false
                            
                            # If migration failed but container was stopped, try to restart it
                            if [[ "$type" == "ct" && "$was_running" == "true" ]]; then
                                log_info "Attempting to restart container $vmid after failed migration"
                                if ssh root@"$ct_node_ip" "pct start '$vmid'" < /dev/null >> "$LOG_FILE" 2>&1; then
                                    log_info "Container $vmid restarted successfully after failed migration on node $ct_node"
                                else
                                    log_error "Failed to restart container $vmid after failed migration on node $ct_node - manual intervention required"
                                fi
                            fi
                        fi
                    else
                        log_error "No disks found for $type $vmid on storage $storage_name"
                        vm_success=false
                        
                        # If we stopped the container but can't find disks, restart it
                        if [[ "$type" == "ct" && "$was_running" == "true" ]]; then
                            log_info "Restarting container $vmid after disk discovery failure"
                            if ssh root@"$ct_node_ip" "pct start '$vmid'" < /dev/null >> "$LOG_FILE" 2>&1; then
                                log_info "Container $vmid restarted successfully on node $ct_node"
                            else
                                log_error "Failed to restart container $vmid - manual intervention required"
                            fi
                        fi
                    fi
                fi
                
                # Write result to file for parent process to check
                if [[ "$vm_success" == "true" ]]; then
                    log_info "*** SUBSHELL EXITING: Migration completed successfully for VM $vmid ***"
                    log_info "*** MODE: $move_storage_mode - If preserve-snapshots, actual migration is still running! ***"
                    echo "SUCCESS:$vmid" > "$result_file"
                else
                    log_error "*** SUBSHELL EXITING: Migration failed for VM $vmid ***"
                    echo "FAILED:$vmid" > "$result_file"
                fi
                
                log_info "*** VM $vmid subshell PID $$ is about to exit ***"
            ) < /dev/null > /dev/null 2>&1 &
            
            local current_pid=$!
            pids+=("$current_pid")
            processing_vms+=("$vmid")
            log_info "Started background process for VM $vmid with PID: $current_pid (disowned for shell exit survival)"
            log_info "Current pids array: ${pids[*]}"
            log_info "Current processing_vms array: ${processing_vms[*]}"
        done
        
        # Show progress for current batch
        log_info "All VMs in batch $batch_num started. Total PIDs to wait for: ${#pids[@]}"
        log_info "PIDs: ${pids[*]}"
        log_info "VMs: ${processing_vms[*]}"
        dialog --infobox "Processing batch $batch_num...\nMigrating VMs: ${processing_vms[*]}\nMode: $move_storage_mode\nWaiting for ${#pids[@]} processes..." 12 70
        
        # Wait for current batch to complete
        local batch_completed=0
        local batch_failed=0
        
        log_info "=== WAITING FOR BATCH $batch_num COMPLETION ==="
        log_info "Starting wait loop for ${#pids[@]} processes"
        
        # Disown all processes in this batch so they survive shell exit
        for pid in "${pids[@]}"; do
            disown $pid 2>/dev/null
        done
        log_info "Disowned all ${#pids[@]} processes for shell exit survival"
        
        local wait_count=0
        for i in "${!pids[@]}"; do
            local pid="${pids[$i]}"
            local vmid="${processing_vms[$i]}"
            ((wait_count++))
            log_info "Monitoring process $wait_count/${#pids[@]} (PID: $pid, VM: $vmid)..."
            local wait_start_time=$(date +%s)
            
            # Use polling approach for disowned processes
            while kill -0 "$pid" 2>/dev/null; do
                sleep 2
            done
            
            local wait_end_time=$(date +%s)
            local wait_duration=$((wait_end_time - wait_start_time))
            
            # Since we can't get exit status from disowned process, check result file
            local result_file="/tmp/pve_migrate_result_${vmid}.tmp"
            if [[ -f "$result_file" ]]; then
                local result=$(cat "$result_file" 2>/dev/null)
                rm -f "$result_file"
                
                if [[ "$result" == "SUCCESS:"* ]]; then
                    log_info "Process $pid (VM $vmid) completed successfully (waited ${wait_duration}s)"
                    ((batch_completed++))
                else
                    log_info "Process $pid (VM $vmid) failed (waited ${wait_duration}s)"
                    ((batch_failed++))
                fi
            else
                # If no result file, assume success (process exited normally)
                log_info "Process $pid completed (waited ${wait_duration}s) - assuming success"
                ((batch_completed++))
            fi
        done
        
        log_info "=== BATCH $batch_num WAIT COMPLETE ==="
        log_info "Batch results: $batch_completed successful, $batch_failed failed"
        
        # Update totals
        completed_vms=$((completed_vms + batch_completed))
        failed_vms=$((failed_vms + batch_failed))
        
        # Move to next batch
        batch_start=$batch_end
        
        # Show batch completion
        log_info "Batch $batch_num completed: $batch_completed successful, $batch_failed failed"
        log_info "Updated totals: $completed_vms successful, $failed_vms failed out of $total_vms"
        log_info "=== BATCH $batch_num DEBUG END ==="
        
        dialog --infobox "Batch $batch_num completed: $batch_completed successful, $batch_failed failed\nTotal progress: $completed_vms/$total_vms completed\nMoving to next batch..." 10 60
        sleep 3
    done
    
    # Final summary
    local success_rate=$(( (completed_vms * 100) / total_vms ))
    local summary_msg="Storage Migration Complete!\n\n"
    summary_msg+="Mode: $move_storage_mode\n"
    summary_msg+="Total VMs processed: $total_vms\n"
    summary_msg+="Successful: $completed_vms\n"
    summary_msg+="Failed: $failed_vms\n"
    summary_msg+="Success rate: $success_rate%"
    
    if [[ $failed_vms -gt 0 ]]; then
        summary_msg+="\n\nPlease check logs for details on failed migrations."
        dialog --msgbox "$summary_msg" 16 60
    else
        summary_msg+="\n\nAll migrations completed successfully!"
        dialog --msgbox "$summary_msg" 14 60
    fi
    
    log_info "Storage migration completed: $completed_vms successful, $failed_vms failed out of $total_vms total"
}

# ADD NFS STORAGE WIZARD FUNCTIONS

# NFS wizard specific variables
selected_nfs_ip=""
selected_pool=""
selected_dataset=""
selected_snapshot=""
selected_pve_storage=""
selected_nfs_ip_index=""
selected_pool_index=""
selected_dataset_index=""
selected_snapshot_index=""
nfs_current_step=1
nfs_total_steps=6
oodp_destinations_json=""
oodp_destination_datasets=()

# Helper function to check for REST API authentication failures
# Returns: 0 if authenticated successfully, 1 if authentication failed, 2 for other errors
check_rest_api_auth_failure() {
    local http_status="$1"
    local response_body="$2"
    local function_name="$3"
    
    log_debug "check_rest_api_auth_failure: Called by '$function_name' with status '$http_status'"
    log_debug "check_rest_api_auth_failure: Response body preview: '${response_body:0:50}...'"
    
    if [[ "$http_status" == "401" ]]; then
        log_error "$function_name: REST API authentication failed (HTTP 401)"
        log_debug "check_rest_api_auth_failure: Authentication failure detected, returning special code 10"
        # Return special code 10 for authentication failure (to be handled at top level)
        return 10
    elif [[ "$http_status" == "403" ]]; then
        log_error "$function_name: REST API access forbidden (HTTP 403)"
        log_debug "check_rest_api_auth_failure: Access forbidden detected, returning special code 11"
        # Return special code 11 for access forbidden (to be handled at top level)
        return 11
    elif [[ ! "$http_status" =~ ^[0-9]+$ ]]; then
        log_error "$function_name: Invalid or missing HTTP status code"
        log_debug "check_rest_api_auth_failure: Invalid status code, returning 2"
        return 2
    fi
    
    log_debug "check_rest_api_auth_failure: Authentication successful, returning 0"
    return 0
}

rest_api_connection_test() {
    local response status

    log_debug "rest_api_connection_test: Starting connection test"
    log_debug "rest_api_connection_test: Target IP: $selected_nfs_ip"
    log_debug "rest_api_connection_test: Target port: $rest_api_port"
    log_debug "rest_api_connection_test: Username: $rest_api_user"
    log_debug "rest_api_connection_test: Password length: ${#rest_api_password}"
    
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/conn_test"
    log_debug "rest_api_connection_test: Full URL: $url"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CONNECTION TEST ==============="
    log_debug "curl -k -s --connect-timeout 2 --max-time 5 -w '\\n%{http_code}' -u '${rest_api_user}:${rest_api_password}' '$url'"
    log_debug "====================================================================="
    
    response=$(curl -k -s --connect-timeout 2 --max-time 5 -w "\n%{http_code}" -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>&1)
    local curl_exit=$?
    
    log_debug "rest_api_connection_test: curl exit code: $curl_exit"
    log_debug "rest_api_connection_test: raw response: $response"
    
    status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    log_debug "rest_api_connection_test: HTTP status: $status"
    log_debug "rest_api_connection_test: Response body: $body"

    if [[ "$status" == "200" ]]; then
        log_info "rest_api_connection_test: Connection test successful"
        return 0
    else
        log_error "REST API connection test failed (HTTP $status)"
        log_error "REST API response body: $body"
        
        # Use the same authentication failure detection as other functions
        check_rest_api_auth_failure "$status" "$body" "rest_api_connection_test"
        local auth_check_result=$?
        
        if [[ $auth_check_result -eq 10 ]]; then
            log_debug "rest_api_connection_test: Authentication failure (401), returning special code 10"
            return 10
        elif [[ $auth_check_result -eq 11 ]]; then
            log_debug "rest_api_connection_test: Access forbidden (403), returning special code 11"
            return 11
        else
            return 1
        fi
    fi
}

# Function to get product information from $PRODUCT REST API
get_jovian_product_info() {
    local ip="$1"
    local response status
    
    log_debug "get_jovian_product_info: Getting product info for IP: $ip"
    
    local url="https://$ip:$rest_api_port/api/v4/product"
    log_debug "get_jovian_product_info: Full URL: $url"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR PRODUCT INFO ==============="
    log_debug "curl -k -s -w '\\n%{http_code}' --connect-timeout 2 --max-time 5 -u '${rest_api_user}:${rest_api_password}' '$url'"
    log_debug "==================================================================="
    
    response=$(curl -k -s -w "\n%{http_code}" --connect-timeout 2 --max-time 5 -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>&1)
    local curl_exit=$?
    
    log_debug "get_jovian_product_info: curl exit code: $curl_exit"
    
    if [[ $curl_exit -ne 0 ]]; then
        log_info "get_jovian_product_info: Server $ip is disconnected (curl exit code: $curl_exit)"
        return 1
    fi
    
    status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    log_debug "get_jovian_product_info: HTTP status: $status"
    
    if [[ "$status" == "200" ]]; then
        log_debug "get_jovian_product_info: Successfully got product info for $ip"
        echo "$body"
        return 0
    elif [[ "$status" == "401" ]]; then
        log_error "get_jovian_product_info: Authentication failed for server $ip (HTTP 401)"
        log_error "get_jovian_product_info: Please check REST API credentials in /etc/pve-tools/config.conf"
        log_error "get_jovian_product_info: Verify rest_api_user and rest_api_password are correct"
        return 1
    elif [[ "$status" == "403" ]]; then
        log_error "get_jovian_product_info: Access forbidden for server $ip (HTTP 403)"
        log_error "get_jovian_product_info: The REST API user may not have sufficient permissions"
        return 1
    else
        log_info "get_jovian_product_info: Server $ip returned HTTP $status (may be disconnected or unreachable)"
        return 1
    fi
}

# Function to get list of IP addresses from vmbr interfaces
get_pve_vmbr_ip_address_list() {
    awk '$1=="iface" && $2 ~ /^vmbr/ {f=1; next} $1=="address" && f {sub(/\/.*/, "", $2); print $2; f=0}' /etc/network/interfaces
}

# Function to update connected storage servers info
update_connected_storage_servers() {
    log_info "Updating connected storage servers information"
    
    # Create connected directory if it doesn't exist
    if [[ ! -d "$CONNECTED_STORAGE_SERVERS_DIR" ]]; then
        mkdir -p "$CONNECTED_STORAGE_SERVERS_DIR"
        log_info "Created connected storage servers directory: $CONNECTED_STORAGE_SERVERS_DIR"
    fi
    
    # Update existing files in place (don't remove them - preserves caching)
    log_debug "Updating connected server info files in place"
    
    # Get all available IPs
    local available_ips=()
    while IFS= read -r ip; do
        [[ -n "$ip" ]] && available_ips+=("$ip")
    done < <(get_current_storage_servers_ip)
    
    log_debug "Found ${#available_ips[@]} available IPs: ${available_ips[*]}"
    
    # For each IP, try to get product information
    local ip
    for ip in "${available_ips[@]}"; do
        log_debug "Processing IP: $ip"
        
        # Skip non-IP entries (special menu options)
        if [[ "$ip" == "Enter new IP" || "$ip" == "Manage custom IPs" ]]; then
            continue
        fi
        
        # Skip invalid IP addresses
        if ! validate_ip "$ip"; then
            log_debug "Skipping invalid IP format: $ip"
            continue
        fi
        
        local product_info
        local current_timestamp=$(date +%s)
        local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
        
        # Check if we have recent connection state info to avoid unnecessary API calls
        local skip_api_call=false
        if [[ -f "$info_file" ]]; then
            local last_check_timestamp
            local connection_state
            
            # Read existing connection state and timestamp
            if connection_state=$(grep "^connection_state=" "$info_file" 2>/dev/null | cut -d'=' -f2) && \
               last_check_timestamp=$(grep "^last_check_timestamp=" "$info_file" 2>/dev/null | cut -d'=' -f2); then
                
                # If disconnected and checked within last 5 minutes (300 seconds), skip API call
                local time_diff=$((current_timestamp - last_check_timestamp))
                if [[ "$connection_state" == "disconnected" && $time_diff -lt 300 ]]; then
                    log_debug "Skipping API call for $ip (disconnected $time_diff seconds ago)"
                    skip_api_call=true
                fi
            fi
        fi
        
        if [[ "$skip_api_call" == "false" ]] && product_info=$(get_jovian_product_info "$ip" 2>/dev/null); then
            # Parse JSON response and extract fields
            local vendor_name=$(echo "$product_info" | grep -o '"vendor_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"vendor_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local name=$(echo "$product_info" | grep -o '"name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local host_name=$(echo "$product_info" | grep -o '"host_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"host_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local version_name=$(echo "$product_info" | grep -o '"version_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"version_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local serial_number=$(echo "$product_info" | grep -o '"serial_number"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"serial_number"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local up=$(echo "$product_info" | grep -o '"up"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"up"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            local release_date=$(echo "$product_info" | grep -o '"release_date"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"release_date"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
            
            # Preserve existing node_role if it exists
            local existing_node_role=""
            if [[ -f "$info_file" ]]; then
                existing_node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
                log_debug "update_connected_storage_servers: Preserving existing role '$existing_node_role' for IP '$ip'"
            fi
            
            # Create info file for this IP with connected state
            cat > "$info_file" << EOF
connection_state=connected
last_check_timestamp=$current_timestamp
vendor_name=$vendor_name
name=$name
host_name=$host_name
version_name=$version_name
serial_number=$serial_number
up=$up
release_date=$release_date
EOF
            
            # Append node_role if it existed
            if [[ -n "$existing_node_role" ]]; then
                echo "node_role=$existing_node_role" >> "$info_file"
                log_debug "update_connected_storage_servers: Preserved role '$existing_node_role' for IP '$ip'"
            fi
            log_info "Created server info file for IP $ip with hostname: $host_name (connected)"
        else
            # API call was attempted but failed OR was skipped due to recent failure
            if [[ "$skip_api_call" == "false" ]]; then
                # Preserve existing node_role for disconnected IP as well
                local existing_node_role=""
                if [[ -f "$info_file" ]]; then
                    existing_node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
                    log_debug "update_connected_storage_servers: Preserving existing role '$existing_node_role' for disconnected IP '$ip'"
                fi
                
                # API call was attempted but failed - create/update info file for disconnected IP
                cat > "$info_file" << EOF
connection_state=disconnected
last_check_timestamp=$current_timestamp
vendor_name=
name=
host_name=disconnected
version_name=
serial_number=
up=
release_date=
EOF
                
                # Append node_role if it existed
                if [[ -n "$existing_node_role" ]]; then
                    echo "node_role=$existing_node_role" >> "$info_file"
                    log_debug "update_connected_storage_servers: Preserved role '$existing_node_role' for disconnected IP '$ip'"
                fi
                
                log_info "Created server info file for IP $ip (disconnected)"
            else
                # API call was skipped due to recent disconnected state - keep existing file unchanged
                log_debug "Preserving existing disconnected state for IP $ip"
            fi
        fi
    done
    
    # Clean up files for IPs that are no longer in available list
    log_debug "Cleaning up orphaned connection files"
    for info_file in "$CONNECTED_STORAGE_SERVERS_DIR"/*; do
        [[ -f "$info_file" ]] || continue
        local file_ip=$(basename "$info_file")
        local found=false
        for available_ip in "${available_ips[@]}"; do
            if [[ "$available_ip" == "$file_ip" ]]; then
                found=true
                break
            fi
        done
        if [[ "$found" == false ]]; then
            log_debug "Removing orphaned connection file for IP: $file_ip"
            rm -f "$info_file"
        fi
    done
}

# Function to check if an IP is in connected state
# Returns: 0 if connected, 1 if disconnected or not found
check_ip_connection_state() {
    local ip="$1"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    
    if [[ -f "$info_file" ]]; then
        local connection_state=$(grep "^connection_state=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        if [[ "$connection_state" == "connected" ]]; then
            log_debug "IP $ip is connected"
            return 0
        else
            log_debug "IP $ip is disconnected"
            return 1
        fi
    else
        log_debug "No connection info found for IP $ip"
        return 1
    fi
}

# Function to attempt reconnection to a disconnected IP
# Returns: 0 if successful, 1 if failed
attempt_reconnect_ip() {
    local ip="$1"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    local current_timestamp=$(date +%s)
    
    log_info "Attempting to reconnect to IP: $ip"
    
    local product_info
    if product_info=$(get_jovian_product_info "$ip" 2>/dev/null); then
        # Parse JSON response and extract fields
        local vendor_name=$(echo "$product_info" | grep -o '"vendor_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"vendor_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local name=$(echo "$product_info" | grep -o '"name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local host_name=$(echo "$product_info" | grep -o '"host_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"host_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local version_name=$(echo "$product_info" | grep -o '"version_name"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"version_name"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local serial_number=$(echo "$product_info" | grep -o '"serial_number"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"serial_number"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local up=$(echo "$product_info" | grep -o '"up"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"up"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        local release_date=$(echo "$product_info" | grep -o '"release_date"[[:space:]]*:[[:space:]]*"[^"]*"' | sed 's/.*"release_date"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/')
        
        # Preserve existing node_role if it exists  
        local existing_node_role=""
        if [[ -f "$info_file" ]]; then
            existing_node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
            log_debug "attempt_reconnect_ip: Preserving existing role '$existing_node_role' for IP '$ip'"
        fi
        
        # Update info file with connected state
        cat > "$info_file" << EOF
connection_state=connected
last_check_timestamp=$current_timestamp
vendor_name=$vendor_name
name=$name
host_name=$host_name
version_name=$version_name
serial_number=$serial_number
up=$up
release_date=$release_date
EOF
        
        # Append node_role if it existed
        if [[ -n "$existing_node_role" ]]; then
            echo "node_role=$existing_node_role" >> "$info_file"
            log_debug "attempt_reconnect_ip: Preserved role '$existing_node_role' for IP '$ip'"
        fi
        log_info "Successfully reconnected to IP $ip with hostname: $host_name"
        return 0
    else
        # Preserve existing node_role for failed reconnection as well
        local existing_node_role=""
        if [[ -f "$info_file" ]]; then
            existing_node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
            log_debug "attempt_reconnect_ip: Preserving existing role '$existing_node_role' for failed reconnection to IP '$ip'"
        fi
        
        # Update timestamp for disconnected state
        cat > "$info_file" << EOF
connection_state=disconnected
last_check_timestamp=$current_timestamp
vendor_name=
name=
host_name=disconnected
version_name=
serial_number=
up=
release_date=
EOF
        
        # Append node_role if it existed
        if [[ -n "$existing_node_role" ]]; then
            echo "node_role=$existing_node_role" >> "$info_file"
            log_debug "attempt_reconnect_ip: Preserved role '$existing_node_role' for failed reconnection to IP '$ip'"
        fi
        
        # More specific error message based on what failed
        # Check if it was an authentication issue by checking the last log messages
        if grep -q "Authentication failed" /var/log/pve-tools/pve-tools.log 2>/dev/null | tail -n 5; then
            log_error "Failed to reconnect to IP $ip: Authentication failed"
            log_error "Please verify REST API credentials in /etc/pve-tools/config.conf"
        else
            log_info "Failed to reconnect to IP $ip: Server may be offline or unreachable"
        fi
        return 1
    fi
}

# Function to mark IP as disconnected
mark_ip_as_disconnected() {
    local ip="$1"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    local current_timestamp=$(date +%s)
    
    log_debug "Marking IP $ip as disconnected"
    
    # Preserve existing node_role if it exists
    local existing_node_role=""
    if [[ -f "$info_file" ]]; then
        existing_node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        log_debug "mark_ip_as_disconnected: Preserving existing role '$existing_node_role' for IP '$ip'"
    fi
    
    # Update info file with disconnected state
    cat > "$info_file" << EOF
connection_state=disconnected
last_check_timestamp=$current_timestamp
vendor_name=
name=
host_name=disconnected
version_name=
serial_number=
up=
release_date=
EOF
    
    # Append node_role if it existed
    if [[ -n "$existing_node_role" ]]; then
        echo "node_role=$existing_node_role" >> "$info_file"
        log_debug "mark_ip_as_disconnected: Preserved role '$existing_node_role' for IP '$ip'"
    fi
    
    log_debug "IP $ip marked as disconnected"
}

# Function to get hostname for IP from connected servers
get_hostname_for_ip() {
    local ip="$1"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    
    if [[ -f "$info_file" ]]; then
        local host_name=$(grep "^host_name=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        if [[ -n "$host_name" ]]; then
            echo "$host_name"
            return 0
        fi
    fi
    
    # Return "Unknown" if no hostname found
    echo "Unknown"
    return 1
}

# Function to get node role for IP from connected servers
get_node_role_for_ip() {
    local ip="$1"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    
    log_debug "get_node_role_for_ip: Looking up role for IP '$ip'"
    
    if [[ -f "$info_file" ]]; then
        local node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        if [[ -n "$node_role" ]]; then
            log_debug "get_node_role_for_ip: Found role '$node_role' for IP '$ip'"
            echo "$node_role"
            return 0
        fi
    fi
    
    # Return "unassigned" if no role found
    log_debug "get_node_role_for_ip: No role found for IP '$ip', returning 'unassigned'"
    echo "unassigned"
    return 1
}

# Function to set node role for IP in connected servers
set_node_role_for_ip() {
    local ip="$1"
    local role="$2"
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$ip"
    
    log_info "set_node_role_for_ip: Setting role '$role' for IP '$ip'"
    
    if [[ ! -f "$info_file" ]]; then
        log_error "set_node_role_for_ip: Info file not found for IP '$ip'"
        return 1
    fi
    
    # Check if node_role line already exists
    if grep -q "^node_role=" "$info_file" 2>/dev/null; then
        # Update existing role
        log_debug "set_node_role_for_ip: Updating existing role for IP '$ip'"
        sed -i "s/^node_role=.*/node_role=$role/" "$info_file"
    else
        # Add new role line
        log_debug "set_node_role_for_ip: Adding new role for IP '$ip'"
        echo "node_role=$role" >> "$info_file"
    fi
    
    log_info "set_node_role_for_ip: Successfully set role '$role' for IP '$ip'"
    return 0
}

# Function to select node role with dialog
select_node_role() {
    local ip="$1"
    local current_role
    current_role=$(get_node_role_for_ip "$ip")
    
    log_info "select_node_role: Starting role selection for IP '$ip', current role: '$current_role'"
    
    local temp_output=$(mktemp)
    local default_item=""
    
    # Set default selection based on current role
    case "$current_role" in
        "ha-cluster-node")
            default_item="--default-item 1"
            ;;
        "backup-dr-node")
            default_item="--default-item 2"
            ;;
        *)
            default_item="--default-item 1"  # Default to ha-cluster-node
            ;;
    esac
    
    log_debug "select_node_role: Displaying role selection dialog for IP '$ip'"
    
    dialog --keep-tite --title "Select Node Role" \
        --ok-label "Select" --cancel-label "Skip" $default_item \
        --menu "Select the role for storage node $ip:" 12 60 5 \
        "1" "ha-cluster-node    (High Availability Cluster Node)" \
        "2" "backup-dr-node     (Backup/Disaster Recovery Node)" 2>"$temp_output"
    
    local dialog_result=$?
    local selected_option=$(< "$temp_output")
    rm -f "$temp_output"
    
    log_debug "select_node_role: Dialog result: $dialog_result, selected: '$selected_option'"
    
    if [[ $dialog_result -eq 0 && -n "$selected_option" ]]; then
        local selected_role=""
        case "$selected_option" in
            "1")
                selected_role="ha-cluster-node"
                ;;
            "2")
                selected_role="backup-dr-node"
                ;;
            *)
                log_error "select_node_role: Invalid selection: '$selected_option'"
                return 1
                ;;
        esac
        
        log_info "select_node_role: User selected role '$selected_role' for IP '$ip'"
        
        # Set the role
        if set_node_role_for_ip "$ip" "$selected_role"; then
            log_info "select_node_role: Successfully set role '$selected_role' for IP '$ip'"
            dialog --msgbox "Node role set to: $selected_role" 6 50
            return 0
        else
            log_error "select_node_role: Failed to set role '$selected_role' for IP '$ip'"
            dialog --msgbox "Failed to set node role. Please try again." 8 50
            return 1
        fi
    else
        log_info "select_node_role: User cancelled or skipped role selection for IP '$ip'"
        return 2  # User cancelled or skipped
    fi
}

get_custom_ips() {
    if [[ -f "$CONFIG_FILE" ]]; then
        grep "^custom_ips=" "$CONFIG_FILE" 2>/dev/null | cut -d'=' -f2- | tr ',' '\n' | grep -v '^$'
    fi
}

validate_ip() {
    local ip="$1"
    
    # Check basic format
    if [[ ! "$ip" =~ ^([0-9]{1,3}\.){3}[0-9]{1,3}$ ]]; then
        return 1
    fi
    
    # Check each octet is in valid range (0-255)
    IFS='.' read -ra octets <<< "$ip"
    for octet in "${octets[@]}"; do
        # Remove leading zeros and check range
        octet=$((10#$octet))
        if [[ $octet -lt 0 || $octet -gt 255 ]]; then
            return 1
        fi
    done
    
    return 0
}

# Validate alphanumeric string with underscore and hyphen
# Returns: 0=valid, 1=invalid (contains invalid characters or spaces)
validate_alphanumeric_name() {
    local input="$1"
    
    # Check if empty
    if [[ -z "$input" ]]; then
        return 1
    fi
    
    # Check if contains only alphanumeric, underscore, and hyphen
    if [[ ! "$input" =~ ^[a-zA-Z0-9_-]+$ ]]; then
        return 1
    fi
    
    return 0
}

# Enhanced IP validation with REST API connectivity testing
# Returns: 0=valid, 1=invalid format, 2=unreachable, 3=auth failure, 4=other error
validate_ip_with_connectivity() {
    local ip="$1"
    local error_msg_var="$2"  # Variable name to store error message
    
    log_debug "validate_ip_with_connectivity: Testing IP: $ip"
    
    # Step 1: Basic IP format validation
    if ! validate_ip "$ip"; then
        local msg="Invalid IP address format. Please enter a valid IP address (e.g., 192.168.1.100)."
        if [[ -n "$error_msg_var" ]]; then
            printf -v "$error_msg_var" "%s" "$msg"
        fi
        log_debug "validate_ip_with_connectivity: Invalid IP format: $ip"
        return 1
    fi
    
    # Step 2: REST API connectivity test
    log_debug "validate_ip_with_connectivity: Testing connectivity to $ip"
    
    # Test connection using get_jovian_product_info with timeout
    if get_jovian_product_info "$ip" >/dev/null 2>&1; then
        log_debug "validate_ip_with_connectivity: Successfully connected to $ip"
        if [[ -n "$error_msg_var" ]]; then
            printf -v "$error_msg_var" ""
        fi
        return 0
    else
        local exit_code=$?
        log_debug "validate_ip_with_connectivity: Connection failed to $ip, exit code: $exit_code"
        
        # Test if it's a basic network connectivity issue vs authentication
        local test_url="https://$ip:$rest_api_port/api/v4/product"
        
        # Log the FULL curl command for manual testing
        log_debug "=============== FULL CURL COMMAND FOR CONNECTIVITY TEST ==============="
        log_debug "curl -k -s -w '\\n%{http_code}' --connect-timeout 2 --max-time 5 '$test_url'"
        log_debug "======================================================================="
        
        local test_response
        test_response=$(curl -k -s -w "\n%{http_code}" --connect-timeout 2 --max-time 5 \
            "$test_url" 2>&1)
        local curl_exit=$?
        
        if [[ $curl_exit -ne 0 ]]; then
            # Network/timeout error
            local msg="Cannot connect to $PRODUCT at $ip. Please verify the IP address and ensure the server is reachable."
            if [[ -n "$error_msg_var" ]]; then
                printf -v "$error_msg_var" "%s" "$msg"
            fi
            log_debug "validate_ip_with_connectivity: Network error for $ip"
            return 2
        else
            # Got HTTP response but authentication might have failed
            local status=$(echo "$test_response" | tail -n1)
            if [[ "$status" == "401" || "$status" == "403" ]]; then
                local msg="Authentication failed for $PRODUCT at $ip. Please check the REST API credentials in configuration."
                if [[ -n "$error_msg_var" ]]; then
                    printf -v "$error_msg_var" "%s" "$msg"
                fi
                log_debug "validate_ip_with_connectivity: Authentication failed for $ip (HTTP $status)"
                return 3
            else
                # Other HTTP error
                local msg="Connection to $PRODUCT at $ip failed (HTTP $status). Please verify the server configuration."
                if [[ -n "$error_msg_var" ]]; then
                    printf -v "$error_msg_var" "%s" "$msg"
                fi
                log_debug "validate_ip_with_connectivity: HTTP error for $ip (status $status)"
                return 4
            fi
        fi
    fi
}

ip_already_exists() {
    local check_ip="$1"
    local existing_ips
    
    # Check custom IPs from config
    existing_ips=$(get_custom_ips)
    if [[ -n "$existing_ips" ]]; then
        while IFS= read -r ip; do
            if [[ "$ip" == "$check_ip" ]]; then
                return 0
            fi
        done <<< "$existing_ips"
    fi
    
    # Check IPs from storage.cfg
    local storage_cfg="/etc/pve/storage.cfg"
    if [[ -f "$storage_cfg" ]]; then
        while IFS= read -r ip; do
            if [[ "$ip" == "$check_ip" ]]; then
                return 0
            fi
        done < <(grep server "$storage_cfg" | awk '{print $2}')
    fi
    
    return 1
}

add_custom_ip() {
    local new_ip="$1"
    local current_ips
    current_ips=$(get_custom_ips | tr '\n' ',' | sed 's/,$//')
    
    # Check if IP already exists
    if [[ -n "$current_ips" && "$current_ips" == *"$new_ip"* ]]; then
        return 0
    fi
    
    # Add new IP
    if [[ -n "$current_ips" ]]; then
        current_ips="$current_ips,$new_ip"
    else
        current_ips="$new_ip"
    fi
    
    # Update config file
    if grep -q "^custom_ips=" "$CONFIG_FILE" 2>/dev/null; then
        sed -i "s/^custom_ips=.*/custom_ips=$current_ips/" "$CONFIG_FILE"
    else
        echo "custom_ips=$current_ips" >> "$CONFIG_FILE"
    fi
    
    log_info "Added custom IP: $new_ip"
}

remove_custom_ip() {
    local ip_to_remove="$1"
    local current_ips
    current_ips=$(get_custom_ips | tr '\n' ',' | sed 's/,$//')
    
    # Remove the IP
    current_ips=$(echo "$current_ips" | sed "s/$ip_to_remove,//g" | sed "s/,$ip_to_remove//g" | sed "s/^$ip_to_remove$//g")
    
    # Update config file
    if [[ -n "$current_ips" ]]; then
        sed -i "s/^custom_ips=.*/custom_ips=$current_ips/" "$CONFIG_FILE"
    else
        sed -i "/^custom_ips=/d" "$CONFIG_FILE"
    fi
    
    log_info "Removed custom IP: $ip_to_remove"
}

manage_custom_ips() {
    local custom_ips_array=()
    local menu_items=()
    
    while IFS= read -r line; do
        [[ -n "$line" ]] && custom_ips_array+=("$line")
    done <<< "$(get_custom_ips)"
    
    if [[ ${#custom_ips_array[@]} -eq 0 ]]; then
        dialog --msgbox "No custom IPs found." 8 40
        return
    fi
    
    for ((i=0; i<${#custom_ips_array[@]}; i++)); do
        local ip="${custom_ips_array[i]}"
        local hostname
        if [[ "$ip" == "Enter new IP" || "$ip" == "Manage custom IPs" ]]; then
            hostname=""
        else
            hostname=$(get_hostname_for_ip "$ip")
        fi
        if [[ -n "$hostname" && "$hostname" != "Unknown" ]]; then
            menu_items+=("$i" "$(printf "%-16s - %s" "$ip" "$hostname")")
        else
            menu_items+=("$i" "$(printf "%-16s" "$ip")")
        fi
    done
    
    local selected_ip
    local temp_output=$(mktemp)
    dialog --keep-tite --title "Manage Custom IPs" \
        --ok-label "Delete" --cancel-label "Back" \
        --menu "Select IP to delete:" 15 50 8 \
        "${menu_items[@]}" 2>"$temp_output"
    
    local dialog_result=$?
    selected_ip=$(< "$temp_output")
    rm -f "$temp_output"
    
    # Only proceed if user selected an IP (not cancelled)
    if [[ $dialog_result -eq 0 && -n "$selected_ip" ]]; then
        local ip_to_delete="${custom_ips_array[selected_ip]}"
        # Ask for confirmation before deletion
        if dialog --yesno "Delete IP: $ip_to_delete?" 8 40; then
            remove_custom_ip "$ip_to_delete"
            # Only show success message after actual deletion
            dialog --msgbox "IP $ip_to_delete deleted." 8 40
        fi
    fi
}

get_current_storage_servers_ip() {
    local storage_cfg="/etc/pve/storage.cfg"
    local ips=()
    while IFS= read -r line; do
        [[ -n "$line" ]] && ips+=("$line")
    done < <(grep server "$storage_cfg" | awk '{print $2}')

    # Add custom IPs from config file
    local custom_ips
    custom_ips=$(get_custom_ips)
    if [[ -n "$custom_ips" ]]; then
        while IFS= read -r line; do
            [[ -n "$line" ]] && ips+=("$line")
        done <<< "$custom_ips"
    fi

    # Remove duplicates
    readarray -t uniq_ips < <(printf "%s\n" "${ips[@]}" | sort -u)
    uniq_ips+=("Enter new IP")
    uniq_ips+=("Manage custom IPs")

    printf '%s\n' "${uniq_ips[@]}"
}

get_pool_names() {
    local addr="$1"
    local response
    
    # Check connection state first
    if ! check_ip_connection_state "$addr"; then
        log_info "Skipping get_pool_names for disconnected IP: $addr"
        return 1
    fi

    response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" -H 'Content-Type: application/json' \
        "https://$addr:$rest_api_port/api/v4/pools")

    local curl_exit=$?

    if [[ $curl_exit -ne 0 ]]; then
        log_error "test_nfs_connection: Failed to connect to NFS server at $addr:$rest_api_port"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "test_nfs_connection: Empty response from server"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get("data", [])
    if not pools:
        sys.exit(1)
    for pool in pools:
        name = pool.get("name")
        if name:
            print(name)
except Exception:
    sys.exit(1)
'
}

get_datasets_in_pool() {
    local pool_name="$1"
    local nfs_ip="${2:-$selected_nfs_ip}"  # Use provided IP or default to selected_nfs_ip
    local response

    response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" -H 'Content-Type: application/json' \
        "https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_datasets_in_pool: Failed to connect to server for datasets"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_datasets_in_pool: Empty response from server (datasets)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)
    for d in entries:
        name = d.get("name")
        origin = d.get("origin")
        if name and origin is None:
            print(name)
except Exception:
    sys.exit(1)
'
}

get_snapshots_in_dataset() {
    local pool_name="$1"
    local dataset_name="$2"
    local response
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR GET SNAPSHOTS IN DATASET ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
    log_debug "=============================================================================="

    response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" -H 'Content-Type: application/json' \
        "$url")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_snapshots_in_dataset: Failed to connect to server for snapshots"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_snapshots_in_dataset: Empty response from server (snapshots)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)

    # Create list of snapshots with creation timestamps
    snapshots = []
    for s in entries:
        name = s.get("name")
        creation = s.get("properties", {}).get("creation")
        if name and creation:
            snapshots.append((int(creation), name))

    # Sort by creation timestamp in descending order (newest first)
    snapshots.sort(reverse=True)

    # Print snapshot names in sorted order
    for _, name in snapshots:
        print(name)
except Exception:
    sys.exit(1)
'
}

get_snapshots_with_times() {
    local pool_name="$1"
    local dataset_name="$2"
    local response
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR GET SNAPSHOTS WITH TIMES ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
    log_debug "=============================================================================="

    response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" -H 'Content-Type: application/json' \
        "$url")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_snapshots_with_times: Failed to connect to server for snapshots"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_snapshots_with_times: Empty response from server (snapshots)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)

    # Create list of snapshots with creation timestamps
    snapshots = []
    for s in entries:
        name = s.get("name")
        creation = s.get("properties", {}).get("creation")
        if name and creation:
            snapshots.append((int(creation), name))

    # Sort by creation timestamp in descending order (newest first)
    snapshots.sort(reverse=True)

    # Print snapshot names and creation times tab-separated
    for creation, name in snapshots:
        print(f"{name}\t{creation}")
except Exception:
    sys.exit(1)
'
}

get_current_timestamp() {
    date +%s
}

calculate_age_seconds() {
    local current_time="$1"
    local creation_time="$2"
    echo $((current_time - creation_time))
}

format_age_human() {
    local age_seconds="$1"
    local age_minutes=$((age_seconds / 60))
    local age_hours=$((age_minutes / 60))
    local age_days=$((age_hours / 24))
    local age_years=$((age_days / 365))
    local output=""
    
    if [[ $age_minutes -lt 60 ]]; then
        echo "${age_minutes}min"
    elif [[ $age_hours -lt 24 ]]; then
        local remaining_minutes=$((age_minutes % 60))
        output="${age_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    elif [[ $age_days -lt 365 ]]; then
        local remaining_hours=$((age_hours % 24))
        local remaining_minutes=$((age_minutes % 60))
        output="${age_days}d"
        [[ $remaining_hours -gt 0 ]] && output+=" ${remaining_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    else
        local remaining_days=$((age_days % 365))
        local remaining_hours=$((age_hours % 24))
        local remaining_minutes=$((age_minutes % 60))
        output="${age_years}y"
        [[ $remaining_days -gt 0 ]] && output+=" ${remaining_days}d"
        [[ $remaining_hours -gt 0 ]] && output+=" ${remaining_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    fi
}

# Get PVE storage name for a given dataset path
# Args: $1 = server IP, $2 = dataset name (can be "Pool-0/dataset1" or just "dataset1")
# Returns: storage name if found, empty string if not
get_pve_storage_for_dataset() {
    local server_ip="$1"
    local dataset="$2"
    local storage_name=""
    
    # We need to check multiple possible export path formats:
    # 1. With pool prefix: /Pool-0/dataset1
    # 2. Without pool prefix: /dataset1 (just the dataset name)
    local export_path_with_pool="/$dataset"
    local export_path_without_pool=""
    
    # Extract just the dataset name (part after the last /)
    if [[ "$dataset" =~ / ]]; then
        export_path_without_pool="/$(echo "$dataset" | sed 's/.*\///')"
    else
        export_path_without_pool="/$dataset"
    fi
    
    log_debug "get_pve_storage_for_dataset: Checking for dataset '$dataset' on server '$server_ip'"
    log_debug "  Export path with pool: '$export_path_with_pool'"
    log_debug "  Export path without pool: '$export_path_without_pool'"
    
    # Parse storage.cfg to find matching NFS storage
    local in_nfs_block=false
    local current_storage=""
    local current_server=""
    local current_export=""
    
    while IFS= read -r line; do
        # Check if this is an NFS storage block
        if [[ "$line" =~ ^nfs:[[:space:]]*(.+)$ ]]; then
            # If we were in a block, check if it matches
            if [[ "$in_nfs_block" == true && -n "$current_storage" ]]; then
                if [[ "$current_server" == "$server_ip" ]] && \
                   [[ "$current_export" == "$export_path_with_pool" || "$current_export" == "$export_path_without_pool" ]]; then
                    storage_name="$current_storage"
                    log_debug "  Found match: storage='$current_storage', server='$current_server', export='$current_export'"
                    break
                fi
            fi
            # Start new block
            in_nfs_block=true
            current_storage="${BASH_REMATCH[1]}"
            current_server=""
            current_export=""
        elif [[ "$in_nfs_block" == true ]]; then
            # Check for server line
            if [[ "$line" =~ ^[[:space:]]+server[[:space:]]+(.+)$ ]]; then
                current_server="${BASH_REMATCH[1]}"
            # Check for export line
            elif [[ "$line" =~ ^[[:space:]]+export[[:space:]]+(.+)$ ]]; then
                current_export="${BASH_REMATCH[1]}"
            # Check for empty line (end of block)
            elif [[ "$line" =~ ^[[:space:]]*$ ]]; then
                # Check if this block matches
                if [[ "$current_server" == "$server_ip" ]] && \
                   [[ "$current_export" == "$export_path_with_pool" || "$current_export" == "$export_path_without_pool" ]]; then
                    storage_name="$current_storage"
                    log_debug "  Found match: storage='$current_storage', server='$current_server', export='$current_export'"
                    break
                fi
                in_nfs_block=false
                current_storage=""
                current_server=""
                current_export=""
            fi
        fi
    done < /etc/pve/storage.cfg
    
    # Check last block if we reached EOF
    if [[ "$in_nfs_block" == true && -n "$current_storage" ]]; then
        if [[ "$current_server" == "$server_ip" ]] && \
           [[ "$current_export" == "$export_path_with_pool" || "$current_export" == "$export_path_without_pool" ]]; then
            storage_name="$current_storage"
            log_debug "  Found match at EOF: storage='$current_storage', server='$current_server', export='$current_export'"
        fi
    fi
    
    echo "$storage_name"
}

nfs_step1_select_ip() {
    log_debug "nfs_step1_select_ip: Function started"
    local line ips=() menu_items=()
    while IFS= read -r line; do
        ips+=("$line")
    done < <(get_current_storage_servers_ip)
    log_debug "nfs_step1_select_ip: Got ${#ips[@]} IPs: ${ips[*]}"

    if [ "${#ips[@]}" -eq 0 ]; then
        dialog --msgbox "No IPs found." 8 40
        return 1
    fi

    # Separate actual IPs from special menu options
    local actual_ips=()
    local special_options=()
    local ip_data=()  # Array to store IP info for sorting
    
    for ip in "${ips[@]}"; do
        if [[ "$ip" == "Enter new IP" || "$ip" == "Manage custom IPs" ]]; then
            special_options+=("$ip")
        else
            actual_ips+=("$ip")
            local hostname=$(get_hostname_for_ip "$ip")
            local node_role=$(get_node_role_for_ip "$ip")
            log_debug "nfs_step1_select_ip: IP '$ip' has hostname '$hostname' and role '$node_role'"
            
            # Store IP data with role priority for sorting
            # Format: "role_priority|ip|hostname|role"
            local role_priority
            case "$node_role" in
                "ha-cluster-node")
                    role_priority="1"  # Highest priority (will appear first)
                    ;;
                "backup-dr-node")
                    role_priority="2"  # Second priority
                    ;;
                *)
                    role_priority="3"  # Lowest priority (unassigned)
                    ;;
            esac
            
            # Convert IP to sortable format (pad octets with zeros)
            local -a octets
            IFS='.' read -ra octets <<< "$ip"
            local sortable_ip
            sortable_ip=$(printf "%03d.%03d.%03d.%03d" "${octets[0]}" "${octets[1]}" "${octets[2]}" "${octets[3]}")
            
            ip_data+=("${role_priority}|${sortable_ip}|${ip}|${hostname}|${node_role}")
        fi
    done
    
    # Sort IP data by role priority (ascending) and then by IP
    IFS=$'\n' sorted_ip_data=($(printf '%s\n' "${ip_data[@]}" | sort))
    unset IFS
    
    log_debug "nfs_step1_select_ip: Sorted ${#sorted_ip_data[@]} IPs by role and IP address"
    
    # Build sorted IP array with special options at the end
    local sorted_ips=()
    for data in "${sorted_ip_data[@]}"; do
        local ip=$(echo "$data" | cut -d'|' -f3)
        sorted_ips+=("$ip")
    done
    
    # Add special options at the end
    for option in "${special_options[@]}"; do
        sorted_ips+=("$option")
    done
    
    # Replace original ips array with sorted one
    ips=("${sorted_ips[@]}")
    
    # Build menu items with proper display formatting
    local i
    for ((i=0; i<${#ips[@]}; i++)); do
        local ip="${ips[i]}"
        local hostname
        local node_role
        
        # Check if this is a special menu option (not an actual IP)
        if [[ "$ip" == "Enter new IP" || "$ip" == "Manage custom IPs" ]]; then
            # Special menu options - don't show role
            menu_items+=("$i" "$ip")
        else
            # Regular IP addresses - show hostname and role
            hostname=$(get_hostname_for_ip "$ip")
            node_role=$(get_node_role_for_ip "$ip")
            
            # Build display string with IP, hostname, and role
            if [[ -n "$hostname" && "$hostname" != "Unknown" ]]; then
                if [[ -n "$node_role" && "$node_role" != "unassigned" ]]; then
                    menu_items+=("$i" "$(printf "%-16s - %-20s [%s]" "$ip" "$hostname" "$node_role")")
                else
                    menu_items+=("$i" "$(printf "%-16s - %-20s [unassigned]" "$ip" "$hostname")")
                fi
            else
                if [[ -n "$node_role" && "$node_role" != "unassigned" ]]; then
                    menu_items+=("$i" "$(printf "%-16s [%s]" "$ip" "$node_role")")
                else
                    menu_items+=("$i" "$(printf "%-16s [unassigned]" "$ip")")
                fi
            fi
        fi
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_nfs_ip_index" ]]; then
        extra_button="--default-item $selected_nfs_ip_index"
    fi

    log_debug "nfs_step1_select_ip: About to display dialog menu with ${#menu_items[@]} items"
    cmd=(dialog --keep-tite --title "Step 1/$nfs_total_steps: Select $PRODUCT NFS Storage IP (VIP)"
         --ok-label "Next" --cancel-label "Back to Main" $extra_button
         --menu "Choose $PRODUCT NFS Storage IP (VIP):" 15 95 8)
    options=("${menu_items[@]}")
    dialog_menu
    log_debug "nfs_step1_select_ip: dialog_menu returned, exit code: $dialog_exit_code, selected: $selected_option"
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_nfs_ip_index="$selected_option"
            if [[ "${ips[selected_option]}" == "Enter new IP" ]]; then
                log_debug "nfs_step1_select_ip: User selected 'Enter new IP', showing input dialog"
                # Get custom IP input with validation loop
                local input_ip
                while true; do
                    log_debug "nfs_step1_select_ip: Displaying IP input dialog"
                    # Kill any background "Please wait..." dialogs that might be interfering
                    pkill -f "Please wait" 2>/dev/null || true  # Don't fail if no processes found
                    sleep 0.1  # Brief delay for cleanup
                    log_debug "nfs_step1_select_ip: Killed background dialogs, showing input dialog"
                    
                    # Use temporary file for dialog output instead of complex redirection
                    local temp_input=$(mktemp)
                    dialog --keep-tite --title "Enter New IP" --inputbox "Enter the NFS Storage IP address:" 8 40 2>"$temp_input"
                    local input_exit_code=$?
                    input_ip=$(< "$temp_input")
                    rm -f "$temp_input"
                    log_debug "nfs_step1_select_ip: Input dialog returned, exit code: $input_exit_code, input: '$input_ip'"
                    if [[ $input_exit_code -ne 0 ]]; then
                        log_debug "nfs_step1_select_ip: User cancelled input dialog"
                        return 2  # User cancelled
                    fi
                    
                    if [[ -z "$input_ip" ]]; then
                        log_debug "nfs_step1_select_ip: Empty IP address entered, showing error"
                        dialog --msgbox "IP address cannot be empty. Please try again." 8 50
                        continue
                    fi
                    
                    # Enhanced validation with REST API connectivity testing
                    local validation_error=""
                    log_debug "nfs_step1_select_ip: Starting enhanced validation for $input_ip"
                    
                    # Show progress dialog while testing connectivity
                    dialog --infobox "Testing connection to $PRODUCT at $input_ip...\nPlease wait (up to 10 seconds)" 5 60 &
                    local progress_pid=$!
                    
                    validate_ip_with_connectivity "$input_ip" validation_error
                    local validation_result=$?
                    
                    # Kill progress dialog
                    kill $progress_pid 2>/dev/null || true
                    wait $progress_pid 2>/dev/null || true
                    
                    if [[ $validation_result -ne 0 ]]; then
                        log_debug "nfs_step1_select_ip: Validation failed for $input_ip, result: $validation_result, error: $validation_error"
                        dialog --msgbox "$validation_error" 12 70
                        continue
                    fi
                    
                    if ip_already_exists "$input_ip"; then
                        dialog --msgbox "IP address $input_ip already exists in configuration. Please enter a different IP." 10 60
                        continue
                    fi
                    
                    # IP is valid, reachable, and new - break the loop
                    log_debug "nfs_step1_select_ip: IP $input_ip validated successfully"
                    break
                done
                
                selected_nfs_ip="$input_ip"
                # Ask if user wants to save this IP (only for manually entered IPs)
                if dialog --yesno "Save this IP ($input_ip) for future use?" 8 50; then
                    add_custom_ip "$input_ip"
                    log_info "nfs_step1_select_ip: Added custom IP '$input_ip', now prompting for role selection"
                    
                    # Update connected storage servers to create info file for new IP
                    update_connected_storage_servers
                    
                    # Prompt for node role selection
                    select_node_role "$input_ip"
                    log_info "nfs_step1_select_ip: Role selection completed for IP '$input_ip'"
                    
                    # Return to step 1 to allow selection of the newly added IP
                    return 2  # Retry same step
                fi
                return 0
            elif [[ "${ips[selected_option]}" == "Manage custom IPs" ]]; then
                manage_custom_ips
                return 2  # Retry same step
            else
                selected_nfs_ip="${ips[selected_option]}"
                
                # Check if the selected IP is disconnected
                if ! check_ip_connection_state "$selected_nfs_ip"; then
                    log_info "Selected disconnected IP: $selected_nfs_ip, attempting reconnection"
                    
                    # Show attempting reconnection dialog
                    dialog --infobox "Attempting to reconnect to $selected_nfs_ip..." 5 50
                    
                    if attempt_reconnect_ip "$selected_nfs_ip"; then
                        # Reconnection successful
                        dialog --msgbox "Successfully reconnected to $selected_nfs_ip" 6 50
                        
                        # Prompt for node role selection after successful reconnection
                        log_info "nfs_step1_select_ip: Successfully reconnected to '$selected_nfs_ip', prompting for role selection"
                        select_node_role "$selected_nfs_ip"
                        log_info "nfs_step1_select_ip: Role selection completed for reconnected IP '$selected_nfs_ip'"
                        
                        return 0
                    else
                        # Reconnection failed - check if it was authentication
                        local error_msg="Failed to connect to $selected_nfs_ip\n\n"
                        if grep -q "Authentication failed" /var/log/pve-tools/pve-tools.log 2>/dev/null | tail -n 5; then
                            error_msg="${error_msg}Authentication failed (HTTP 401)\n\n"
                            error_msg="${error_msg}Please check REST API credentials in Main Menu -> Setup\n\n"
                        elif grep -q "Access forbidden" /var/log/pve-tools/pve-tools.log 2>/dev/null | tail -n 5; then
                            error_msg="${error_msg}Access forbidden (HTTP 403)\n\n"
                            error_msg="${error_msg}The REST API user may not have sufficient permissions.\n\n"
                        else
                            error_msg="${error_msg}The storage server appears to be offline or unreachable.\n\n"
                        fi
                        error_msg="${error_msg}Returning to IP selection menu."
                        
                        dialog --msgbox "$error_msg" 12 65
                        return 2  # Retry same step
                    fi
                fi
                
                # Prompt for node role selection for connected IP (if role is unassigned)
                local current_role
                current_role=$(get_node_role_for_ip "$selected_nfs_ip")
                if [[ "$current_role" == "unassigned" ]]; then
                    log_info "nfs_step1_select_ip: Selected IP '$selected_nfs_ip' has unassigned role, prompting for role selection"
                    select_node_role "$selected_nfs_ip"
                    log_info "nfs_step1_select_ip: Role selection completed for connected IP '$selected_nfs_ip'"
                else
                    log_debug "nfs_step1_select_ip: Selected IP '$selected_nfs_ip' already has role '$current_role', skipping role selection"
                fi
                
                return 0
            fi
            ;;
        1)  # Back to Main
            return 1
            ;;
    esac
}

nfs_step2_select_pool() {
    # Test API connection first
    rest_api_connection_test
    local conn_test_exit=$?
    
    if [[ $conn_test_exit -eq 10 ]]; then
        # Authentication failure (401)
        dialog --msgbox "REST API Authentication Failed!\n\nThe REST API credentials are incorrect or have been changed on the $PRODUCT.\n\nPlease go to Setup → Configure REST API and update your credentials.\n\nCurrent user: $rest_api_user" 12 70
        return 2  # Go back
    elif [[ $conn_test_exit -eq 11 ]]; then
        # Access forbidden (403)
        dialog --msgbox "REST API Access Forbidden!\n\nThe user '$rest_api_user' does not have permission to perform this operation.\n\nPlease check user permissions on the $PRODUCT." 10 70
        return 2  # Go back
    elif [[ $conn_test_exit -ne 0 ]]; then
        # Other connection error
        dialog --msgbox "API connection test failed for $selected_nfs_ip\n\nPlease check your network connection and server configuration." 10 60
        return 2  # Go back
    fi

    local line pools=() menu_items=()
    while IFS= read -r line; do
        pools+=("$line")
    done < <(get_pool_names "$selected_nfs_ip")

    if [ "${#pools[@]}" -eq 0 ]; then
        # Mark server as disconnected when no pools found
        log_info "No pools found on $selected_nfs_ip, marking as disconnected"
        mark_ip_as_disconnected "$selected_nfs_ip"
        return 2  # Go back silently to IP selection
    fi

    local i
    for ((i=0; i<${#pools[@]}; i++)); do
        menu_items+=("$i" "${pools[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_pool_index" ]]; then
        extra_button="--default-item $selected_pool_index"
    fi

    cmd=(dialog --keep-tite --title "Step 2/$nfs_total_steps: Select Pool"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Pool from $selected_nfs_ip:" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_pool_index="$selected_option"
            selected_pool="${pools[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step3_select_dataset() {
    local line datasets=() menu_items=()
    while IFS= read -r line; do
        datasets+=("$line")
    done < <(get_datasets_in_pool "$selected_pool")

    if [ "${#datasets[@]}" -eq 0 ]; then
        dialog --msgbox "No datasets found in pool $selected_pool" 8 50
        return 2  # Go back
    fi

    # Build menu items with dataset name and PVE storage (if any)
    local i
    for ((i=0; i<${#datasets[@]}; i++)); do
        local dataset="${datasets[i]}"
        local full_dataset="$selected_pool/$dataset"
        local pve_storage=$(get_pve_storage_for_dataset "$selected_nfs_ip" "$full_dataset")
        
        if [[ -n "$pve_storage" ]]; then
            # Dataset has PVE storage attached
            menu_items+=("$i" "$(printf "%-30s | PVE: %s" "$dataset" "$pve_storage")")
        else
            # Dataset is not attached to any PVE storage
            menu_items+=("$i" "$(printf "%-30s | (Not in PVE)" "$dataset")")
        fi
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_dataset_index" ]]; then
        extra_button="--default-item $selected_dataset_index"
    fi

    cmd=(dialog --keep-tite --title "Step 3/$nfs_total_steps: Select Dataset"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Dataset from Pool $selected_pool:\n(Shows existing PVE storage mappings)" 17 75 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_dataset_index="$selected_option"
            selected_dataset="${datasets[selected_option]}"
            # Get and store the PVE storage name for the selected dataset
            local full_dataset="$selected_pool/$selected_dataset"
            selected_pve_storage=$(get_pve_storage_for_dataset "$selected_nfs_ip" "$full_dataset")
            log_debug "nfs_step3_select_dataset: Dataset '$selected_dataset' has PVE storage: '$selected_pve_storage'"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step4_select_snapshot() {
    local line snapshots=() snapshot_times=() menu_items=()
    
    while IFS=$'\t' read -r snapshot_name creation_time; do
        snapshots+=("$snapshot_name")
        snapshot_times+=("$creation_time")
    done < <(get_snapshots_with_times "$selected_pool" "$selected_dataset")

    if [ "${#snapshots[@]}" -eq 0 ]; then
        dialog --msgbox "No snapshots found in dataset $selected_dataset\n\nPool: $selected_pool\nDataset: $selected_dataset" 10 60
        return 2  # Go back
    fi

    local current_time
    current_time=$(get_current_timestamp)
    
    local i
    for ((i=0; i<${#snapshots[@]}; i++)); do
        local snapshot_name="${snapshots[i]}"
        local creation_time="${snapshot_times[i]}"
        local age_text=""
        
        if [[ -n "$creation_time" ]]; then
            local age_seconds
            age_seconds=$(calculate_age_seconds "$current_time" "$creation_time")
            age_text=$(format_age_human "$age_seconds")
        else
            age_text="unknown"
        fi
        
        # Use non-breaking spaces for proper column alignment in dialog
        local nbsp=$'\u00A0'  # non-breaking space
        menu_items+=("$i" "$snapshot_name${nbsp}${nbsp}${nbsp}${nbsp}$age_text")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_snapshot_index" ]]; then
        extra_button="--default-item $selected_snapshot_index"
    fi

    cmd=(dialog --keep-tite --title "Step 4/$nfs_total_steps: Select Snapshot"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Snapshot from Dataset $selected_dataset:" 18 80 10)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_snapshot_index="$selected_option"
            selected_snapshot="${snapshots[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step5_summary() {
    local clone_name=$(make_clone_name "$selected_pool" "$selected_dataset" "$selected_snapshot" "$selected_pve_storage")
    local summary_text="Summary of selections:\n\n"
    summary_text+="NFS Server IP: $selected_nfs_ip\n"
    summary_text+="Pool: $selected_pool\n"
    summary_text+="Dataset: $selected_dataset\n"
    summary_text+="Snapshot: $selected_snapshot\n"
    summary_text+="Clone name: $clone_name\n\n"
    summary_text+="Click 'Apply' to create the cloned NFS storage,\nor click 'Back' to modify your selections."

    dialog --keep-tite --title "Step 5/$nfs_total_steps: Confirmation" \
           --ok-label "Apply" --cancel-label "Back to Main" --extra-button --extra-label "Back" \
           --msgbox "$summary_text" 18 80

    case $? in
        0)  # Apply
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

extract_timestamp_from_snapname() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapname="$3"

    if [[ $snapname =~ ([0-9]{4}-[0-9]{2}-[0-9]{2})-([0-9]{6}) ]]; then
        local day_part="${BASH_REMATCH[1]}"
        local time_part="${BASH_REMATCH[2]}"
        local hourmin="${time_part:0:4}"
        echo "${day_part}-${hourmin}"
    else
        local response
        response=$(curl -k -s --connect-timeout 5 --max-time 30 -X GET -u "${rest_api_user}:${rest_api_password}" -H 'Content-Type: application/json' \
            "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc")
        local curl_exit=$?
        if [[ $curl_exit -ne 0 ]]; then
            log_error "extract_timestamp_from_snapname: Failed to connect to server for snapshot creation time"
            return 1
        fi

        local creation
        creation=$(printf '%s' "$response" | python3 -c "
import sys, json, datetime
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    found = False
    for snap in entries:
        if snap.get('name') == sys.argv[1]:
            creation = snap.get('properties', {}).get('creation')
            if creation:
                dt = datetime.datetime.fromtimestamp(int(creation))
                print(dt.strftime('%Y-%m-%d-%H%M'))
                found = True
                break
    if not found:
        sys.exit(1)
except Exception:
    sys.exit(1)
" "$snapname")

        if [[ -z "$creation" ]]; then
            log_error "extract_timestamp_from_snapname: Failed to extract creation time from snapshot data"
            return 1
        fi
        echo "$creation"
    fi
}

make_clone_name() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapshot_name="$3"
    local pve_storage_name="$4"  # New parameter for PVE storage name
    
    local ts
    ts=$(extract_timestamp_from_snapname "$pool_name" "$dataset_name" "$snapshot_name")
    
    # Use PVE storage name if available, otherwise fall back to dataset name
    local base_name="${pve_storage_name:-$dataset_name}"
    
    # Remove any leading/trailing whitespace
    base_name=$(trim_str "$base_name")
    
    # If still empty, use dataset name as fallback
    if [[ -z "$base_name" ]]; then
        base_name="$dataset_name"
    fi
    
    echo "${base_name}_clone_${ts}"
}

create_clone_from_snapshot() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapshot_name="$3"
    local clone_name="$4"

    local json_input="{\"name\": \"${clone_name}\"}"
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots/$snapshot_name/clones"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE CLONE FROM SNAPSHOT ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_input' '$url'"
    log_debug "================================================================================="

    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X POST -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "$url")

    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)

    if [[ $status -eq 200 ]]; then
        log_info "Clone '$clone_name' created"
        return 0
    elif [[ $status -eq 500 ]]; then
        # Check if it's a "dataset already exists" error
        if echo "$body" | grep -q "dataset already exists"; then
            log_info "Clone '$clone_name' already exists"
            return 2  # Special return code for "already exists"
        else
            log_error "Failed to create clone. Status: $status"
            log_error "Response: $body"
            return 1
        fi
    else
        log_error "Failed to create clone. Status: $status"
        log_error "Response: $body"
        return 1
    fi
}

create_nfs_share_for_clone() {
    local pool_name="$1"
    local clone_name="$2"

    local path="${pool_name}/${clone_name}"
    local json_input
    json_input=$(cat <<EOF
{
    "name": "$clone_name",
    "path": "$path",
    "nfs": {
        "enabled": true,
        "synchronous_data_record": true,
        "no_root_squash": true
    }
}
EOF
)

    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/shares"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE NFS SHARE ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_input' '$url'"
    log_debug "======================================================================"

    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X POST -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "$url")

    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)

    if [[ $status -eq 201 ]]; then
        log_info "NFS share '$clone_name' created at: $path"
    else
        log_error "Failed to create NFS share. Status: $status"
        log_error "Response: $body"
        return 1
    fi
}

get_ha_nodes_csv() {
    get_ha_nodes | paste -sd,
}

add_nfs_storage_to_proxmox() {
    local storage_id="$1"
    local server="$2"
    local export="/$storage_id"
    local nodes_csv="$3"

    local content="images,iso,vztmpl,backup,rootdir,snippets"

    log_info "Adding NFS storage '$storage_id' with server: $server, export: $export, nodes: $nodes_csv"

    # Build the pvesh command arguments
    local pvesh_args=()
    pvesh_args+=(create /storage)
    pvesh_args+=(--storage "$storage_id")
    pvesh_args+=(--type nfs)
    pvesh_args+=(--server "$server")
    pvesh_args+=(--export "$export")
    pvesh_args+=(--content "$content")
    
    # Add nodes parameter only if nodes_csv is not empty
    if [[ -n "$nodes_csv" ]]; then
        pvesh_args+=(--nodes "$nodes_csv")
        log_debug "add_nfs_storage_to_proxmox: Adding nodes parameter: $nodes_csv"
    else
        log_debug "add_nfs_storage_to_proxmox: No nodes parameter - nodes_csv is empty"
    fi
    
    log_debug "add_nfs_storage_to_proxmox: Executing pvesh with args: ${pvesh_args[*]}"
    
    local pvesh_output
    local pvesh_exit_code
    pvesh_output=$(pvesh "${pvesh_args[@]}" 2>&1)
    pvesh_exit_code=$?
    
    log_debug "add_nfs_storage_to_proxmox: pvesh exit code: $pvesh_exit_code"
    log_debug "add_nfs_storage_to_proxmox: pvesh output: $pvesh_output"
    
    if [[ $pvesh_exit_code -ne 0 ]]; then
        log_error "add_nfs_storage_to_proxmox: Failed to add NFS storage (exit code: $pvesh_exit_code): $pvesh_output"
        return 1
    fi
    
    log_info "add_nfs_storage_to_proxmox: Successfully added NFS storage: $storage_id"
    return 0
}

wait_for_storage_mount() {
    local storage_name="$1"
    local max_wait=30
    local wait_time=0
    
    log_info "Waiting for storage '$storage_name' to become available..."
    
    while [[ $wait_time -lt $max_wait ]]; do
        if [[ -d "/mnt/pve/$storage_name" ]]; then
            log_info "Storage '$storage_name' is now available"
            return 0
        fi
        
        sleep 2
        wait_time=$((wait_time + 2))
        log_debug "Waiting for storage mount... ${wait_time}s/${max_wait}s"
    done
    
    log_error "Storage '$storage_name' failed to mount within ${max_wait}s"
    return 1
}

refresh_storage_cache() {
    local storage_name="$1"
    
    log_debug "Refreshing storage cache for '$storage_name'..."
    
    # Force Proxmox VE to refresh storage information
    pvesh set /nodes/"$(hostname)"/storage/"$storage_name" --enabled 1 2>/dev/null || true
    
    # Wait a moment for the refresh to take effect
    sleep 2
    
    # Clear any cached storage information
    rm -f /tmp/pve-storage-* 2>/dev/null || true
}

sync_storage_discovery() {
    log_info "Synchronizing storage discovery for restore wizard..."
    
    # Rebuild storage IP mapping to ensure all storage is discovered
    build_storage_ip_map
    
    # Initialize API cache for optimal storage discovery and clone detection
    log_debug "Initializing API cache for storage discovery..."
    if ! init_api_cache; then
        log_error "Failed to initialize API cache for storage discovery"
        # Continue anyway - fallback to non-cached methods
    fi
    
    # Clear any cached storage information
    rm -f /tmp/pve-storage-* 2>/dev/null || true
    
    # Note: Removed pvesm status call which was causing 10-second delay
    # Storage discovery is handled by build_storage_ip_map reading config directly
    
    log_info "Storage discovery synchronization complete"
}

wait_for_config_files() {
    local storage_name="$1"
    local max_wait=60
    local wait_time=0
    
    log_info "Waiting for VM/CT config files to be discoverable in storage '$storage_name'..."
    
    while [[ $wait_time -lt $max_wait ]]; do
        local config_count=$(ls -1 /mnt/pve/"$storage_name"/images/*/*.conf 2>/dev/null | wc -l)
        if [[ $config_count -gt 0 ]]; then
            log_info "Found $config_count config files in storage '$storage_name'"
            return 0
        fi
        
        sleep 3
        wait_time=$((wait_time + 3))
        log_debug "Waiting for config files... ${wait_time}s/${max_wait}s"
    done
    
    log_info "No config files found in storage '$storage_name' within ${max_wait}s (this is normal for new storage)"
    return 0
}

set_readonly_conf_files() {
    local storage_name="$1"
    local conf_path="/mnt/pve/$storage_name/images"
    
    log_info "Setting read-only permissions on conf files in storage '$storage_name'..."
    
    # Check if the images directory exists
    if [[ ! -d "$conf_path" ]]; then
        log_info "Images directory '$conf_path' not found - no conf files to process"
        return 0
    fi
    
    # Find all .conf files and set read-only attribute
    local conf_files=()
    local conf_count=0
    
    while IFS= read -r -d '' conf_file; do
        conf_files+=("$conf_file")
        ((conf_count++))
    done < <(find "$conf_path" -name "*.conf" -type f -print0 2>/dev/null)
    
    if [[ $conf_count -eq 0 ]]; then
        log_info "No conf files found in '$conf_path' - nothing to set read-only"
        return 0
    fi
    
    log_info "Found $conf_count conf files, setting read-only permissions..."
    
    local success_count=0
    local error_count=0
    
    for conf_file in "${conf_files[@]}"; do
        if chmod 444 "$conf_file" 2>/dev/null; then
            log_debug "Set read-only permissions on: $conf_file"
            ((success_count++))
        else
            log_error "Failed to set read-only permissions on: $conf_file"
            ((error_count++))
        fi
    done
    
    if [[ $error_count -eq 0 ]]; then
        log_info "Successfully set read-only permissions on $success_count conf files"
    else
        log_error "Set read-only permissions on $success_count files, $error_count failed"
    fi
    
    return 0
}

# Check if a clone already exists by querying the API
check_clone_exists() {
    local pool_name="$1"
    local clone_name="$2"
    
    local response
    response=$(curl -k -s -w "\n%{http_code}" -X GET \
        -u "${rest_api_user}:${rest_api_password}" \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$clone_name" 2>/dev/null)
    
    local status=$(echo "$response" | tail -n1)
    
    if [[ $status -eq 200 ]]; then
        return 0  # Clone exists
    else
        return 1  # Clone doesn't exist
    fi
}

# Generate a unique clone name by appending a suffix if needed
generate_unique_clone_name() {
    local pool_name="$1"
    local base_clone_name="$2"
    local clone_name="$base_clone_name"
    local suffix=1
    
    # Check if the base name exists
    if ! check_clone_exists "$pool_name" "$clone_name"; then
        echo "$clone_name"
        return 0
    fi
    
    # Keep trying with incrementing suffix until we find an available name
    while [[ $suffix -le 99 ]]; do
        clone_name="${base_clone_name}_$(printf "%02d" $suffix)"
        if ! check_clone_exists "$pool_name" "$clone_name"; then
            echo "$clone_name"
            return 0
        fi
        ((suffix++))
    done
    
    # If we couldn't find an available name after 99 tries, return error
    log_error "Could not generate unique clone name after 99 attempts"
    return 1
}

# Handle the dialog when a clone already exists
handle_existing_clone_dialog() {
    local existing_clone="$1"
    local proposed_clone="$2"
    
    local message="A clone with the name '$existing_clone' already exists.\n\n"
    message+="Would you like to:\n"
    message+="  * Create another clone with name '$proposed_clone'\n"
    message+="  * Go back to select a different snapshot\n\n"
    message+="Current clone: $existing_clone\n"
    message+="Proposed name: $proposed_clone"
    
    dialog --title "Clone Already Exists" \
           --yes-label "Continue" \
           --no-label "Back" \
           --yesno "$message" 14 70
    
    return $?  # 0 for Continue, 1 for Back
}

perform_clone_and_setup() {
    local clone_name=$(make_clone_name "$selected_pool" "$selected_dataset" "$selected_snapshot" "$selected_pve_storage")
    local actual_clone_name="$clone_name"

    # Show progress
    dialog --title "Creating Clone" --infobox "Creating clone from snapshot:\nPool: $selected_pool\nDataset: $selected_dataset\nSnapshot: $selected_snapshot\nClone: $clone_name" 12 70
    sleep 1

    # Try to create the clone
    create_clone_from_snapshot "$selected_pool" "$selected_dataset" "$selected_snapshot" "$clone_name"
    local create_result=$?
    
    if [[ $create_result -eq 2 ]]; then
        # Clone already exists, ask user what to do
        local new_clone_name
        new_clone_name=$(generate_unique_clone_name "$selected_pool" "$clone_name")
        
        if [[ $? -ne 0 ]]; then
            dialog --msgbox "Unable to generate a unique clone name. Too many clones exist." 8 60
            return 1
        fi
        
        if handle_existing_clone_dialog "$clone_name" "$new_clone_name"; then
            # User chose to continue with new name
            actual_clone_name="$new_clone_name"
            dialog --title "Creating Clone" --infobox "Creating clone with new name:\nPool: $selected_pool\nDataset: $selected_dataset\nSnapshot: $selected_snapshot\nClone: $actual_clone_name" 12 70
            sleep 1
            
            if ! create_clone_from_snapshot "$selected_pool" "$selected_dataset" "$selected_snapshot" "$actual_clone_name"; then
                dialog --msgbox "Failed to create clone with new name!\n\nPool: $selected_pool\nDataset: $selected_dataset\nSnapshot: $selected_snapshot\nClone: $actual_clone_name" 12 60
                return 1
            fi
        else
            # User chose to go back
            return 2  # Special return code to indicate going back to snapshot selection
        fi
    elif [[ $create_result -ne 0 ]]; then
        # Other error occurred
        dialog --msgbox "Failed to create clone!\n\nPool: $selected_pool\nDataset: $selected_dataset\nSnapshot: $selected_snapshot\nClone: $clone_name" 12 60
        return 1
    fi
    
    # Update clone_name to actual_clone_name for rest of the function
    clone_name="$actual_clone_name"

    dialog --title "Processing..." --infobox "Creating NFS share..." 6 50
    sleep 1

    if ! create_nfs_share_for_clone "$selected_pool" "$clone_name"; then
        dialog --msgbox "Failed to create NFS share!" 8 40
        return 1
    fi

    dialog --title "Processing..." --infobox "Adding storage to Proxmox VE ..." 6 50
    sleep 1

    local nodes_csv=$(get_ha_nodes_csv)
    if ! add_nfs_storage_to_proxmox "$clone_name" "$selected_nfs_ip" "$nodes_csv"; then
        dialog --msgbox "Failed to add storage to Proxmox VE!" 8 40
        return 1
    fi

    dialog --title "Processing..." --infobox "Waiting for storage to become available..." 6 50

    if ! wait_for_storage_mount "$clone_name"; then
        dialog --msgbox "Storage was added but failed to mount properly.\nIt may take a few minutes to become available." 8 60
        return 1
    fi

    dialog --title "Processing..." --infobox "Refreshing storage information..." 6 50

    refresh_storage_cache "$clone_name"
    
    # Rebuild storage IP mapping to include the new storage
    build_storage_ip_map

    dialog --title "Processing..." --infobox "Scanning for VM/CT configurations..." 6 50

    wait_for_config_files "$clone_name"

    dialog --title "Processing..." --infobox "Setting read-only permissions on conf files..." 6 50

    set_readonly_conf_files "$clone_name"

    dialog --msgbox "Clone and NFS setup completed successfully!\n\nPool: $selected_pool\nDataset: $selected_dataset\nSnapshot: $selected_snapshot\nClone: $clone_name\nNFS Share: $clone_name\nProxmox VE Storage: $clone_name" 14 70
}

add_nfs_storage_wizard() {
    # Update connected storage servers information
    update_connected_storage_servers
    
    # Reset NFS wizard variables
    selected_nfs_ip=""
    selected_pool=""
    selected_dataset=""
    selected_snapshot=""
    selected_pve_storage=""
    selected_nfs_ip_index=""
    selected_pool_index=""
    selected_dataset_index=""
    selected_snapshot_index=""
    nfs_current_step=1

    while true; do
        case $nfs_current_step in
            1)
                nfs_step1_select_ip
                case $? in
                    0)  # Next
                        nfs_current_step=2
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Retry same step
                        ;;
                esac
                ;;
            2)
                nfs_step2_select_pool
                case $? in
                    0)  # Next
                        nfs_current_step=3
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=1
                        ;;
                esac
                ;;
            3)
                nfs_step3_select_dataset
                case $? in
                    0)  # Next
                        nfs_current_step=4
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=2
                        ;;
                esac
                ;;
            4)
                nfs_step4_select_snapshot
                case $? in
                    0)  # Next
                        nfs_current_step=5
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=3
                        ;;
                esac
                ;;
            5)
                nfs_step5_summary
                case $? in
                    0)  # Apply
                        perform_clone_and_setup
                        local setup_result=$?
                        if [[ $setup_result -eq 2 ]]; then
                            # User chose to go back to snapshot selection
                            nfs_current_step=4
                        elif [[ $setup_result -eq 0 ]]; then
                            # Success
                            return
                        else
                            # Error occurred, stay on current step
                            continue
                        fi
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=4
                        ;;
                esac
                ;;
        esac
    done
}

# ADD NEW NFS STORAGE FROM POOL FUNCTIONS

# Main wizard for adding new NFS storage from a pool
add_new_nfs_storage_wizard() {
    log_info "Starting add new NFS storage wizard"
    
    # Update connected storage servers information
    update_connected_storage_servers
    log_debug "add_new_nfs_storage_wizard: Finished update_connected_storage_servers"
    
    # Reset wizard variables
    selected_nfs_ip=""
    selected_pool=""
    selected_dataset=""
    selected_dataset_option=""  # "create" or "existing"
    new_dataset_name=""
    new_storage_name=""
    selected_nfs_ip_index=""
    selected_pool_index=""
    selected_dataset_index=""
    nfs_new_current_step=1
    nfs_new_total_steps=6
    oodp_destinations_json=""
    oodp_destination_datasets=()
    
    log_debug "add_new_nfs_storage_wizard: Reset wizard variables"
    
    while true; do
        log_debug "add_new_nfs_storage_wizard: Entering while loop, current step: $nfs_new_current_step"
        log_debug "add_new_nfs_storage_wizard: selected_dataset_option='$selected_dataset_option'"
        log_debug "add_new_nfs_storage_wizard: oodp_destinations_json='$oodp_destinations_json'"
        case $nfs_new_current_step in
            1)
                log_debug "add_new_nfs_storage_wizard: About to call nfs_step1_select_ip"
                # Temporarily set the variable name that nfs_step1_select_ip expects
                nfs_total_steps=$nfs_new_total_steps
                nfs_step1_select_ip
                local step1_result=$?  # Capture return code IMMEDIATELY
                # Restore the variable name
                nfs_total_steps=
                log_debug "add_new_nfs_storage_wizard: nfs_step1_select_ip returned: $step1_result"
                case $step1_result in
                    0)  # Next
                        log_debug "add_new_nfs_storage_wizard: Moving to step 2"
                        nfs_new_current_step=2
                        ;;
                    1)  # Back to Main
                        log_debug "add_new_nfs_storage_wizard: User wants back to main, exiting wizard"
                        return
                        ;;
                    2)  # Retry same step
                        log_debug "add_new_nfs_storage_wizard: Retrying step 1"
                        ;;
                esac
                ;;
            2)
                nfs_new_step2_select_pool
                case $? in
                    0)  # Next
                        nfs_new_current_step=3
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_new_current_step=1
                        ;;
                esac
                ;;
            3)
                nfs_new_step3_dataset_choice
                case $? in
                    0)  # Next
                        nfs_new_current_step=4  # Go to dataset handling (create or select)
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_new_current_step=2
                        ;;
                esac
                ;;
            4)
                log_debug "add_new_nfs_storage_wizard: Step 4 - Dataset handling"
                # Handle dataset creation or selection based on user choice
                if [[ "$selected_dataset_option" == "create" ]]; then
                    log_debug "add_new_nfs_storage_wizard: Calling nfs_new_step4a_create_dataset"
                    nfs_new_step4a_create_dataset
                else
                    log_debug "add_new_nfs_storage_wizard: Calling nfs_new_step4b_select_dataset"
                    nfs_new_step4b_select_dataset
                fi
                local step4_result=$?
                log_debug "add_new_nfs_storage_wizard: Step 4 returned: $step4_result"
                case $step4_result in
                    0)  # Next
                        log_debug "add_new_nfs_storage_wizard: Moving to step 5 (OODP destinations)"
                        nfs_new_current_step=5  # Go to OODP destinations
                        ;;
                    1)  # Back to Main
                        log_debug "add_new_nfs_storage_wizard: User wants back to main from step 4"
                        return
                        ;;
                    2)  # Back
                        log_debug "add_new_nfs_storage_wizard: Going back to step 3 from step 4"
                        nfs_new_current_step=3
                        ;;
                esac
                ;;
            5)
                log_debug "add_new_nfs_storage_wizard: Step 5 - OODP destinations"
                nfs_new_step5_oodp_destinations
                local step5_result=$?
                log_debug "add_new_nfs_storage_wizard: Step 5 returned: $step5_result"
                case $step5_result in
                    0)  # Next
                        log_debug "add_new_nfs_storage_wizard: Moving to step 6 (summary)"
                        nfs_new_current_step=6  # Go to summary
                        ;;
                    1)  # Back to Main
                        log_debug "add_new_nfs_storage_wizard: User wants back to main from step 5"
                        return
                        ;;
                    2)  # Back
                        log_debug "add_new_nfs_storage_wizard: Going back to step 4 from step 5"
                        nfs_new_current_step=4
                        ;;
                esac
                ;;
            6)
                log_debug "add_new_nfs_storage_wizard: Step 6 - Summary"
                nfs_new_step6_summary
                case $? in
                    0)  # Apply
                        perform_new_nfs_setup
                        local setup_result=$?
                        if [[ $setup_result -eq 0 ]]; then
                            # Success
                            return
                        else
                            # Error occurred, stay on current step
                            continue
                        fi
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        # Go back to step 3 (dataset option selection)
                        nfs_new_current_step=3
                        ;;
                esac
                ;;
        esac
    done
}


# Step 2: Select Pool
nfs_new_step2_select_pool() {
    local pools=()
    local menu_items=()
    
    # Get pools from selected IP
    while IFS= read -r pool; do
        [[ -n "$pool" ]] && pools+=("$pool")
    done < <(get_pools)
    
    if [[ ${#pools[@]} -eq 0 ]]; then
        # Mark server as disconnected when no pools found
        log_info "No pools found on $selected_nfs_ip, marking as disconnected"
        mark_ip_as_disconnected "$selected_nfs_ip"
        return 2  # Go back silently to IP selection
    fi
    
    local i
    for ((i=0; i<${#pools[@]}; i++)); do
        menu_items+=("$i" "${pools[i]}")
    done
    
    local dialog_selected=$(mktemp)
    local extra_button=""
    if [[ -n "$selected_pool_index" ]]; then
        extra_button="--default-item $selected_pool_index"
    fi
    
    cmd=(dialog --keep-tite --title "Step 2/$nfs_new_total_steps: Select Pool"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Pool:" 15 60 8)
    options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            selected_pool_index="$selected_option"
            selected_pool="${pools[selected_option]}"
            log_info "Selected pool: $selected_pool"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

# Step 3: Choose Dataset Creation Method
nfs_new_step3_dataset_choice() {
    local dialog_selected=$(mktemp)
    
    cmd=(dialog --keep-tite --title "Step 3/$nfs_new_total_steps: Dataset Selection"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back"
         --menu "Choose dataset option:" 10 60 3)
    options=("create" "Create NEW dataset"
             "existing" "Select existing dataset")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            selected_dataset_option="$selected_option"
            log_info "Selected dataset option: $selected_dataset_option"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

# Step 4A: Create New Dataset
nfs_new_step4a_create_dataset() {
    # Find next available dataset number
    local next_number
    if ! next_number=$(find_next_dataset_number "$selected_pool"); then
        dialog --msgbox "Failed to determine next dataset number!\n\nPool: $selected_pool" 10 60
        return 2  # Go back
    fi
    
    new_dataset_name="${dataset_auto_prefix}-$next_number"
    selected_dataset="$new_dataset_name"
    
    log_info "Generated new dataset name: $new_dataset_name"
    return 0
}

# Step 4B: Select Existing Dataset
nfs_new_step4b_select_dataset() {
    local all_datasets=()
    local available_datasets=()
    local menu_items=()
    
    # Get all datasets in selected pool
    while IFS= read -r dataset; do
        [[ -n "$dataset" ]] && all_datasets+=("$dataset")
    done < <(get_datasets_in_pool "$selected_pool")
    
    if [[ ${#all_datasets[@]} -eq 0 ]]; then
        dialog --msgbox "No datasets found in pool $selected_pool" 8 50
        return 2  # Go back
    fi
    
    # Filter out datasets that already have PVE storage attached
    local i
    for dataset in "${all_datasets[@]}"; do
        local full_dataset="$selected_pool/$dataset"
        log_debug "Checking dataset: $dataset (full path: $full_dataset) on server: $selected_nfs_ip"
        local pve_storage=$(get_pve_storage_for_dataset "$selected_nfs_ip" "$full_dataset")
        
        if [[ -z "$pve_storage" ]]; then
            # Dataset is available (not attached to any PVE storage)
            available_datasets+=("$dataset")
            log_debug "Dataset $dataset is available (not attached to PVE)"
        else
            log_debug "Dataset $dataset already attached to PVE storage: $pve_storage"
        fi
    done
    
    if [[ ${#available_datasets[@]} -eq 0 ]]; then
        dialog --msgbox "No available datasets found in pool $selected_pool\n\nAll datasets are already attached to PVE storage." 10 60
        return 2  # Go back
    fi
    
    # Build menu items with dataset name and status
    for ((i=0; i<${#available_datasets[@]}; i++)); do
        local dataset="${available_datasets[i]}"
        # Format: dataset name padded to consistent width
        menu_items+=("$i" "$(printf "%-30s (Available)" "$dataset")")
    done
    
    local dialog_selected=$(mktemp)
    local extra_button=""
    if [[ -n "$selected_dataset_index" ]]; then
        extra_button="--default-item $selected_dataset_index"
    fi
    
    cmd=(dialog --keep-tite --title "Step 4/$nfs_new_total_steps: Select Dataset"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose existing dataset from pool $selected_pool:\n(Only showing datasets not attached to PVE)" 17 70 8)
    options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            selected_dataset_index="$selected_option"
            selected_dataset="${available_datasets[selected_option]}"
            log_info "Selected existing dataset: $selected_dataset"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

# Step 5: Configure OODP Backup Destinations
nfs_new_step5_oodp_destinations() {
    log_debug "nfs_new_step5_oodp_destinations: Starting OODP destinations configuration"
    local backup_servers=()
    local destinations=()
    
    # Get list of backup-dr-node servers
    log_debug "nfs_new_step5_oodp_destinations: Calling get_backup_dr_servers"
    while IFS= read -r server; do
        [[ -n "$server" ]] && backup_servers+=("$server")
    done < <(get_backup_dr_servers)
    
    log_debug "nfs_new_step5_oodp_destinations: Found ${#backup_servers[@]} backup servers: ${backup_servers[*]}"
    
    if [[ ${#backup_servers[@]} -eq 0 ]]; then
        # No backup servers available, skip to summary
        log_info "No backup-dr-node servers found, skipping destination configuration"
        oodp_destinations_json=""
        oodp_destination_datasets=()
        return 0
    fi
    
    # Ask if user wants to configure backup destinations
    dialog --keep-tite --title "Step 5/$nfs_new_total_steps: OODP Backup Configuration" \
           --yes-label "Configure" --no-label "Skip" \
           --yesno "Found ${#backup_servers[@]} backup/DR server(s) available.\n\nDo you want to configure OODP backup destinations now?\n\n(You can also configure them later in JovianDSS GUI)" 12 65
    
    if [[ $? -ne 0 ]]; then
        # User chose to skip
        log_info "User skipped OODP backup destination configuration"
        oodp_destinations_json=""
        oodp_destination_datasets=()
        return 0
    fi
    
    # Collect destinations for each backup server
    local dst_plan="$oodp_dst_default_plan"
    local port="${oodp_backup_destination_port:-40000}"
    
    for backup_ip in "${backup_servers[@]}"; do
        # Show server selection dialog
        dialog --keep-tite --title "Configure Backup Destination" \
               --yes-label "Configure" --no-label "Skip" \
               --yesno "Configure backup destination on server:\n$backup_ip?" 10 60
        
        if [[ $? -ne 0 ]]; then
            continue  # Skip this server
        fi
        
        # Check if this destination needs to be registered (requires web GUI password)
        log_info "nfs_new_step5_oodp_destinations: Checking if destination $backup_ip:$port needs registration"
        if ! check_odp_destination_registered "$backup_ip" "$port" "$selected_nfs_ip"; then
            log_info "nfs_new_step5_oodp_destinations: Destination $backup_ip:$port not registered, need web GUI password"
            
            # Prompt for storage server web GUI password
            local temp_password=$(mktemp)
            dialog --keep-tite --title "Backup Server Authentication" \
                   --passwordbox "To register backup server $backup_ip, please provide the WEB GUI password for that server:" 12 70 2>"$temp_password"
            local dialog_result=$?
            
            if [[ $dialog_result -ne 0 ]]; then
                rm -f "$temp_password"
                log_info "nfs_new_step5_oodp_destinations: User cancelled password input for $backup_ip"
                continue  # Skip this server
            fi
            
            storage_server_web_gui_password=$(< "$temp_password")
            rm -f "$temp_password"
            
            if [[ -z "$storage_server_web_gui_password" ]]; then
                dialog --title "Password Required" --msgbox "Web GUI password is required to register backup server.\n\nSkipping server $backup_ip" 10 60
                log_error "nfs_new_step5_oodp_destinations: Empty password provided for $backup_ip"
                continue  # Skip this server
            fi
            
            log_info "nfs_new_step5_oodp_destinations: Web GUI password provided for $backup_ip (length: ${#storage_server_web_gui_password})"
            
            # NOW register the ODP destination immediately
            log_info "nfs_new_step5_oodp_destinations: Registering ODP destination $backup_ip:$port now..."
            dialog --title "Registering Backup Server" --infobox "Registering backup server $backup_ip...\n\nThis may take up to 90 seconds." 8 60
            
            if register_odp_destination_node "$backup_ip" "$port" "$selected_nfs_ip" "Backup DR Node for backup operations"; then
                log_info "nfs_new_step5_oodp_destinations: ODP destination $backup_ip:$port registered successfully"
                dialog --title "Registration Complete" --infobox "Backup server $backup_ip registered successfully!" 6 60
                sleep 2
            else
                log_error "nfs_new_step5_oodp_destinations: Failed to register ODP destination $backup_ip:$port"
                dialog --title "Registration Failed" --msgbox "Failed to register backup server $backup_ip!\n\nCheck logs for details.\nYou may need to register manually in JovianDSS GUI." 10 60
                continue  # Skip this server
            fi
        else
            log_info "nfs_new_step5_oodp_destinations: Destination $backup_ip:$port already registered, no password needed"
        fi
        
        # Get pools from backup server
        local backup_pools=()
        while IFS= read -r pool; do
            [[ -n "$pool" ]] && backup_pools+=("$pool")
        done < <(get_pools "$backup_ip" 2>/dev/null | grep -v "^$")
        
        if [[ ${#backup_pools[@]} -eq 0 ]]; then
            dialog --msgbox "No pools found on backup server $backup_ip" 8 60
            continue
        fi
        
        # Select pool on backup server
        local menu_items=()
        for ((i=0; i<${#backup_pools[@]}; i++)); do
            menu_items+=("$i" "${backup_pools[i]}")
        done
        
        local dialog_selected=$(mktemp)
        cmd=(dialog --keep-tite --title "Select Backup Pool"
             --ok-label "Select" --cancel-label "Skip"
             --menu "Choose pool on backup server $backup_ip:" 15 60 7)
        options=("${menu_items[@]}")
        
        dialog_menu
        local result=$dialog_exit_code
        rm -f "$dialog_selected"
        
        if [[ $result -ne 0 ]]; then
            continue  # Skip this server
        fi
        
        local selected_backup_pool="${backup_pools[selected_option]}"
        
        # Ask for dataset creation choice
        local backup_dataset=""
        dialog --keep-tite --title "Backup Dataset" \
               --yes-label "Create New" --no-label "Select Existing" \
               --yesno "Create new dataset for backup destination or select existing?\n\nPool: $selected_backup_pool" 10 60
        
        if [[ $? -eq 0 ]]; then
            # Create new dataset with -backup suffix
            backup_dataset="${selected_dataset}-backup"
            log_info "Will create backup dataset: $backup_dataset on $backup_ip"
        else
            # Select existing dataset
            local backup_datasets=()
            while IFS= read -r dataset; do
                [[ -n "$dataset" ]] && backup_datasets+=("$dataset")
            done < <(get_datasets_in_pool "$selected_backup_pool" "$backup_ip" 2>/dev/null | grep -v "^$")
            
            if [[ ${#backup_datasets[@]} -eq 0 ]]; then
                dialog --msgbox "No datasets found in pool $selected_backup_pool" 8 60
                continue
            fi
            
            # Build menu for existing datasets
            menu_items=()
            for ((i=0; i<${#backup_datasets[@]}; i++)); do
                menu_items+=("$i" "${backup_datasets[i]}")
            done
            
            dialog_selected=$(mktemp)
            cmd=(dialog --keep-tite --title "Select Backup Dataset"
                 --ok-label "Select" --cancel-label "Skip"
                 --menu "Choose dataset for backup destination:" 15 60 7)
            options=("${menu_items[@]}")
            
            dialog_menu
            result=$dialog_exit_code
            rm -f "$dialog_selected"
            
            if [[ $result -ne 0 ]]; then
                continue  # Skip this server
            fi
            
            backup_dataset="${backup_datasets[selected_option]}"
        fi
        
        # Add this destination to the list
        local destination_entry="{\"plan\": \"$dst_plan\", \"dataset\": \"${backup_ip}:${port}/${selected_backup_pool}/${backup_dataset}\"}"
        destinations+=("$destination_entry")
        
        # ALSO store dataset info separately for dataset creation (without port!)
        oodp_destination_datasets+=("${backup_ip}|${selected_backup_pool}|${backup_dataset}")
        
        log_info "nfs_new_step5_oodp_destinations: Added backup destination: ${backup_ip}:${port}/${selected_backup_pool}/${backup_dataset}"
        log_debug "nfs_new_step5_oodp_destinations: Destination entry JSON: $destination_entry"
        log_debug "nfs_new_step5_oodp_destinations: Dataset creation info: ${backup_ip}|${selected_backup_pool}|${backup_dataset}"
        log_debug "nfs_new_step5_oodp_destinations: DEBUG - backup_ip='$backup_ip', port='$port', selected_backup_pool='$selected_backup_pool', backup_dataset='$backup_dataset'"
    done
    
    # Build JSON array of destinations
    if [[ ${#destinations[@]} -gt 0 ]]; then
        log_info "nfs_new_step5_oodp_destinations: Building JSON array from ${#destinations[@]} destinations"
        oodp_destinations_json="["
        for ((i=0; i<${#destinations[@]}; i++)); do
            if [[ $i -gt 0 ]]; then
                oodp_destinations_json+=", "
            fi
            oodp_destinations_json+="${destinations[i]}"
            log_debug "nfs_new_step5_oodp_destinations: Added destination $((i+1)): ${destinations[i]}"
        done
        oodp_destinations_json+="]"
        log_info "nfs_new_step5_oodp_destinations: Final OODP destinations JSON: $oodp_destinations_json"
        
        # Log pretty-printed JSON for debugging
        log_debug "=== FINAL DESTINATIONS JSON (PRETTY-PRINTED) ==="
        if command -v python3 >/dev/null 2>&1; then
            echo "$oodp_destinations_json" | python3 -m json.tool 2>/dev/null | while IFS= read -r line; do
                log_debug "  $line"
            done || log_debug "  (Could not pretty-print JSON)"
        else
            log_debug "  $oodp_destinations_json"
        fi
        log_debug "================================================"
    else
        oodp_destinations_json=""
        oodp_destination_datasets=()
        log_info "nfs_new_step5_oodp_destinations: No OODP backup destinations configured"
    fi
    
    return 0
}

# Step 6: Summary and Confirmation (was Step 5)
nfs_new_step6_summary() {
    # Generate storage name
    new_storage_name=$(find_next_storage_name)
    
    if [[ $? -ne 0 ]]; then
        dialog --msgbox "Failed to generate storage name!" 8 50
        return 2  # Go back
    fi
    
    local summary="Summary of NFS Storage Configuration:\n\n"
    summary+="Server IP: $selected_nfs_ip\n"
    summary+="Pool: $selected_pool\n"
    
    if [[ "$selected_dataset_option" == "create" ]]; then
        summary+="\n--- NEW DATASET WILL BE CREATED ---\n"
        summary+="Dataset Name: $new_dataset_name\n"
    else
        summary+="Dataset: $selected_dataset (EXISTING)\n"
    fi
    
    summary+="\n--- STORAGE CONFIGURATION ---\n"
    summary+="NFS Share: /$selected_dataset\n"
    summary+="PVE Storage Name: $new_storage_name\n"
    
    summary+="\n--- OODP AUTO-SNAPSHOT CONFIGURATION ---\n"
    local src_plan="$oodp_src_default_plan"
    summary+="Source Schedule: $src_plan\n"
    
    # Show backup destinations if configured
    if [[ -n "$oodp_destinations_json" ]]; then
        summary+="\n--- OODP BACKUP DESTINATIONS ---\n"
        # Parse destinations from JSON for display
        local dest_count=$(echo "$oodp_destinations_json" | grep -o '"dataset"' | wc -l)
        summary+="Configured destinations: $dest_count\n"
        
        # Extract destination info for display (simplified parsing)
        local dest_info=$(echo "$oodp_destinations_json" | sed -n 's/.*"dataset"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/p')
        if [[ -n "$dest_info" ]]; then
            while IFS= read -r dest; do
                [[ -n "$dest" ]] && summary+="  * $dest\n"
            done <<< "$dest_info"
        fi
        
        # Also show destination plan
        local dest_plan="$oodp_dst_default_plan"
        summary+="Destination Schedule: $dest_plan\n"
    else
        summary+="Backup destinations: Not configured\n"
        summary+="Note: Can be configured later in JovianDSS GUI\n"
    fi
    
    summary+="\nDo you want to proceed with this configuration?"
    
    # Adjust dialog height based on content
    local dialog_height=25
    if [[ -n "$oodp_destinations_json" ]]; then
        dialog_height=28
    fi
    local dialog_width=70
    
    dialog --keep-tite --title "Step 6/$nfs_new_total_steps: Confirm Configuration" \
           --yes-label "Apply" --no-label "Back" \
           --yesno "$summary" $dialog_height $dialog_width    
    case $? in
        0)  # Apply
            return 0
            ;;
        1)  # Back
            return 2
            ;;
    esac
}

# Perform the actual setup of new NFS storage with correct sequence
perform_new_nfs_setup() {
    log_info "Starting NFS storage setup with new sequence: destinations -> source -> ODP -> PVE"
    
    # STEP 1: Create DESTINATION datasets FIRST (fail fast if any fail)
    if [[ -n "$oodp_destinations_json" ]]; then
        log_info "perform_new_nfs_setup: STEP 1 - Creating destination datasets"
        log_debug "perform_new_nfs_setup: Destinations JSON: '$oodp_destinations_json'"
        
        if ! create_destination_datasets; then
            log_error "perform_new_nfs_setup: STEP 1 FAILED - Destination dataset creation failed, stopping process"
            return 1  # Error dialog already shown by create_destination_datasets
        fi
        
        log_info "perform_new_nfs_setup: STEP 1 SUCCESS - All destination datasets created"
    else
        log_info "perform_new_nfs_setup: STEP 1 SKIPPED - No destinations configured"
    fi
    
    # STEP 2: Create SOURCE dataset + NFS share
    log_info "perform_new_nfs_setup: STEP 2 - Creating source dataset and NFS share"
    
    if [[ "$selected_dataset_option" == "create" ]]; then
        log_info "perform_new_nfs_setup: Creating SOURCE dataset pool_name='$selected_pool' dataset_name='$new_dataset_name' on server='$selected_nfs_ip'"
        dialog --title "Creating SOURCE Dataset" --infobox "Creating SOURCE dataset:\nServer: $selected_nfs_ip\nPool: $selected_pool\nDataset: $new_dataset_name" 10 70
        
        if ! create_dataset "$selected_pool" "$new_dataset_name"; then
            log_error "perform_new_nfs_setup: STEP 2 FAILED - SOURCE dataset creation failed"
            dialog --title "SOURCE Dataset Creation Failed" --msgbox \
"Failed to create SOURCE dataset!\n\n\
SERVER: $selected_nfs_ip\n\
POOL: $selected_pool\n\
DATASET: $new_dataset_name\n\n\
SUMMARY:\n\
Destination datasets: CREATED\n\
Source dataset: FAILED\n\
PVE storage: NOT CREATED\n\n\
Please check server connectivity and credentials.\n\
Check logs for detailed error information." \
18 70
            return 1
        fi
        
        log_info "perform_new_nfs_setup: SOURCE dataset created SUCCESS pool_name='$selected_pool' dataset_name='$new_dataset_name'"
        selected_dataset="$new_dataset_name"
    fi
    
    # Create NFS share for source dataset
    dialog --title "Creating SOURCE NFS Share" --infobox "Creating NFS share for source dataset...\nServer: $selected_nfs_ip\nPool: $selected_pool\nDataset: $selected_dataset" 10 70
    
    if ! create_nfs_share_for_dataset "$selected_pool" "$selected_dataset"; then
        log_error "perform_new_nfs_setup: STEP 2 FAILED - SOURCE NFS share creation failed"
        dialog --title "SOURCE NFS Share Creation Failed" --msgbox \
"Failed to create SOURCE NFS share!\n\n\
SERVER: $selected_nfs_ip\n\
POOL: $selected_pool\n\
DATASET: $selected_dataset\n\n\
SUMMARY:\n\
Destination datasets: CREATED\n\
Source dataset: CREATED\n\
Source NFS share: FAILED\n\
PVE storage: NOT CREATED\n\n\
Please check server NFS configuration.\n\
Check logs for detailed error information." \
18 70
        return 1
    fi
    
    log_info "perform_new_nfs_setup: STEP 2 SUCCESS - Source dataset and NFS share created"
    dialog --title "Success" --infobox "SOURCE dataset and NFS share created successfully!\nServer: $selected_nfs_ip\nPool: $selected_pool\nDataset: $selected_dataset" 8 70
    sleep 2
    
    # STEP 3: Register ODP destination nodes if needed (before ODP task creation)
    if [[ -n "$oodp_destinations_json" ]]; then
        log_info "perform_new_nfs_setup: STEP 3 - Registering ODP destination nodes"
        dialog --title "Registering ODP Destinations" --infobox "Registering backup destination servers..." 8 60
        
        local registration_failed=false
        for dest_info in "${oodp_destination_datasets[@]}"; do
            # Parse format: IP|POOL|DATASET
            IFS='|' read -r dst_ip dst_pool dst_dataset <<< "$dest_info"
            
            if [[ -z "$dst_ip" ]]; then
                log_error "perform_new_nfs_setup: STEP 3 - Invalid destination info: $dest_info"
                continue
            fi
            
            local dst_port="${oodp_backup_destination_port:-40000}"
            
            # Check if destination is already registered
            log_info "perform_new_nfs_setup: Checking ODP registration for destination $dst_ip:$dst_port"
            if check_odp_destination_registered "$dst_ip" "$dst_port" "$selected_nfs_ip"; then
                log_info "perform_new_nfs_setup: ODP destination already registered: $dst_ip:$dst_port"
            else
                log_info "perform_new_nfs_setup: Registering ODP destination: $dst_ip:$dst_port"
                if ! register_odp_destination_node "$dst_ip" "$dst_port" "$selected_nfs_ip" "Backup DR Node for $dst_pool/$dst_dataset"; then
                    log_error "perform_new_nfs_setup: STEP 3 FAILED - Could not register ODP destination $dst_ip:$dst_port"
                    registration_failed=true
                fi
            fi
        done
        
        if [[ "$registration_failed" == "true" ]]; then
            log_error "perform_new_nfs_setup: STEP 3 PARTIAL FAILURE - Some ODP destinations could not be registered"
            dialog --title "ODP Registration Warning" --msgbox \
"Some backup destinations could not be registered!\n\n\
This may affect ODP task creation.\n\
Check logs for details.\n\n\
Continuing with remaining setup..." \
12 60
        else
            log_info "perform_new_nfs_setup: STEP 3 SUCCESS - All ODP destinations registered"
        fi
    else
        log_info "perform_new_nfs_setup: STEP 3 SKIPPED - No ODP destinations configured"
    fi
    
    log_info "perform_new_nfs_setup: STEP 3 COMPLETED - ODP destination registration"
    
    # STEP 4: Create ODP task (warning only if fails, not critical)
    log_info "perform_new_nfs_setup: STEP 4 - Creating ODP task"
    
    local oodp_status="OODP auto-snapshot: Not configured"
    
    # Check if OODP task already exists
    if check_oodp_task_exists "$selected_pool" "$selected_dataset" "$selected_nfs_ip"; then
        log_info "perform_new_nfs_setup: OODP task already exists for $selected_pool/$selected_dataset"
        
        # Debug: Log existing destinations for analysis
        log_debug "perform_new_nfs_setup: Existing OODP_TASK_DESTINATIONS: '$OODP_TASK_DESTINATIONS'"
        log_debug "perform_new_nfs_setup: New oodp_destinations_json: '$oodp_destinations_json'"
        
        # Check if we have new destinations to add and if they differ from existing ones
        if [[ -n "$oodp_destinations_json" ]]; then
            local needs_update=false
            
            # Check if existing destinations are empty or if new destinations are different
            if [[ -z "$OODP_TASK_DESTINATIONS" ]] || [[ "$OODP_TASK_DESTINATIONS" == "[]" ]]; then
                log_debug "perform_new_nfs_setup: Existing task has no destinations, needs update"
                needs_update=true
            else
                # Check if the new destinations contain different entries than existing ones
                # Extract destination dataset paths from both for comparison
                local new_destinations_check
                new_destinations_check=$(echo "$oodp_destinations_json" | jq -r '.[].dataset' 2>/dev/null | sort)
                
                local existing_destinations_check
                existing_destinations_check=$(echo "$OODP_TASK_DESTINATIONS" | jq -r '.[].dataset' 2>/dev/null | sort)
                
                log_debug "perform_new_nfs_setup: New destinations: '$new_destinations_check'"
                log_debug "perform_new_nfs_setup: Existing destinations: '$existing_destinations_check'"
                
                if [[ "$new_destinations_check" != "$existing_destinations_check" ]]; then
                    log_debug "perform_new_nfs_setup: Destinations differ, needs update"
                    needs_update=true
                else
                    log_debug "perform_new_nfs_setup: Destinations are identical, no update needed"
                fi
            fi
            
            if [[ "$needs_update" == "true" ]]; then
                log_info "perform_new_nfs_setup: Updating existing OODP task with new/different destinations"
                dialog --title "Creating ODP Task" --infobox "Updating existing OODP task with backup destinations..." 8 60
                
                if update_oodp_task_destinations "$selected_pool" "$selected_dataset" "$selected_nfs_ip" "$oodp_destinations_json"; then
                    log_info "perform_new_nfs_setup: OODP task updated successfully with destinations"
                    oodp_status="OODP auto-snapshot: Configured with backup destinations"
                else
                    log_error "perform_new_nfs_setup: Failed to update OODP task with destinations (non-critical)"
                    oodp_status="OODP auto-snapshot: Configured (destinations need manual setup)"
                fi
            else
                log_info "perform_new_nfs_setup: Existing OODP task already has same destinations configured"
                oodp_status="OODP auto-snapshot: Already configured with destinations"
            fi
        else
            log_info "perform_new_nfs_setup: No new destinations specified, existing OODP task unchanged"
            oodp_status="OODP auto-snapshot: Already configured"
        fi
    else
        # Create new OODP task
        log_info "perform_new_nfs_setup: Creating new OODP task"
        
        if [[ -n "$oodp_destinations_json" ]]; then
            dialog --title "Creating ODP Task" --infobox "Creating OODP task with backup destinations..." 8 60
            log_debug "perform_new_nfs_setup: Creating OODP task with destinations: $oodp_destinations_json"
            
            if create_oodp_task "$selected_pool" "$selected_dataset" "$new_storage_name" "$selected_nfs_ip" "$oodp_destinations_json"; then
                log_info "perform_new_nfs_setup: OODP task created successfully with destinations"
                oodp_status="OODP auto-snapshot: Configured with backup destinations"
            else
                log_error "perform_new_nfs_setup: Failed to create OODP task with destinations (non-critical)"
                oodp_status="OODP auto-snapshot: Manual setup required"
            fi
        else
            dialog --title "Creating ODP Task" --infobox "Creating OODP auto-snapshot task..." 8 60
            
            if create_oodp_task "$selected_pool" "$selected_dataset" "$new_storage_name" "$selected_nfs_ip"; then
                log_info "perform_new_nfs_setup: OODP task created successfully"
                oodp_status="OODP auto-snapshot: Configured"
            else
                log_error "perform_new_nfs_setup: Failed to create OODP task (non-critical)"
                oodp_status="OODP auto-snapshot: Manual setup required"
            fi
        fi
    fi
    
    log_info "perform_new_nfs_setup: STEP 4 COMPLETED - ODP task status: $oodp_status"
    
    # STEP 5: Add storage to Proxmox VE
    log_info "perform_new_nfs_setup: STEP 5 - Adding storage to Proxmox VE"
    dialog --title "Adding PVE Storage" --infobox "Adding storage to Proxmox VE...\nStorage: $new_storage_name\nServer: $selected_nfs_ip" 8 60
    
    local nodes_csv=$(get_ha_nodes_csv)
    log_debug "get_ha_nodes_csv returned: '$nodes_csv'"
    
    # NFS server exports datasets by name only, not including pool name
    local export_path="/$selected_dataset"
    
    # Log all variables for debugging
    log_debug "Variables for PVE storage creation:"
    log_debug "  new_storage_name='$new_storage_name'"
    log_debug "  selected_nfs_ip='$selected_nfs_ip'"
    log_debug "  selected_pool='$selected_pool'"
    log_debug "  selected_dataset='$selected_dataset'"
    log_debug "  export_path='$export_path'"
    log_debug "  nodes_csv='$nodes_csv'"
    
    # Check if mount point already exists and clean it up if needed
    local mount_point="/mnt/pve/$new_storage_name"
    if [[ -d "$mount_point" ]]; then
        log_info "Mount point $mount_point already exists, attempting cleanup"
        
        # Check if it's mounted and unmount if needed
        if mountpoint -q "$mount_point" 2>/dev/null; then
            log_info "Unmounting existing mount at $mount_point"
            umount "$mount_point" 2>/dev/null || {
                log_error "Failed to unmount $mount_point"
                dialog --title "PVE Storage Creation Failed" --msgbox \
"Cannot create PVE storage!\n\n\
REASON: Mount point $mount_point is in use\n\n\
SUMMARY:\n\
Destination datasets: CREATED\n\
Source dataset: CREATED\n\
Source NFS share: CREATED\n\
ODP task: $oodp_status\n\
PVE storage: FAILED\n\n\
Please manually unmount or remove $mount_point\n\
and try adding the storage again." \
18 70
                return 1
            }
        fi
        
        # Remove the directory if it's empty
        if ! rmdir "$mount_point" 2>/dev/null; then
            log_error "Cannot remove $mount_point - directory not empty or permission denied"
            dialog --title "PVE Storage Creation Failed" --msgbox \
"Cannot create PVE storage!\n\n\
REASON: Directory $mount_point exists and cannot be removed\n\n\
SUMMARY:\n\
Destination datasets: CREATED\n\
Source dataset: CREATED\n\
Source NFS share: CREATED\n\
ODP task: $oodp_status\n\
PVE storage: FAILED\n\n\
Please manually remove $mount_point\n\
and try adding the storage again." \
18 70
            return 1
        fi
    fi
    
    # Build the pvesh command arguments
    local pvesh_args=()
    pvesh_args+=(create /storage)
    pvesh_args+=(--storage "$new_storage_name")
    pvesh_args+=(--type nfs)
    pvesh_args+=(--server "$selected_nfs_ip")
    pvesh_args+=(--export "$export_path")
    pvesh_args+=(--content "images,iso,vztmpl,backup,rootdir,snippets")
    
    # Add nodes parameter only if nodes_csv is not empty
    if [[ -n "$nodes_csv" ]]; then
        pvesh_args+=(--nodes "$nodes_csv")
        log_debug "Adding nodes parameter: $nodes_csv"
    fi
    
    log_debug "Executing pvesh command with args: ${pvesh_args[*]}"
    
    local pvesh_output
    local pvesh_exit_code
    pvesh_output=$(pvesh "${pvesh_args[@]}" 2>&1)
    pvesh_exit_code=$?
    
    log_debug "pvesh exit code: $pvesh_exit_code"
    log_debug "pvesh output: $pvesh_output"
    
    if [[ $pvesh_exit_code -ne 0 ]]; then
        log_error "perform_new_nfs_setup: STEP 4 FAILED - PVE storage creation failed"
        dialog --title "PVE Storage Creation Failed" --msgbox \
"Failed to add storage to Proxmox VE!\n\n\
ERROR: $pvesh_output\n\n\
SUMMARY:\n\
Destination datasets: CREATED\n\
Source dataset: CREATED\n\
Source NFS share: CREATED\n\
ODP task: $oodp_status\n\
PVE storage: FAILED\n\n\
The datasets and NFS shares are ready.\n\
You can try manually adding the storage:\n\
Server: $selected_nfs_ip\n\
Export: $export_path" \
20 80
        return 1
    fi
    
    log_info "perform_new_nfs_setup: STEP 4 SUCCESS - PVE storage created successfully"
    
    # Wait for storage to mount and refresh cache
    dialog --title "Finalizing" --infobox "Waiting for storage to become available..." 6 50
    wait_for_storage_mount "$new_storage_name" || true  # Non-critical
    refresh_storage_cache "$new_storage_name"
    build_storage_ip_map
    
    # SUCCESS - All steps completed
    log_info "perform_new_nfs_setup: ALL STEPS SUCCESS - NFS storage setup completed successfully"
    
    # Final success message
    local destinations_summary=""
    if [[ -n "$oodp_destinations_json" ]]; then
        local dest_count=$(echo "$oodp_destinations_json" | grep -o '"dataset"' | wc -l)
        destinations_summary="\nDestination datasets: $dest_count created"
    fi
    
    dialog --title "Setup Complete!" --msgbox \
"NFS storage setup completed successfully!\n\n\
STORAGE DETAILS:\n\
Storage Name: $new_storage_name\n\
Server: $selected_nfs_ip\n\
Pool: $selected_pool\n\
Dataset: $selected_dataset\n\
Path: /mnt/pve/$new_storage_name$destinations_summary\n\n\
ODP STATUS:\n\
$oodp_status\n\n\
The storage is now available for use in Proxmox VE." \
18 70
    
    log_info "NFS storage setup completed: $new_storage_name"
    return 0
}

# Helper function: Create OODP task for automatic snapshots with destinations
create_oodp_task() {
    local pool_name="$1"
    local dataset_name="$2"
    local storage_name="$3"
    local ip_address="$4"
    local destinations_json="$5"  # Optional: JSON array of destinations
    
    log_info "Creating OODP task for dataset: $pool_name/$dataset_name"
    
    # Check if REST API credentials are configured
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]] || [[ -z "${rest_api_port}" ]]; then
        log_error "create_oodp_task: REST API credentials not configured. User='${rest_api_user}', Port='${rest_api_port}', Password length=${#rest_api_password}"
        return 1
    fi
    
    # Use configured plan or default
    local src_plan="$oodp_src_default_plan"
    
    # Build the JSON payload
    local json_payload
    if [[ -n "$destinations_json" ]]; then
        # Include destinations in the payload
        json_payload=$(cat <<EOF
{
    "name": "$pool_name/$dataset_name",
    "plan": "$src_plan",
    "description": "$storage_name",
    "dedup": false,
    "granular_resync": true,
    "mbuffer": false,
    "destinations": $destinations_json,
    "send_encrypted": true
}
EOF
        )
    else
        # No destinations - original payload
        json_payload=$(cat <<EOF
{
    "name": "$pool_name/$dataset_name",
    "plan": "$src_plan",
    "description": "$storage_name",
    "dedup": false,
    "granular_resync": true,
    "mbuffer": false,
    "send_encrypted": true
}
EOF
        )
    fi
    
    log_debug "create_oodp_task: JSON payload: $json_payload"
    
    # Make REST API call to create OODP task
    local url="https://$ip_address:$rest_api_port/api/v4/odp/tasks"
    log_debug "create_oodp_task: Making POST request to: $url"
    log_debug "create_oodp_task: Using REST API port: $rest_api_port"
    log_debug "create_oodp_task: Using REST API user: ${rest_api_user}"
    log_debug "create_oodp_task: REST API password length: ${#rest_api_password}"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR MANUAL TESTING ==============="
    log_debug "curl -k -s -w '\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_payload' '$url'"
    log_debug "====================================================================="
    
    # Also log a pretty-printed version of the JSON for easier reading
    log_debug "=== FORMATTED JSON PAYLOAD ==="
    if command -v python3 >/dev/null 2>&1; then
        echo "$json_payload" | python3 -m json.tool 2>/dev/null | while IFS= read -r line; do
            log_debug "  $line"
        done || log_debug "  (Could not pretty-print JSON)"
    else
        log_debug "  $json_payload"
    fi
    log_debug "==============================="
    
    local response
    log_debug "create_oodp_task: About to execute curl command"
    log_debug "curl -k -s --connect-timeout 10 --max-time 30 -w '\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_payload' '$url'"
    response=$(curl -k -s --connect-timeout 10 --max-time 30 -w "\n%{http_code}" -X POST \
        -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_payload" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    log_debug "create_oodp_task: Curl completed with exit code: $curl_exit"
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "create_oodp_task: Failed to connect to OODP API at $ip_address:$rest_api_port (curl exit code: $curl_exit)"
        return 1
    fi
    
    # Extract HTTP status code (last line)
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_debug "create_oodp_task: HTTP status code: $http_code"
    log_debug "create_oodp_task: Response body: $response_body"
    
    # Check if task was created successfully (2xx status codes)
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        log_info "Successfully created OODP task for $pool_name/$dataset_name"
        return 0
    else
        # Check for specific destination configuration error
        if [[ "$http_code" == "403" ]] && echo "$response_body" | grep -q "Destination.*not configured"; then
            log_error "Failed to create OODP task: Backup destinations must be configured in JovianDSS GUI first"
            log_error "Please configure destination servers in Data Protection > Off-Site Protection > Destinations"
        else
            log_error "Failed to create OODP task. HTTP status: $http_code, Response: $response_body"
        fi
        return 1
    fi
}
# Helper function: Get list of backup-dr-node servers
get_backup_dr_servers() {
    log_info "get_backup_dr_servers: Starting function (INFO level for debugging)"
    local backup_servers=()
    local processed_files=0
    
    # Check connected storage servers directory
    if [[ ! -d "$CONNECTED_STORAGE_SERVERS_DIR" ]]; then
        log_info "get_backup_dr_servers: No connected servers directory found: $CONNECTED_STORAGE_SERVERS_DIR"
        return 1
    fi
    
    log_info "get_backup_dr_servers: Connected servers directory exists: $CONNECTED_STORAGE_SERVERS_DIR"
    
    # Read all server info files and find backup-dr-nodes
    for info_file in "$CONNECTED_STORAGE_SERVERS_DIR"/*; do
        log_info "get_backup_dr_servers: Checking file: $info_file"
        [[ ! -f "$info_file" ]] && continue
        
        ((processed_files++))
        
        local ip=$(basename "$info_file")
        log_info "get_backup_dr_servers: Processing IP: $ip"
        
        local node_role=$(grep "^node_role=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        local connection_state=$(grep "^connection_state=" "$info_file" 2>/dev/null | cut -d'=' -f2)
        
        log_info "get_backup_dr_servers: IP $ip - role='$node_role', connection_state='$connection_state'"
        
        # Include connected backup-dr-nodes
        if [[ "$node_role" == "backup-dr-node" ]] && [[ "$connection_state" == "connected" ]]; then
            backup_servers+=("$ip")
            log_info "get_backup_dr_servers: Found backup-dr-node: $ip"
        fi
    done
    
    # Return the list
    if [[ ${#backup_servers[@]} -gt 0 ]]; then
        log_debug "get_backup_dr_servers: Found ${#backup_servers[@]} backup servers: ${backup_servers[*]}"
        printf "%s\n" "${backup_servers[@]}"
        return 0
    else
        log_info "get_backup_dr_servers: No backup-dr-nodes found (checked $processed_files files)"
        return 1
    fi
}

# Helper function: Check if OODP task already exists for dataset
check_oodp_task_exists() {
    local pool_name="$1"
    local dataset_name="$2"
    local ip_address="$3"
    
    log_info "Checking if OODP task exists for dataset: $pool_name/$dataset_name"
    log_debug "check_oodp_task_exists: Called with pool_name='$pool_name', dataset_name='$dataset_name', ip_address='$ip_address'"
    
    # Use the existing get_oodp_details function to check for task
    log_debug "check_oodp_task_exists: About to call get_oodp_details"
    get_oodp_details "$ip_address" "$pool_name" "$dataset_name"
    local get_result=$?
    log_debug "check_oodp_task_exists: get_oodp_details returned exit code: $get_result"
    
    case $get_result in
        0)
            log_info "OODP task found for $pool_name/$dataset_name"
            return 0  # Task exists
            ;;
        1)
            log_info "No OODP task found for $pool_name/$dataset_name"
            return 1  # Task does not exist
            ;;
        10)
            log_error "Authentication failure when checking OODP task"
            return 10  # Auth failure
            ;;
        11)
            log_error "Access forbidden when checking OODP task"
            return 11  # Access forbidden
            ;;
        *)
            log_error "Error checking OODP task (code: $get_result)"
            return 1  # Treat as no task found
            ;;
    esac
}

# Helper function: Create a new dataset via REST API
create_dataset() {
    local pool_name="$1"
    local dataset_name="$2"
    
    log_info "create_dataset: STARTING creation of SOURCE dataset pool_name='$pool_name' dataset_name='$dataset_name' on server='$selected_nfs_ip:$rest_api_port'"
    log_info "create_dataset: SOURCE API endpoint: https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes"
    
    # Validate API credentials are configured
    if [[ -z "$rest_api_user" || -z "$rest_api_password" ]]; then
        log_error "create_dataset: API credentials not configured"
        return 1
    fi
    
    # Prepare JSON payload for dataset creation
    # Based on API docs: only "name" is required
    local json_input="{\"name\": \"${dataset_name}\"}"
    log_info "create_dataset: JSON payload: $json_input for SOURCE pool_name='$pool_name' dataset_name='$dataset_name'"
    
    log_info "create_dataset: Making API call to SOURCE server='$selected_nfs_ip:$rest_api_port' for pool_name='$pool_name' dataset_name='$dataset_name'"
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE DATASET ==============="
    log_debug "curl -k -s --connect-timeout 10 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_input' '$url'"
    log_debug "===================================================================="

    local response
    response=$(curl -k -s --connect-timeout 10 --max-time 30 -w "\n%{http_code}" -X POST -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "$url" 2>/dev/null)
    
    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "create_dataset: CURL FAILED to connect to SOURCE server='$selected_nfs_ip:$rest_api_port' for pool_name='$pool_name' dataset_name='$dataset_name' curl_exit='$curl_exit'"
        return 1
    fi
    
    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)
    
    log_info "create_dataset: API response HTTP_STATUS='$status' for SOURCE pool_name='$pool_name' dataset_name='$dataset_name' on server='$selected_nfs_ip:$rest_api_port'"
    log_debug "create_dataset: API response BODY='$body' for SOURCE pool_name='$pool_name' dataset_name='$dataset_name'"
    
    if [[ $status -eq 200 || $status -eq 201 ]]; then
        log_info "create_dataset: SOURCE dataset created SUCCESS pool_name='$pool_name' dataset_name='$dataset_name' on server='$selected_nfs_ip:$rest_api_port' HTTP_STATUS='$status'"
        return 0
    else
        log_error "create_dataset: SOURCE dataset creation FAILED pool_name='$pool_name' dataset_name='$dataset_name' on server='$selected_nfs_ip:$rest_api_port' HTTP_STATUS='$status'"
        log_error "create_dataset: SOURCE dataset FAILED RESPONSE='$body' for pool_name='$pool_name' dataset_name='$dataset_name'"
        
        # Try to extract error message from response
        if [[ -n "$body" ]]; then
            local error_msg=$(echo "$body" | grep -o '"message":"[^"]*"' | sed 's/"message":"\([^"]*\)"/\1/' 2>/dev/null || echo "$body")
            log_error "create_dataset: SOURCE dataset ERROR DETAILS='$error_msg' for pool_name='$pool_name' dataset_name='$dataset_name'"
        fi
        
        return 1
    fi
}

# Helper function: Create NFS share for a dataset
create_nfs_share_for_dataset() {
    local pool_name="$1"
    local dataset_name="$2"
    local share_name="$dataset_name"
    local path="${pool_name}/${dataset_name}"
    
    log_info "Creating NFS share for dataset: $dataset_name"
    
    # Create NFS share using the same structure as create_nfs_share_for_clone
    local json_input
    json_input=$(cat <<EOF
{
    "name": "$share_name",
    "path": "$path",
    "nfs": {
        "enabled": true,
        "synchronous_data_record": true,
        "no_root_squash": true
    }
}
EOF
)
    
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/shares"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE NFS SHARE FOR DATASET ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_input' '$url'"
    log_debug "================================================================================="

    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X POST -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "$url")
    
    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)
    
    if [[ $status -eq 201 ]]; then
        log_info "NFS share '$share_name' created at: $path"
        return 0
    elif [[ $status -eq 409 ]]; then
        # Share already exists
        log_info "NFS share '$share_name' already exists"
        return 0
    else
        log_error "Failed to create NFS share. Status: $status"
        log_error "Response: $body"
        return 1
    fi
}

# Helper function: Find next available dataset number
find_next_dataset_number() {
    local pool_name="$1"
    local start_pattern="$new_NFS_storage_volume_running_number_starts_with"
    
    # Determine starting number and format from pattern (same logic as storage naming)
    local start_number
    local format_pattern
    
    case "$start_pattern" in
        "00")
            start_number=0
            format_pattern="%02d"
            ;;
        "01")
            start_number=1
            format_pattern="%02d"
            ;;
        "000")
            start_number=0
            format_pattern="%03d"
            ;;
        "001")
            start_number=1
            format_pattern="%03d"
            ;;
        "10")
            start_number=10
            format_pattern="%d"
            ;;
        "100")
            start_number=100
            format_pattern="%d"
            ;;
        *)
            # Fallback to default
            start_number=1
            format_pattern="%02d"
            log_error "Unknown number pattern '$start_pattern', using default %02d format"
            ;;
    esac
    
    local number=$start_number
    
    log_debug "Finding next available dataset number for pool: $pool_name, start_pattern: $start_pattern, start_number: $start_number, format: $format_pattern"
    
    # Get all existing datasets with the auto-prefix
    local existing_datasets=()
    while IFS= read -r dataset; do
        if [[ "$dataset" =~ ^${dataset_auto_prefix}-([0-9]+)$ ]]; then
            existing_datasets+=("$dataset")
        fi
    done < <(get_datasets_in_pool "$pool_name" 2>/dev/null)
    
    # Find first available number starting from configured pattern
    while true; do
        local candidate="${dataset_auto_prefix}-$(printf "$format_pattern" "$number")"
        local exists=false
        
        for existing in "${existing_datasets[@]}"; do
            if [[ "$existing" == "$candidate" ]]; then
                exists=true
                break
            fi
        done
        
        if [[ "$exists" == false ]]; then
            printf "$format_pattern" "$number"
            return 0
        fi
        
        ((number++))
        
        # Safety check to prevent infinite loop - adjust limit based on format
        local max_limit=999
        if [[ "$format_pattern" == "%02d" ]]; then
            max_limit=99
        elif [[ "$format_pattern" == "%03d" ]]; then
            max_limit=999
        fi
        
        if [[ $number -gt $max_limit ]]; then
            log_error "find_next_dataset_number: Reached maximum dataset number limit ($max_limit) for pattern $start_pattern"
            return 1
        fi
    done
}

# Helper function: Find next available storage name
find_next_storage_name() {
    local prefix="${storage_vm_name_contains}-nfs"
    local start_pattern="$new_NFS_storage_volume_running_number_starts_with"
    
    # Determine starting number and format from pattern
    local start_number
    local format_pattern
    
    case "$start_pattern" in
        "00")
            start_number=0
            format_pattern="%02d"
            ;;
        "01")
            start_number=1
            format_pattern="%02d"
            ;;
        "000")
            start_number=0
            format_pattern="%03d"
            ;;
        "001")
            start_number=1
            format_pattern="%03d"
            ;;
        "10")
            start_number=10
            format_pattern="%d"
            ;;
        "100")
            start_number=100
            format_pattern="%d"
            ;;
        *)
            # Fallback to default
            start_number=1
            format_pattern="%02d"
            log_error "Unknown number pattern '$start_pattern', using default %02d format"
            ;;
    esac
    
    local number=$start_number
    
    log_debug "Finding next available storage name with prefix: $prefix, start_pattern: $start_pattern, start_number: $start_number, format: $format_pattern"
    
    # Get all existing storage names
    local existing_storages=()
    while IFS= read -r storage_name; do
        if [[ "$storage_name" =~ ^${prefix}([0-9]+)$ ]]; then
            existing_storages+=("$storage_name")
            log_debug "find_next_storage_name: Found matching storage: $storage_name"
        fi
    done < <(grep -Po '^nfs:\s*\K\S+' /etc/pve/storage.cfg 2>/dev/null)
    
    log_debug "find_next_storage_name: Total existing storages: ${#existing_storages[@]} [${existing_storages[*]}]"
    
    # Find first available number starting from configured pattern
    while true; do
        local candidate="${prefix}$(printf "$format_pattern" "$number")"
        local exists=false
        
        log_debug "find_next_storage_name: Checking candidate: $candidate"
        
        for existing in "${existing_storages[@]}"; do
            if [[ "$existing" == "$candidate" ]]; then
                exists=true
                log_debug "find_next_storage_name: Candidate $candidate already exists"
                break
            fi
        done
        
        if [[ "$exists" == false ]]; then
            log_debug "find_next_storage_name: Found available name: $candidate"
            echo "$candidate"
            return 0
        fi
        
        ((number++))
        
        # Safety check to prevent infinite loop - adjust limit based on format
        local max_limit=999
        if [[ "$format_pattern" == "%02d" ]]; then
            max_limit=99
        elif [[ "$format_pattern" == "%03d" ]]; then
            max_limit=999
        fi
        
        if [[ $number -gt $max_limit ]]; then
            log_error "find_next_storage_name: Reached maximum storage number limit ($max_limit) for pattern $start_pattern"
            return 1
        fi
    done
}

# Helper function: Get export path from PVE storage configuration
# Example: joviandss-nfs03 -> /datastore-pve-03
get_pve_storage_export_path() {
    local pve_storage_name="$1"
    
    if [[ -z "$pve_storage_name" ]]; then
        log_error "get_pve_storage_export_path: pve_storage_name is required"
        return 1
    fi
    
    log_debug "get_pve_storage_export_path: Looking for export path of storage '$pve_storage_name'"
    
    # Parse /etc/pve/storage.cfg to find export path
    local in_storage_section=false
    local export_path=""
    
    while IFS= read -r line; do
        # Check if we're entering the target storage section
        if [[ "$line" =~ ^nfs:[[:space:]]*${pve_storage_name}[[:space:]]*$ ]]; then
            in_storage_section=true
            log_debug "get_pve_storage_export_path: Found storage section for '$pve_storage_name'"
            continue
        fi
        
        # Check if we're entering a different storage section
        if [[ "$line" =~ ^[a-zA-Z]+:[[:space:]]*[^[:space:]]+ ]] && [[ "$in_storage_section" == "true" ]]; then
            # We've moved to a different storage section, stop processing
            break
        fi
        
        # If we're in the target storage section, look for export line
        if [[ "$in_storage_section" == "true" ]] && [[ "$line" =~ ^[[:space:]]*export[[:space:]]+(.+)$ ]]; then
            export_path="${BASH_REMATCH[1]}"
            log_debug "get_pve_storage_export_path: Found export path '$export_path'"
            break
        fi
    done < /etc/pve/storage.cfg
    
    if [[ -z "$export_path" ]]; then
        log_error "get_pve_storage_export_path: No export path found for storage '$pve_storage_name'"
        return 1
    fi
    
    echo "$export_path"
    return 0
}

# Helper function: Get dataset path from share name using REST API
# Example: datastore-pve-03 -> Pool-0/datastore-pve-03
get_dataset_from_share_name() {
    local share_name="$1"
    local nfs_ip="$2"
    
    if [[ -z "$share_name" ]]; then
        log_error "get_dataset_from_share_name: share_name is required"
        return 1
    fi
    
    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_from_share_name: nfs_ip is required"
        return 1
    fi
    
    log_debug "get_dataset_from_share_name: Looking for dataset containing share '$share_name' on IP '$nfs_ip'"
    
    # Make REST API call to get shares
    local url="https://${nfs_ip}:${rest_api_port}/api/v4/shares?where=name==${share_name}"
    
    # Use temporary file for reliable parsing
    local temp_response=$(mktemp)
    local temp_status=$(mktemp)
    
    curl -k -s -X GET \
        -u "${rest_api_user}:${rest_api_password}" \
        -w "%{http_code}" \
        -o "$temp_response" \
        "$url" > "$temp_status" 2>/dev/null
    
    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_dataset_from_share_name: Failed to connect to server"
        rm -f "$temp_response" "$temp_status"
        return 1
    fi
    
    # Parse response
    local response_body=$(cat "$temp_response")
    local http_status=$(cat "$temp_status")
    rm -f "$temp_response" "$temp_status"
    
    log_debug "get_dataset_from_share_name: HTTP Status: $http_status"
    log_debug "get_dataset_from_share_name: Response: $response_body"
    
    # Check for authentication failure
    if check_rest_api_auth_failure "get_dataset_from_share_name" "$http_status" "$response_body"; then
        local auth_result=$?
        if [[ $auth_result -eq 10 ]]; then
            log_error "get_dataset_from_share_name: REST API authentication failed (401)"
            return 1
        elif [[ $auth_result -eq 11 ]]; then
            log_error "get_dataset_from_share_name: REST API access forbidden (403)"
            return 1
        fi
    fi
    
    if [[ "$http_status" == "200" ]]; then
        # Extract dataset path from JSON response
        local dataset_path
        dataset_path=$(echo "$response_body" | grep -Po '"path"\s*:\s*"\K[^"]+' | head -1)
        
        if [[ -n "$dataset_path" ]]; then
            log_debug "get_dataset_from_share_name: Found dataset path '$dataset_path' for share '$share_name'"
            echo "$dataset_path"
            return 0
        else
            log_error "get_dataset_from_share_name: Share '$share_name' not found in API response"
            return 1
        fi
    else
        log_error "get_dataset_from_share_name: Failed to get shares. HTTP status: $http_status, Response: $response_body"
        return 1
    fi
}

# Helper function: Create backup datasets on destination servers
create_backup_datasets() {
    log_info "create_backup_datasets: Starting backup dataset creation"
    
    # Check if OODP destinations were configured in the wizard
    if [[ -z "$oodp_destinations_json" ]]; then
        log_debug "create_backup_datasets: No destinations configured, skipping"
        return 0
    fi
    
    log_debug "create_backup_datasets: Processing destinations JSON: $oodp_destinations_json"
    
    # Parse destinations from JSON and create datasets
    local success_count=0
    local total_count=0
    local failed_destinations=()
    
    # Extract destination info from JSON
    # Expected format: [{"plan":"...","dataset":"192.168.21.142:40000/poolname/datasetname"}]
    while IFS= read -r destination_entry; do
        ((total_count++))
        
        # Extract dataset field from JSON entry
        local dataset_spec=$(echo "$destination_entry" | grep -Po '"dataset"\s*:\s*"\K[^"]+')
        
        if [[ -z "$dataset_spec" ]]; then
            log_error "create_backup_datasets: Invalid destination format in JSON: $destination_entry"
            failed_destinations+=("Invalid JSON format")
            continue
        fi
        
        log_debug "create_backup_datasets: Processing destination dataset: $dataset_spec"
        
        # Parse format: IP:PORT/POOL/DATASET
        if [[ "$dataset_spec" =~ ^([0-9.]+):([0-9]+)/([^/]+)/(.+)$ ]]; then
            local dst_ip="${BASH_REMATCH[1]}"
            local dst_port="${BASH_REMATCH[2]}"
            local dst_pool="${BASH_REMATCH[3]}"
            local dst_dataset="${BASH_REMATCH[4]}"
            
            log_info "create_backup_datasets: Creating DESTINATION dataset pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$dst_port'"
            
            # Show progress for this specific destination
            dialog --title "Creating DESTINATION Dataset" --infobox "Creating DESTINATION dataset:\nServer: $dst_ip:$dst_port\nPool: $dst_pool\nDataset: $dst_dataset" 10 70
            
            # Verify that the destination pool exists before creating dataset
            # TEMPORARY: Skip pool verification due to credential issues
            log_info "create_backup_datasets: Skipping pool verification (credential issue), attempting dataset creation"
            # if verify_destination_pool_exists "$dst_ip" "$dst_port" "$dst_pool"; then
                # Create the dataset on the destination server
                if create_remote_backup_dataset "$dst_ip" "$dst_port" "$dst_pool" "$dst_dataset"; then
                    log_info "create_backup_datasets: DESTINATION dataset created SUCCESS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$dst_port'"
                    ((success_count++))
                else
                    log_error "create_backup_datasets: DESTINATION dataset creation FAILED pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$dst_port'"
                    failed_destinations+=("$dst_ip:$dst_port/$dst_pool/$dst_dataset")
                fi
            # else
            #     log_error "create_backup_datasets: Destination pool '$dst_pool' does not exist on $dst_ip:$dst_port"
            #     failed_destinations+=("$dst_ip:$dst_port/$dst_pool/$dst_dataset")
            # fi
        else
            log_error "create_backup_datasets: Invalid dataset specification format: $dataset_spec"
            failed_destinations+=("$dataset_spec")
        fi
        
    done < <(echo "$oodp_destinations_json" | grep -Po '\{[^}]*"dataset"[^}]*\}')
    
    # Report results
    log_info "create_backup_datasets: Created $success_count of $total_count backup datasets"
    
    if [[ ${#failed_destinations[@]} -gt 0 ]]; then
        log_error "create_backup_datasets: Failed destinations: ${failed_destinations[*]}"
        return 1
    elif [[ $total_count -eq 0 ]]; then
        log_info "create_backup_datasets: No destinations found in JSON"
        return 0
    else
        log_info "create_backup_datasets: All backup datasets created successfully"
        return 0
    fi
}

# Helper function: Create a dataset on a remote backup server
create_remote_backup_dataset() {
    local dst_ip="$1"
    local dst_port="$2"
    local dst_pool="$3"
    local dst_dataset="$4"
    
    log_info "create_remote_backup_dataset: STARTING creation of DESTINATION dataset pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$dst_port'"
    
    # Prepare JSON payload for dataset creation
    local json_payload
    json_payload=$(cat <<EOF
{
    "name": "$dst_dataset"
}
EOF
)
    
    log_info "create_remote_backup_dataset: JSON payload: $json_payload for pool_name='$dst_pool' dataset_name='$dst_dataset'"
    
    # Make REST API call to create ZFS dataset (not NFS share) on destination server  
    local url="https://$dst_ip:$rest_api_port/api/v4/pools/$dst_pool/datasets"
    log_info "create_remote_backup_dataset: Making API call to DESTINATION server='$dst_ip:$rest_api_port' URL='$url' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE REMOTE BACKUP DATASET ==============="
    log_debug "curl -k -s --connect-timeout 10 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_payload' '$url'"
    log_debug "================================================================================="
    
    local response
    response=$(curl -k -s --connect-timeout 10 --max-time 30 -w "\n%{http_code}" -X POST \
        -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_payload" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "create_remote_backup_dataset: CURL FAILED to connect to DESTINATION server='$dst_ip:$rest_api_port' for pool_name='$dst_pool' dataset_name='$dst_dataset' curl_exit='$curl_exit'"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_info "create_remote_backup_dataset: API response HTTP_CODE='$http_code' for DESTINATION pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port'"
    log_debug "create_remote_backup_dataset: API response BODY='$response_body' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
    
    # Check if dataset was created successfully
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        log_info "create_remote_backup_dataset: DESTINATION dataset created SUCCESS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port' HTTP_CODE='$http_code'"
        return 0
    elif [[ "$http_code" == "409" ]]; then
        # Dataset already exists - this is acceptable
        log_info "create_remote_backup_dataset: DESTINATION dataset ALREADY EXISTS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port' HTTP_CODE='$http_code'"
        return 0
    else
        log_error "create_remote_backup_dataset: DESTINATION dataset creation FAILED pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port' HTTP_CODE='$http_code'"
        log_error "create_remote_backup_dataset: DESTINATION dataset FAILED RESPONSE='$response_body' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
        return 1
    fi
}

# Helper function: Create dataset on remote server using same logic as source
create_remote_dataset() {
    local dst_ip="$1"
    local dst_pool="$2"
    local dst_dataset="$3"
    
    log_info "create_remote_dataset: Creating DESTINATION dataset pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port'"
    log_info "create_remote_dataset: Using same logic as source dataset creation"
    
    # Validate API credentials are configured
    if [[ -z "$rest_api_user" || -z "$rest_api_password" ]]; then
        log_error "create_remote_dataset: API credentials not configured for destination"
        return 1
    fi
    
    # Prepare JSON payload for dataset creation (same as source)
    local json_input="{\"name\": \"${dst_dataset}\"}"
    log_info "create_remote_dataset: JSON payload: $json_input for DESTINATION pool_name='$dst_pool' dataset_name='$dst_dataset'"
    
    log_info "create_remote_dataset: Making API call to DESTINATION server='$dst_ip:$rest_api_port' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
    local url="https://$dst_ip:$rest_api_port/api/v4/pools/$dst_pool/nas-volumes"

    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR CREATE REMOTE DATASET ==============="
    log_debug "curl -k -s --connect-timeout 10 --max-time 30 -w '\\n%{http_code}' -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_input' '$url'"
    log_debug "============================================================================"

    local response
    response=$(curl -k -s --connect-timeout 10 --max-time 30 -w "\n%{http_code}" -X POST -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "$url" 2>/dev/null)
    
    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "create_remote_dataset: CURL FAILED to connect to DESTINATION server='$dst_ip:$rest_api_port' for pool_name='$dst_pool' dataset_name='$dst_dataset' curl_exit='$curl_exit'"
        return 1
    fi
    
    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)
    
    log_info "create_remote_dataset: API response HTTP_STATUS='$status' for DESTINATION pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port'"
    log_debug "create_remote_dataset: API response BODY='$body' for DESTINATION pool_name='$dst_pool' dataset_name='$dst_dataset'"
    
    if [[ $status -eq 200 || $status -eq 201 ]]; then
        log_info "create_remote_dataset: DESTINATION dataset created SUCCESS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port' HTTP_STATUS='$status'"
        return 0
    else
        log_error "create_remote_dataset: DESTINATION dataset creation FAILED pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip:$rest_api_port' HTTP_STATUS='$status'"
        log_error "create_remote_dataset: DESTINATION dataset FAILED RESPONSE='$body' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
        
        # Try to extract error message from response
        if [[ -n "$body" ]]; then
            local error_msg=$(echo "$body" | grep -o '"message":"[^"]*"' | sed 's/"message":"\([^"]*\)"/\1/' 2>/dev/null || echo "$body")
            log_error "create_remote_dataset: DESTINATION dataset ERROR DETAILS='$error_msg' for pool_name='$dst_pool' dataset_name='$dst_dataset'"
        fi
        
        return 1
    fi
}

# Helper function: Check if a destination dataset exists
check_destination_dataset_exists() {
    local dst_ip="$1"
    local dst_pool="$2"
    local dst_dataset="$3"
    
    log_debug "check_destination_dataset_exists: Checking if dataset exists - pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
    
    # Check if REST API credentials are configured
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]] || [[ -z "${rest_api_port}" ]]; then
        log_error "check_destination_dataset_exists: REST API credentials not configured"
        return 2  # Return 2 for configuration error
    fi
    
    # Make API call to get datasets in the pool
    local url="https://$dst_ip:$rest_api_port/api/v4/pools/$dst_pool/nas-volumes"
    log_debug "check_destination_dataset_exists: Making GET request to: $url"
    
    local response
    response=$(curl -k -s -w "\n%{http_code}" -X GET \
        -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "check_destination_dataset_exists: Failed to connect to API at $dst_ip:$rest_api_port (curl exit code: $curl_exit)"
        return 2  # Return 2 for connection error
    fi
    
    # Extract HTTP status code (last line) and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_debug "check_destination_dataset_exists: HTTP status code: $http_code"
    log_debug "check_destination_dataset_exists: Response length: ${#response_body} chars"
    
    # Check if API call was successful
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        # Check if dataset exists in the response
        if echo "$response_body" | grep -q "\"name\": \"$dst_dataset\""; then
            log_info "check_destination_dataset_exists: Dataset EXISTS - pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
            return 0  # Dataset exists
        else
            log_debug "check_destination_dataset_exists: Dataset NOT found - pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
            return 1  # Dataset does not exist
        fi
    else
        log_error "check_destination_dataset_exists: API call failed. HTTP status: $http_code"
        log_debug "check_destination_dataset_exists: Response: $response_body"
        return 2  # Return 2 for API error
    fi
}

# Helper function: Check if ODP destination node is registered
check_odp_destination_registered() {
    local dst_ip="$1"
    local dst_port="$2"
    local src_ip="$3"  # Source server IP where we check registration
    
    local dst_address="${dst_ip}:${dst_port}"
    log_debug "check_odp_destination_registered: Checking if ODP destination is registered - address='$dst_address' on source='$src_ip'"
    
    # Check if REST API credentials are configured
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]] || [[ -z "${rest_api_port}" ]]; then
        log_error "check_odp_destination_registered: REST API credentials not configured"
        return 2  # Return 2 for configuration error
    fi
    
    # Make API call to get registered ODP nodes
    local url="https://$src_ip:$rest_api_port/api/v4/odp/nodes"
    log_debug "check_odp_destination_registered: Making GET request to: $url"
    
    local response
    response=$(curl -k -s -w "\n%{http_code}" -X GET \
        --connect-timeout 5 --max-time 90 \
        -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "check_odp_destination_registered: Failed to connect to API at $src_ip:$rest_api_port (curl exit code: $curl_exit)"
        return 2  # Return 2 for connection error
    fi
    
    # Extract HTTP status code (last line) and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_debug "check_odp_destination_registered: HTTP status code: $http_code"
    log_debug "check_odp_destination_registered: Response: $response_body"
    
    # Check if API call was successful
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        # Check if destination is registered in the response
        if echo "$response_body" | grep -q "\"address\": \"$dst_address\""; then
            log_info "check_odp_destination_registered: ODP destination REGISTERED - address='$dst_address' on source='$src_ip'"
            return 0  # Destination is registered
        else
            log_debug "check_odp_destination_registered: ODP destination NOT registered - address='$dst_address' on source='$src_ip'"
            return 1  # Destination is not registered
        fi
    else
        log_error "check_odp_destination_registered: API call failed. HTTP status: $http_code"
        log_debug "check_odp_destination_registered: Response: $response_body"
        return 2  # Return 2 for API error
    fi
}

# Helper function: Register ODP destination node
register_odp_destination_node() {
    local dst_ip="$1"
    local dst_port="$2"
    local src_ip="$3"  # Source server IP where we register
    local description="$4"  # Optional description
    
    local dst_address="${dst_ip}:${dst_port}"
    log_info "register_odp_destination_node: Registering ODP destination - address='$dst_address' on source='$src_ip'"
    
    # Check if REST API credentials are configured
    if [[ -z "${rest_api_user}" ]] || [[ -z "${rest_api_password}" ]] || [[ -z "${rest_api_port}" ]]; then
        log_error "register_odp_destination_node: REST API credentials not configured"
        return 1
    fi
    
    # Build JSON payload for ODP node registration
    local json_payload
    json_payload=$(cat <<EOF
{
    "address": "$dst_address",
    "description": "${description:-Backup DR Node via pve-tools}",
    "password": "${storage_server_web_gui_password}"
}
EOF
)
    
    log_debug "register_odp_destination_node: JSON payload: $json_payload"
    
    # Make API call to register ODP destination node
    local url="https://$src_ip:$rest_api_port/api/v4/odp/nodes"
    log_debug "register_odp_destination_node: Making POST request to: $url"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR ODP NODE REGISTRATION ==============="
    log_debug "curl -k -s -w '\\n%{http_code}' -X POST --connect-timeout 5 --max-time 90 -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_payload' '$url'"
    log_debug "============================================================================"
    
    # Also log a pretty-printed version of the JSON for easier reading
    log_debug "=== FORMATTED JSON PAYLOAD FOR ODP NODE REGISTRATION ==="
    if command -v python3 >/dev/null 2>&1; then
        echo "$json_payload" | python3 -m json.tool 2>/dev/null | while IFS= read -r line; do
            log_debug "  $line"
        done || log_debug "  (Could not pretty-print JSON)"
    else
        log_debug "  $json_payload"
    fi
    log_debug "========================================================="
    
    local response
    response=$(curl -k -s -w "\n%{http_code}" -X POST \
        --connect-timeout 5 --max-time 90 \
        -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_payload" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "register_odp_destination_node: Failed to connect to API at $src_ip:$rest_api_port (curl exit code: $curl_exit)"
        return 1
    fi
    
    # Extract HTTP status code (last line) and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_debug "register_odp_destination_node: HTTP status code: $http_code"
    log_debug "register_odp_destination_node: Response: $response_body"
    
    # Check if registration was successful
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        log_info "register_odp_destination_node: ODP destination registered SUCCESS - address='$dst_address' on source='$src_ip'"
        return 0
    elif [[ "$http_code" == "409" ]]; then
        # Node already registered - this is acceptable
        log_info "register_odp_destination_node: ODP destination ALREADY registered - address='$dst_address' on source='$src_ip'"
        return 0
    else
        log_error "register_odp_destination_node: Failed to register ODP destination. HTTP status: $http_code"
        log_error "register_odp_destination_node: Response: $response_body"
        return 1
    fi
}

# Helper function: Check if server supports destination flag feature (up >= 32)
check_server_supports_destination_flag() {
    local server_ip="$1"
    
    log_debug "check_server_supports_destination_flag: Checking version support for server $server_ip"
    
    # Look for the info file for this server IP
    local info_file="$CONNECTED_STORAGE_SERVERS_DIR/$server_ip"
    
    if [[ ! -f "$info_file" ]]; then
        log_debug "check_server_supports_destination_flag: No info file found for server $server_ip"
        return 1
    fi
    
    # Extract the "up" value from the server info file
    local up_value=$(grep "^up=" "$info_file" 2>/dev/null | cut -d'=' -f2)
    
    if [[ -z "$up_value" ]]; then
        log_debug "check_server_supports_destination_flag: No 'up' value found for server $server_ip"
        return 1
    fi
    
    # Check if up value is numeric and >= 32
    if [[ "$up_value" =~ ^[0-9]+$ ]]; then
        if [[ "$up_value" -ge 32 ]]; then
            log_debug "check_server_supports_destination_flag: Server $server_ip supports destination flag (up=$up_value >= 32)"
            return 0
        else
            log_debug "check_server_supports_destination_flag: Server $server_ip does not support destination flag (up=$up_value < 32)"
            return 1
        fi
    else
        log_debug "check_server_supports_destination_flag: Invalid 'up' value for server $server_ip: $up_value"
        return 1
    fi
}

# Helper function: Check if dataset is available in ODP volumes collection
check_dataset_available_in_odp() {
    local dst_ip="$1"
    local dst_pool="$2"
    local dst_dataset="$3"
    
    # Check if REST API credentials are configured
    if [[ -z "$rest_api_user" ]] || [[ -z "$rest_api_password" ]] || [[ -z "$rest_api_port" ]]; then
        log_error "check_dataset_available_in_odp: REST API credentials not configured"
        return 1
    fi
    
    local url="https://$dst_ip:$rest_api_port/api/v4/odp/volumes/$dst_pool/$dst_dataset"
    
    local response=$(curl -k -s -w "\n%{http_code}" --connect-timeout 5 --max-time 10 -u "${rest_api_user}:${rest_api_password}" "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        return 1
    fi
    
    local http_code=$(echo "$response" | tail -n1)
    
    # HTTP 200 means dataset is available in ODP collection
    if [[ "$http_code" == "200" ]]; then
        return 0
    else
        return 1
    fi
}

# Helper function: Set destination flag for OODP volumes (JovianDSS version >= 32)
set_destination_flag_for_volume() {
    local dst_ip="$1"
    local dst_pool="$2" 
    local dst_dataset="$3"
    
    log_info "set_destination_flag_for_volume: Setting destination flag for volume $dst_pool/$dst_dataset on server $dst_ip"
    
    # Check if REST API credentials are configured
    if [[ -z "$rest_api_user" ]] || [[ -z "$rest_api_password" ]] || [[ -z "$rest_api_port" ]]; then
        log_error "set_destination_flag_for_volume: REST API credentials not configured"
        return 1
    fi
    
    local url="https://$dst_ip:$rest_api_port/api/v4/odp/nas-volumes/$dst_pool/$dst_dataset/set-destination-flag"
    
    log_debug "set_destination_flag_for_volume: Making POST request to: $url"
    log_debug "=============== FULL CURL COMMAND FOR SET DESTINATION FLAG ==============="
    log_debug "curl -k -s -w '\\n%{http_code}' --connect-timeout 30 --max-time 60 -X POST -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '{}' '$url'"
    log_debug "================================================================="
    
    local response
    response=$(curl -k -s -w "\n%{http_code}" \
        --connect-timeout 30 \
        --max-time 60 \
        -X POST \
        -H "Content-Type: application/json" \
        -d '{}' \
        -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>&1)
    local curl_exit=$?
    
    log_debug "set_destination_flag_for_volume: curl exit code: $curl_exit"
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "set_destination_flag_for_volume: Failed to connect to API at $dst_ip:$rest_api_port (curl exit code: $curl_exit)"
        return 1
    fi
    
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | sed '$d')
    
    log_debug "set_destination_flag_for_volume: HTTP status code: $http_code"
    log_debug "set_destination_flag_for_volume: Response: $response_body"
    
    case "$http_code" in
        200|201|204)
            log_info "set_destination_flag_for_volume: Destination flag set successfully for $dst_pool/$dst_dataset on server $dst_ip"
            return 0
            ;;
        *)
            log_error "set_destination_flag_for_volume: Failed to set destination flag. HTTP status: $http_code"
            log_error "set_destination_flag_for_volume: Response: $response_body"
            return 1
            ;;
    esac
}

# Helper function: Create destination datasets with proper error handling
create_destination_datasets() {
    local failed_destinations=()
    local success_count=0
    local total_count=${#oodp_destination_datasets[@]}
    
    log_info "create_destination_datasets: Starting destination dataset creation"
    log_debug "create_destination_datasets: Processing ${#oodp_destination_datasets[@]} destination datasets"
    
    # Check if destinations are configured
    if [[ ${#oodp_destination_datasets[@]} -eq 0 ]]; then
        log_debug "create_destination_datasets: No destinations configured, skipping"
        return 0
    fi
    
    # Process each destination dataset
    local dest_index=0
    for dest_info in "${oodp_destination_datasets[@]}"; do
        ((dest_index++))
        
        # Parse format: IP|POOL|DATASET (simple pipe-separated, no port!)
        IFS='|' read -r dst_ip dst_pool dst_dataset <<< "$dest_info"
        
        if [[ -z "$dst_ip" ]] || [[ -z "$dst_pool" ]] || [[ -z "$dst_dataset" ]]; then
            log_error "create_destination_datasets: Invalid destination format: $dest_info"
            failed_destinations+=("Invalid format: $dest_info")
            continue
        fi
        
        log_info "create_destination_datasets: Creating DESTINATION dataset $dest_index of $total_count - pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
        log_debug "create_destination_datasets: Parsed - IP='$dst_ip', Pool='$dst_pool', Dataset='$dst_dataset'"
        
        # Check if destination dataset already exists
        log_info "create_destination_datasets: Checking if destination dataset already exists..."
        dialog --title "Creating Destination Datasets" --infobox "Checking destination dataset $dest_index of $total_count...\n\nServer: $dst_ip\nPool: $dst_pool\nDataset: $dst_dataset" 10 70
        
        check_destination_dataset_exists "$dst_ip" "$dst_pool" "$dst_dataset"
        local exists_result=$?
        
        if [[ $exists_result -eq 0 ]]; then
            # Dataset already exists - prompt user for confirmation
            log_info "create_destination_datasets: DESTINATION dataset ALREADY EXISTS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
            
            dialog --title "Destination Dataset Already Exists" --msgbox \
"The destination dataset already exists!\n\n\
Server: $dst_ip\n\
Pool: $dst_pool\n\
Dataset: $dst_dataset\n\n\
This is acceptable for backup destinations.\n\
The ODP task will use the existing dataset.\n\n\
Click OK to continue with this destination." \
16 60
            
            log_info "create_destination_datasets: User confirmed using existing DESTINATION dataset pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
            ((success_count++))
            
            # Set destination flag if server supports it (up >= 32) for existing dataset
            if check_server_supports_destination_flag "$dst_ip"; then
                log_info "create_destination_datasets: Setting destination flag for existing dataset $dst_pool/$dst_dataset on server $dst_ip"
                dialog --title "Creating Destination Datasets" --infobox "Setting destination flag for existing dataset $dest_index of $total_count...\n\nServer: $dst_ip\nPool: $dst_pool\nDataset: $dst_dataset" 10 70
                
                if set_destination_flag_for_volume "$dst_ip" "$dst_pool" "$dst_dataset"; then
                    log_info "create_destination_datasets: Destination flag set successfully for existing dataset $dst_pool/$dst_dataset on server $dst_ip"
                else
                    log_error "create_destination_datasets: Failed to set destination flag for existing dataset $dst_pool/$dst_dataset on server $dst_ip (non-critical)"
                fi
            else
                log_debug "create_destination_datasets: Server $dst_ip does not support destination flag feature, skipping"
            fi
            
        elif [[ $exists_result -eq 1 ]]; then
            # Dataset does not exist - create it
            log_info "create_destination_datasets: DESTINATION dataset does not exist, creating it now..."
            dialog --title "Creating Destination Datasets" --infobox "Creating destination dataset $dest_index of $total_count...\n\nServer: $dst_ip\nPool: $dst_pool\nDataset: $dst_dataset" 10 70
            
            # Create the dataset on the destination server using same logic as source
            if create_remote_dataset "$dst_ip" "$dst_pool" "$dst_dataset"; then
                log_info "create_destination_datasets: DESTINATION dataset created SUCCESS pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
                ((success_count++))
                
                # For up32, set destination flag if server supports it
                if check_server_supports_destination_flag "$dst_ip"; then
                    log_info "create_destination_datasets: Server $dst_ip supports destination flag (up32), setting flag for dataset $dst_pool/$dst_dataset"
                    
                    # Wait for dataset to be available for destination flag setting
                    log_debug "create_destination_datasets: Waiting for dataset $dst_pool/$dst_dataset to be ready for destination flag"
                    local wait_count=0
                    local max_wait=5
                    local dataset_ready=false
                    
                    while [[ $wait_count -lt $max_wait ]]; do
                        log_debug "create_destination_datasets: Checking dataset availability (attempt $((wait_count + 1)))"
                        
                        # Log the full curl command for debugging
                        log_debug "=============== FULL CURL COMMAND FOR TEST DESTINATION FLAG (attempt $((wait_count + 1))) ==============="
                        log_debug "curl -k -s -w '\\n%{http_code}' --connect-timeout 5 --max-time 10 -X POST -H 'Content-Type: application/json' -d '{}' -u '${rest_api_user}:${rest_api_password}' 'https://$dst_ip:$rest_api_port/api/v4/odp/nas-volumes/$dst_pool/$dst_dataset/set-destination-flag'"
                        log_debug "========================================================================================"
                        
                        # Try to set destination flag directly - if it fails with "not found", wait more
                        local test_response=$(curl -k -s -w "\n%{http_code}" --connect-timeout 5 --max-time 10 -X POST \
                            -H "Content-Type: application/json" -d '{}' -u "${rest_api_user}:${rest_api_password}" \
                            "https://$dst_ip:$rest_api_port/api/v4/odp/nas-volumes/$dst_pool/$dst_dataset/set-destination-flag" 2>/dev/null)
                        local test_http_code=$(echo "$test_response" | tail -n1)
                        local test_body=$(echo "$test_response" | sed '$d')
                        
                        log_debug "create_destination_datasets: Test destination flag call - HTTP: $test_http_code, Body: $test_body"
                        
                        if [[ "$test_http_code" == "200" ]]; then
                            log_info "create_destination_datasets: Destination flag set successfully for $dst_pool/$dst_dataset on server $dst_ip"
                            dataset_ready=true
                            break
                        elif [[ "$test_body" == *"not found"* ]]; then
                            log_debug "create_destination_datasets: Dataset not yet available in ODP, waiting... (${wait_count}s elapsed)"
                        else
                            log_debug "create_destination_datasets: Unexpected response: HTTP $test_http_code, Body: $test_body"
                        fi
                        
                        ((wait_count++))
                        sleep 1
                        
                        if [[ $((wait_count % 10)) -eq 0 ]]; then
                            log_debug "create_destination_datasets: Still waiting for dataset to be ready for destination flag (${wait_count}s elapsed)"
                        fi
                    done
                    
                    if [[ "$dataset_ready" == "false" ]]; then
                        log_error "create_destination_datasets: Timeout waiting for dataset $dst_pool/$dst_dataset to be ready for destination flag (${max_wait}s)"
                        log_error "create_destination_datasets: Final test response: HTTP $test_http_code, Body: $test_body"
                    fi
                else
                    log_debug "create_destination_datasets: Server $dst_ip does not support destination flag feature, skipping"
                fi
            else
                log_error "create_destination_datasets: DESTINATION dataset creation FAILED pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
                failed_destinations+=("$dst_ip/$dst_pool/$dst_dataset")
            fi
            
        else
            # Error checking dataset existence
            log_error "create_destination_datasets: Error checking if DESTINATION dataset exists pool_name='$dst_pool' dataset_name='$dst_dataset' on server='$dst_ip'"
            failed_destinations+=("$dst_ip/$dst_pool/$dst_dataset (connection error)")
        fi
    done
    
    # Report results
    log_info "create_destination_datasets: Created $success_count of $total_count destination datasets"
    
    if [[ ${#failed_destinations[@]} -gt 0 ]]; then
        log_error "create_destination_datasets: Failed destinations: ${failed_destinations[*]}"
        
        # Show detailed error dialog
        local error_details=""
        for failed_dest in "${failed_destinations[@]}"; do
            error_details+="  * $failed_dest\n"
        done
        
        dialog --title "DESTINATION Dataset Creation Failed" --msgbox \
"Failed to create destination datasets!\n\n\
FAILED DESTINATIONS:\n\
$error_details\n\
SUMMARY:\n\
Created: $success_count of $total_count destination datasets\n\
Source dataset: NOT CREATED\n\
PVE storage: NOT CREATED\n\n\
Please check destination server credentials,\n\
network connectivity, and pool availability.\n\n\
Check logs for detailed error information." \
20 80
        
        return 1
    elif [[ $total_count -eq 0 ]]; then
        log_info "create_destination_datasets: No destinations found in JSON"
        return 0
    else
        log_info "create_destination_datasets: All destination datasets created successfully"
        
        # Show success confirmation
        dialog --title "Success" --infobox "All destination datasets created successfully!\n\nCreated: $success_count of $total_count datasets" 8 60
        sleep 2
        
        return 0
    fi
}

# Helper function: Update existing OODP task with destinations
update_oodp_task_destinations() {
    local pool_name="$1"
    local dataset_name="$2"
    local ip_address="$3"
    local destinations_json="$4"
    
    log_info "update_oodp_task_destinations: Updating OODP task for $pool_name/$dataset_name with destinations"
    log_debug "update_oodp_task_destinations: Destinations JSON: $destinations_json"
    
    # First, get the existing task details to preserve other settings
    get_oodp_details "$ip_address" "$pool_name" "$dataset_name"
    local get_result=$?
    
    if [[ $get_result -ne 0 ]]; then
        log_error "update_oodp_task_destinations: Cannot retrieve existing task details"
        return 1
    fi
    
    # Parse destinations JSON into proper array format for API
    local destinations_array="[]"
    if [[ -n "$destinations_json" ]]; then
        destinations_array="$destinations_json"
    fi
    
    log_debug "update_oodp_task_destinations: Using destinations array: $destinations_array"
    
    # Prepare JSON payload for updating the task (only destinations field)
    local json_payload
    json_payload=$(cat <<EOF
{
    "destinations": $destinations_array
}
EOF
)
    
    log_debug "update_oodp_task_destinations: Update JSON payload: $json_payload"
    
    # Make REST API call to update OODP task
    # Task name format should match what's returned by API: "Pool-0/dataset-name"
    local task_name="${pool_name}/${dataset_name}"
    local url="https://$ip_address:$rest_api_port/api/v4/odp/tasks/$task_name"
    log_debug "update_oodp_task_destinations: Making PUT request to: $url"
    
    # Log the full curl command for debugging
    log_debug "=============== FULL CURL COMMAND FOR UPDATE OODP TASK ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X PUT -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' -d '$json_payload' '$url'"
    log_debug "======================================================================"
    
    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X PUT \
        -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d "$json_payload" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "update_oodp_task_destinations: Failed to connect to OODP API at $ip_address:$rest_api_port"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    log_debug "update_oodp_task_destinations: HTTP status code: $http_code"
    log_debug "update_oodp_task_destinations: Response body: $response_body"
    
    # Check if task was updated successfully
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        log_info "update_oodp_task_destinations: Successfully updated OODP task for $pool_name/$dataset_name"
        return 0
    else
        log_error "update_oodp_task_destinations: Failed to update OODP task. HTTP status: $http_code"
        log_error "update_oodp_task_destinations: Response: $response_body"
        return 1
    fi
}


# Function: get_remote_pools - Get list of available pools from destination server
get_remote_pools() {
    local dst_ip="$1"
    local dst_port="$2"
    
    log_debug "get_remote_pools: Getting pools from $dst_ip:$dst_port"
    
    local url="https://$dst_ip:$rest_api_port/api/v4/pools"
    local response
    response=$(curl -k -s -w "\n%{http_code}" -X GET \
        -u "${rest_api_user}:${rest_api_password}" \
        "$url" 2>/dev/null)
    local curl_exit=$?
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_remote_pools: Failed to connect to API at $dst_ip:$dst_port"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_code=$(echo "$response" | tail -n1)
    local response_body=$(echo "$response" | head -n -1)
    
    if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
        echo "$response_body"
        return 0
    else
        log_error "get_remote_pools: Failed to get pools. HTTP status: $http_code"
        log_debug "get_remote_pools: Response: $response_body"
        return 1
    fi
}

# Function: verify_destination_pool_exists - Verify that destination pool exists
verify_destination_pool_exists() {
    local dst_ip="$1"
    local dst_port="$2" 
    local dst_pool="$3"
    
    log_debug "verify_destination_pool_exists: Verifying pool '$dst_pool' exists on $dst_ip:$dst_port"
    
    local pools_response
    pools_response=$(get_remote_pools "$dst_ip" "$dst_port")
    local get_result=$?
    
    if [[ $get_result -ne 0 ]]; then
        log_error "verify_destination_pool_exists: Cannot retrieve pools from $dst_ip:$dst_port"
        return 1
    fi
    
    # Parse JSON response to check if pool exists
    if echo "$pools_response" | grep -q "\"name\"[[:space:]]*:[[:space:]]*\"$dst_pool\""; then
        log_debug "verify_destination_pool_exists: Pool '$dst_pool' found on $dst_ip:$dst_port"
        return 0
    else
        log_error "verify_destination_pool_exists: Pool '$dst_pool' not found on $dst_ip:$dst_port"
        log_debug "verify_destination_pool_exists: Available pools response: $pools_response"
        
        # Extract available pool names for error message
        local available_pools
        available_pools=$(echo "$pools_response" | grep -o '"name"[[:space:]]*:[[:space:]]*"[^"]*"' | cut -d'"' -f4 | tr '\n' ', ' | sed 's/,$//')
        
        if [[ -n "$available_pools" ]]; then
            log_error "verify_destination_pool_exists: Available pools on $dst_ip: $available_pools"
        fi
        
        return 1
    fi
}

# DELETE CLONED NFS STORAGE FUNCTIONS

# Function: get_server_ip_address_of_given_nfs_storage
get_server_ip_address_of_given_nfs_storage() {
    awk -v name="$1" '
    $1 == "nfs:" && $2 == name { in_block = 1; next }
    in_block && $1 == "server" { print $2; exit }
    /^nfs:/ && $2 != name { in_block = 0 }
    ' /etc/pve/storage.cfg
}

# Function: get_pve_NFS_storage
get_pve_NFS_storage() {
    local storage_cfg="/etc/pve/storage.cfg"

    if [[ ! -f "$storage_cfg" ]]; then
        log_error "get_nfs_server_ip_of_given_storage: $storage_cfg not found"
        return 1
    fi

    # Parse storage.cfg to extract NFS storage names
    awk '
    /^nfs:/ {
        # Extract storage name after "nfs: "
        storage_name = $2
        print storage_name
    }
    ' "$storage_cfg"
}

# Function: auto_select_nfs_ip
auto_select_nfs_ip() {
    local storage_name="$1"  # Optional: specific storage name to match
    local available_ips=()
    
    log_debug "auto_select_nfs_ip: Called with storage_name='$storage_name'"
    log_debug "auto_select_nfs_ip: Current selected_nfs_ip='$selected_nfs_ip'"
    
    # Get all unique NFS server IPs from storage config
    while IFS= read -r ip; do
        [[ -n "$ip" ]] && available_ips+=("$ip")
    done < <(grep "server" /etc/pve/storage.cfg | awk '{print $2}' | sort -u)
    
    log_debug "auto_select_nfs_ip: Found ${#available_ips[@]} IP(s): ${available_ips[*]}"
    
    if [[ ${#available_ips[@]} -eq 0 ]]; then
        log_error "auto_select_nfs_ip: No NFS server IPs found in storage config"
        return 1
    fi
    
    # If only one IP, use it
    if [[ ${#available_ips[@]} -eq 1 ]]; then
        selected_nfs_ip="${available_ips[0]}"
        log_info "auto_select_nfs_ip: Using single NFS IP: $selected_nfs_ip"
        return 0
    fi
    
    # If specific storage name provided, try to match it first
    if [[ -n "$storage_name" ]]; then
        local storage_ip
        storage_ip=$(get_server_ip_address_of_given_nfs_storage "$storage_name")
        if [[ -n "$storage_ip" ]]; then
            # Test if this IP works
            local temp_selected_nfs_ip="$selected_nfs_ip"
            selected_nfs_ip="$storage_ip"
            if rest_api_connection_test >/dev/null 2>&1; then
                log_info "auto_select_nfs_ip: Using storage-specific IP: $selected_nfs_ip"
                return 0
            else
                selected_nfs_ip="$temp_selected_nfs_ip"
                log_debug "auto_select_nfs_ip: Storage-specific IP $storage_ip failed connection test"
            fi
        fi
    fi
    
    # Try each IP sequentially
    for ip in "${available_ips[@]}"; do
        log_debug "auto_select_nfs_ip: Testing IP: $ip"
        local temp_selected_nfs_ip="$selected_nfs_ip"
        selected_nfs_ip="$ip"
        if rest_api_connection_test >/dev/null 2>&1; then
            log_info "auto_select_nfs_ip: Successfully connected to IP: $selected_nfs_ip"
            return 0
        else
            selected_nfs_ip="$temp_selected_nfs_ip"
            log_debug "auto_select_nfs_ip: IP $ip failed connection test"
        fi
    done
    
    # All IPs failed, ask user to select
    log_info "auto_select_nfs_ip: All IPs failed, prompting user for selection"
    
    # Reuse existing IP selection logic
    local line ips=() menu_items=()
    for ip in "${available_ips[@]}"; do
        ips+=("$ip")
    done
    ips+=("Enter new IP")
    
    local i
    for ((i=0; i<${#ips[@]}; i++)); do
        menu_items+=("$i" "${ips[i]}")
    done
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "NFS Server Selection"
         --ok-label "Select" --cancel-label "Cancel"
         --menu "Multiple NFS servers found. Choose one:" 15 60 8)
    local options=("${menu_items[@]}")
    
    local selected_option dialog_exit_code
    local temp_selection=$(mktemp)
    dialog "${cmd[@]}" "${options[@]}" 2>"$temp_selection"
    dialog_exit_code=$?
    selected_option=$(< "$temp_selection")
    rm -f "$temp_selection"
    
    case $dialog_exit_code in
        0)  # User selected an option
            if [[ "${ips[selected_option]}" == "Enter new IP" ]]; then
                # Get custom IP input
                local input_ip
                local temp_input=$(mktemp)
                dialog --keep-tite --title "Enter New IP" --inputbox "Enter the NFS Storage IP address:" 8 40 2>"$temp_input"
                local input_exit_code=$?
                input_ip=$(< "$temp_input")
                rm -f "$temp_input"
                if [[ $input_exit_code -eq 0 && -n "$input_ip" ]]; then
                    # Enhanced validation with REST API connectivity testing
                    local validation_error=""
                    log_debug "auto_select_nfs_ip: Starting enhanced validation for $input_ip"
                    
                    # Show progress dialog while testing connectivity
                    dialog --infobox "Testing connection to $PRODUCT at $input_ip...\nPlease wait (up to 10 seconds)" 5 60 &
                    local progress_pid=$!
                    
                    validate_ip_with_connectivity "$input_ip" validation_error
                    local validation_result=$?
                    
                    # Kill progress dialog
                    kill $progress_pid 2>/dev/null || true
                    wait $progress_pid 2>/dev/null || true
                    
                    if [[ $validation_result -eq 0 ]]; then
                        selected_nfs_ip="$input_ip"
                        log_info "auto_select_nfs_ip: User provided IP: $selected_nfs_ip (validated successfully)"
                        return 0
                    else
                        log_error "auto_select_nfs_ip: Validation failed for $input_ip, result: $validation_result, error: $validation_error"
                        dialog --msgbox "$validation_error" 12 70
                        return 1
                    fi
                else
                    log_error "auto_select_nfs_ip: User cancelled IP input"
                    return 1
                fi
            else
                selected_nfs_ip="${ips[selected_option]}"
                log_info "auto_select_nfs_ip: User selected IP: $selected_nfs_ip"
                return 0
            fi
            ;;
        1)  # User cancelled
            log_error "auto_select_nfs_ip: User cancelled IP selection"
            return 1
            ;;
    esac
    
    return 1
}

# Function: ensure_nfs_ip_selected
ensure_nfs_ip_selected() {
    local storage_name="$1"  # Optional: specific storage name
    
    log_debug "ensure_nfs_ip_selected: Called with storage_name='$storage_name'"
    log_debug "ensure_nfs_ip_selected: Current selected_nfs_ip='$selected_nfs_ip'"
    
    # If IP is already selected, just return success
    if [[ -n "$selected_nfs_ip" ]]; then
        log_debug "ensure_nfs_ip_selected: IP already selected, returning success"
        return 0
    fi
    
    log_debug "ensure_nfs_ip_selected: No IP selected, trying auto-select"
    
    # Try to auto-select IP
    if auto_select_nfs_ip "$storage_name"; then
        log_debug "ensure_nfs_ip_selected: auto_select_nfs_ip succeeded, selected_nfs_ip='$selected_nfs_ip'"
        return 0
    else
        log_error "ensure_nfs_ip_selected: Failed to select NFS IP"
        return 1
    fi
}

# Function: get_pools
get_pools() {
    local nfs_ip="$1"
    
    log_debug "get_pools: Starting function"
    log_debug "get_pools: nfs_ip='$nfs_ip'"
    log_debug "get_pools: rest_api_user='$rest_api_user'"
    log_debug "get_pools: rest_api_port='$rest_api_port'"
    
    # If no IP provided, try to use global selected_nfs_ip
    if [[ -z "$nfs_ip" ]]; then
        if ! ensure_nfs_ip_selected; then
            log_error "get_pools: ensure_nfs_ip_selected failed"
            log_error "get_pools: Failed to select NFS IP"
            return 1
        fi
        nfs_ip="$selected_nfs_ip"
        log_debug "get_pools: Using global selected_nfs_ip='$nfs_ip'"
    fi
    
    # Check connection state first
    if ! check_ip_connection_state "$nfs_ip"; then
        log_info "Skipping get_pools for disconnected IP: $nfs_ip"
        return 1
    fi

    # Make REST API call to get all pools
    local response
    local url="https://$nfs_ip:$rest_api_port/api/v4/pools"
    log_debug "get_pools: Making request to: $url"
    
    # Log the FULL curl command for manual testing
    log_debug "=============== FULL CURL COMMAND FOR GET POOLS ==============="
    log_debug "curl -k -s --connect-timeout 5 --max-time 30 -w '\\n%{http_code}' -X GET -u '${rest_api_user}:${rest_api_password}' -H 'Content-Type: application/json' '$url'"
    log_debug "==============================================================="
    
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X GET -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        "$url" 2>/dev/null)

    local curl_exit=$?
    log_debug "get_pools: curl exit code: $curl_exit"
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_pools: curl failed with exit code $curl_exit"
        log_error "get_pools: Failed to connect to server"
        return 1
    fi

    # Check if response is empty or invalid
    if [[ -z "$response" ]]; then
        log_error "get_pools: Empty response from server"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    log_debug "get_pools: HTTP status: $http_status"
    log_debug "get_pools: Response body: ${body:0:200}..."
    
    # Check for authentication failure
    log_debug "get_pools: About to check authentication failure with status '$http_status'"
    check_rest_api_auth_failure "$http_status" "$body" "get_pools"
    local auth_check_result=$?
    log_debug "get_pools: check_rest_api_auth_failure returned: $auth_check_result"
    
    if [[ $auth_check_result -eq 10 ]]; then
        log_debug "get_pools: Authentication failure (401), returning special code 10"
        return 10
    elif [[ $auth_check_result -eq 11 ]]; then
        log_debug "get_pools: Access forbidden (403), returning special code 11"
        return 11
    elif [[ $auth_check_result -ne 0 ]]; then
        log_debug "get_pools: Authentication check failed with code $auth_check_result, returning error"
        return 1
    fi
    
    log_debug "get_pools: Authentication check passed, continuing"
    
    # Check for successful response
    if [[ "$http_status" != "200" ]]; then
        log_error "get_pools: REST API call failed with HTTP status $http_status"
        return 1
    fi
    
    # Parse JSON response to extract pool names
    printf '%s' "$body" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get('data', [])
    for pool in pools:
        name = pool.get('name')
        if name:
            print(name)
except json.JSONDecodeError as e:
    print(f'get_pools: JSON decode error: {e}', file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f'get_pools: Error: {e}', file=sys.stderr)
    sys.exit(1)
"

    local python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        log_error "get_pools: Failed to parse JSON response"
        return 1
    fi
}

get_nfs_server_ip_of_given_storage() {
    local storage="$1"
    awk -v storage="$storage" '
        # When we hit an nfs: line, check if its name matches
        /^nfs:[[:space:]]+/ {
            in_block = ($2 == storage)
        }
        # While in the right block, look for a “server” line
        in_block && /^[[:space:]]*server[[:space:]]+/ {
            print $2     # print the IP
            exit         # and quit immediately
        }
    ' /etc/pve/storage.cfg
}


# Function: get_dataset_pool_name
get_dataset_pool_name() {
    local dataset_name="$1"
    local nfs_ip="$2"

    if [[ -z "$dataset_name" ]]; then
        log_error "get_dataset_pool_name: dataset_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    log_debug "get_dataset_pool_name: Looking for dataset '$dataset_name' on IP '$nfs_ip'"

    # Get all pools
    log_debug "get_dataset_pool_name: About to call get_pools for IP '$nfs_ip'"
    local pools
    pools=$(get_pools "$nfs_ip")
    local get_pools_exit=$?
    log_debug "get_dataset_pool_name: get_pools returned exit code: $get_pools_exit"
    
    if [[ $get_pools_exit -eq 10 ]]; then
        log_debug "get_dataset_pool_name: get_pools returned authentication failure (401), propagating code 10"
        return 10
    elif [[ $get_pools_exit -eq 11 ]]; then
        log_debug "get_dataset_pool_name: get_pools returned access forbidden (403), propagating code 11"
        return 11
    elif [[ $get_pools_exit -ne 0 ]]; then
        log_error "get_dataset_pool_name: Failed to get pools list (exit code: $get_pools_exit)"
        log_debug "get_dataset_pool_name: get_pools failed, returning error immediately"
        return 1
    fi
    
    log_debug "get_dataset_pool_name: get_pools succeeded, pools data: '${pools:0:100}...'"

    # Loop through each pool and check if dataset exists
    while IFS= read -r pool_name; do
        if [[ -z "$pool_name" ]]; then
            continue
        fi

        log_debug "get_dataset_pool_name: Checking pool '$pool_name' for dataset '$dataset_name'"

        # Get nas-volumes for this pool
        local response
        response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X GET -u "${rest_api_user}:${rest_api_password}" \
            -H 'Content-Type: application/json' \
            "https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes" 2>/dev/null)

        local curl_exit=$?
        if [[ $curl_exit -ne 0 ]]; then
            log_debug "get_dataset_pool_name: curl failed for pool '$pool_name', skipping"
            continue  # Skip this pool if we can't access it
        fi
        
        # Extract HTTP status code and response body
        local http_status=$(echo "$response" | tail -n1)
        local body=$(echo "$response" | sed '$d')
        
        log_debug "get_dataset_pool_name: Pool '$pool_name' - HTTP status: $http_status"
        
        # Check for authentication failure (but don't show dialog for each pool)
        if [[ "$http_status" == "401" ]]; then
            log_error "get_dataset_pool_name: REST API authentication failed (HTTP 401)"
            # For this function, we'll return error without showing dialog to avoid multiple popups
            return 1
        elif [[ "$http_status" != "200" ]]; then
            log_debug "get_dataset_pool_name: Non-200 status for pool '$pool_name': $http_status"
            continue  # Skip this pool if request failed
        fi

        # Debug: Show available datasets in this pool
        local available_datasets
        available_datasets=$(printf '%s' "$body" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    names = [d.get('name', 'UNKNOWN') for d in entries]
    print(' '.join(names))
except Exception as e:
    print('ERROR_PARSING')
    sys.exit(1)
" 2>/dev/null)
        log_debug "get_dataset_pool_name: Pool '$pool_name' contains datasets: $available_datasets"

        # Check if dataset exists in this pool
        local dataset_found
        dataset_found=$(printf '%s' "$body" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            print('FOUND')
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
")

        log_debug "get_dataset_pool_name: Dataset search result in pool '$pool_name': $dataset_found"

        if [[ "$dataset_found" == "FOUND" ]]; then
            log_debug "get_dataset_pool_name: Found dataset '$dataset_name' in pool '$pool_name'"
            echo "$pool_name"
            return 0
        fi
    done <<< "$pools"

    # Dataset not found in any pool
    log_error "get_dataset_pool_name: Dataset '$dataset_name' not found in any pool. Searched pools: $pools"
    return 1
}

# Function: is_clone
is_clone() {
    local volume_name="$1"
    local nfs_ip="$2"

    if [[ -z "$volume_name" ]]; then
        log_error "is_clone: volume_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    # Parse pool and dataset from volume_name
    local pool_name="${volume_name%%/*}"
    local dataset_name="${volume_name#*/}"

    if [[ -z "$pool_name" || -z "$dataset_name" || "$pool_name" == "$dataset_name" ]]; then
        log_error "is_clone: Invalid volume_name format. Expected: pool_name/dataset_name"
        return 1
    fi

    log_debug "is_clone: Checking if '$volume_name' is a clone on IP '$nfs_ip'"

    # Make REST API call to get nas-volumes for the pool
    local response
    response=$(curl -k -s --connect-timeout 5 --max-time 30 -w "\n%{http_code}" -X GET -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        "https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes" 2>/dev/null)

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "is_clone: Failed to connect to server"
        return 1
    fi
    
    # Extract HTTP status code and response body
    local http_status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    # Check for authentication failure
    check_rest_api_auth_failure "$http_status" "$body" "is_clone"
    local auth_check_result=$?
    if [[ $auth_check_result -ne 0 ]]; then
        return 1
    fi
    
    # Check for successful response
    if [[ "$http_status" != "200" ]]; then
        log_error "is_clone: REST API call failed with HTTP status $http_status"
        return 1
    fi

    # Parse JSON response to find the dataset and check its origin
    local origin
    origin=$(printf '%s' "$body" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            origin = d.get('origin')
            if origin is None:
                print('None')
            else:
                print(origin)
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
")

    local python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        log_error "is_clone: Failed to parse JSON response"
        return 1
    fi

    if [[ "$origin" == "NOT_FOUND" ]]; then
        log_error "is_clone: Dataset '$dataset_name' not found in pool '$pool_name'"
        return 1
    fi

    if [[ "$origin" == "None" ]]; then
        # origin is None, so it's a dataset (not a clone)
        return 1
    else
        # origin is set, so it's a clone
        return 0
    fi
}

# Function: delete_cloned_dataset
# Function to delete a regular dataset (not necessarily a clone)
delete_dataset() {
    local volume_name="$1"
    local nfs_ip="$2"

    if [[ -z "$volume_name" ]]; then
        log_error "delete_dataset: volume_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "delete_dataset: nfs_ip is required"
        return 1
    fi

    log_info "Deleting dataset: $volume_name on IP: $nfs_ip"

    # Extract pool name and dataset name from volume_name (format: pool/dataset)
    local pool_name="${volume_name%/*}"
    local dataset_name="${volume_name##*/}"

    log_info "Pool: '$pool_name', Dataset: '$dataset_name'"

    # Make REST API call to delete the dataset
    local url="https://${nfs_ip}:${rest_api_port}/api/v4/pools/${pool_name}/nas-volumes/${dataset_name}"

    local response
    response=$(curl -k -s -X DELETE \
        --connect-timeout 5 --max-time 120 \
        -u "${rest_api_user}:${rest_api_password}" \
        -w "\n%{http_code}\n%{redirect_url}" \
        "$url" 2>/dev/null)

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "Failed to connect to server for deletion"
        return 1
    fi

    # Parse response
    local response_body=$(echo "$response" | sed -n '1p')
    local http_status=$(echo "$response" | sed -n '2p')
    local redirect_url=$(echo "$response" | sed -n '3p')

    log_debug "delete_dataset: HTTP Status: $http_status"
    log_debug "delete_dataset: Response: $response_body"

    # Check for authentication failure
    if check_rest_api_auth_failure "delete_dataset" "$http_status" "$response_body"; then
        local auth_result=$?
        if [[ $auth_result -eq 10 ]]; then
            log_error "delete_dataset: REST API authentication failed (401)"
            return 10
        elif [[ $auth_result -eq 11 ]]; then
            log_error "delete_dataset: REST API access forbidden (403)"
            return 11
        fi
    fi

    # Check if deletion was successful
    if [[ "$http_status" == "200" ]] || [[ "$http_status" == "204" ]]; then
        log_info "Dataset '$volume_name' deleted successfully"
        return 0
    else
        log_error "Failed to delete dataset '$volume_name'. HTTP status: $http_status, Response: $response_body"
        return 1
    fi
}

delete_cloned_dataset() {
    local dataset_name="$1"
    local nfs_ip="$2"

    if [[ -z "$dataset_name" ]]; then
        log_error "get_dataset_pool_name: dataset_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    log_info "Checking dataset: $dataset_name on IP: $nfs_ip"

    # Find which pool this dataset belongs to
    local pool_name
    pool_name=$(get_dataset_pool_name "$dataset_name" "$nfs_ip")
    if [[ $? -ne 0 ]]; then
        # echo "Error: Dataset '$dataset_name' not found in any pool" >&2
        return 1
    fi

    log_info "Dataset '$dataset_name' found in pool '$pool_name'"

    # Check if this dataset is a clone
    local volume_name="$pool_name/$dataset_name"
    if ! is_clone "$volume_name" "$nfs_ip"; then
        log_error "Dataset '$dataset_name' is not a clone. Refusing to delete original dataset."
        return 1
    fi

    log_info "Confirmed: Dataset '$dataset_name' is a clone. Proceeding with deletion..."

    # Make REST API call to delete the dataset
    local url="https://${nfs_ip}:${rest_api_port}/api/v4/pools/${pool_name}/nas-volumes/${dataset_name}"

    local response
    response=$(curl -k -s -X DELETE \
        --connect-timeout 5 --max-time 120 \
        -u "${rest_api_user}:${rest_api_password}" \
        -H 'Content-Type: application/json' \
        -d '{}' \
        -w "\n%{http_code}\n%{redirect_url}" \
        "$url" 2>/dev/null)

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "Failed to connect to server for deletion"
        return 1
    fi

    # Parse curl response (body, http_code, redirect_url)
    local response_lines
    readarray -t response_lines <<< "$response"
    local response_body="${response_lines[0]}"
    local http_code="${response_lines[1]}"
    local redirect_url="${response_lines[2]}"
    
    # Check for authentication failure
    check_rest_api_auth_failure "$http_code" "$response_body" "delete_cloned_dataset"
    local auth_check_result=$?
    if [[ $auth_check_result -ne 0 ]]; then
        return 1
    fi

    if [[ "$http_code" == "204" ]]; then
        log_info "Success: Dataset '$dataset_name' has been deleted"
        return 0
    else
        log_error "Failed to delete dataset '$dataset_name'. HTTP code: $http_code"
        if [[ -n "$response_body" ]]; then
            log_error "Response: $response_body"
        fi
        return 1
    fi
}

# Function: get_cloned_storage_list
get_cloned_storage_list() {
    # Check if we have NFS IP configured

    # Get all NFS storages from PVE
    local nfs_storages
    nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        log_error "Failed to get NFS storages from PVE"
        return 1
    fi

    if [[ -z "$nfs_storages" ]]; then
        # No NFS storages found, return successfully but with no output
        return 0
    fi

    # Loop through each NFS storage and check if it's a clone
    while IFS= read -r storage_name; do
        if [[ -z "$storage_name" ]]; then
            continue
        fi

        # Get the IP address for this storage
        local storage_ip
        storage_ip=$(get_storage_ip "$storage_name")
        if [[ $? -ne 0 ]]; then
            log_error "get_cloned_storage_list: No IP mapping found for storage '$storage_name'"
            continue
        fi

        # Find which pool this storage belongs to (using cached data if available)
        local pool_name
        if [[ "$API_CACHE_INITIALIZED" == "true" ]]; then
            pool_name=$(get_dataset_pool_name_cached "$storage_name" "$storage_ip" 2>/dev/null)
        else
            pool_name=$(get_dataset_pool_name "$storage_name" "$storage_ip")
        fi
        
        if [[ $? -ne 0 ]]; then
            # Storage not found in any pool, skip it
            continue
        fi

        # Check if this storage is a clone (using cached data if available)
        local volume_name="$pool_name/$storage_name"
        if [[ "$API_CACHE_INITIALIZED" == "true" ]]; then
            if is_clone_cached "$volume_name" "$storage_ip"; then
                echo "$storage_name"
            fi
        else
            if is_clone "$volume_name" "$storage_ip"; then
                echo "$storage_name"
            fi
        fi

    done <<< "$nfs_storages"
}

# Function: delete_pve_NFS_storage
delete_pve_NFS_storage() {
    local pve_storage_name="$1"

    if [[ -z "$pve_storage_name" ]]; then
        log_error "delete_pve_NFS_storage: pve_storage_name is required"
        return 1
    fi

    log_info "Processing PVE NFS storage: $pve_storage_name"

    # Step 1: Check if given storage is a PVE NFS storage
    local pve_nfs_storages
    pve_nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        log_error "delete_pve_NFS_storage: Failed to get PVE NFS storages"
        return 1
    fi

    local is_pve_nfs_storage=false
    while IFS= read -r storage; do
        if [[ "$storage" == "$pve_storage_name" ]]; then
            is_pve_nfs_storage=true
            break
        fi
    done <<< "$pve_nfs_storages"

    if [[ "$is_pve_nfs_storage" == "false" ]]; then
        log_error "'$pve_storage_name' is not a PVE NFS storage"
        return 1
    fi

    log_info "Confirmed: '$pve_storage_name' is a PVE NFS storage"

    # Step 2: Get the IP address for this storage
    get_storage_ip "$pve_storage_name" >/dev/null
    if [[ $? -ne 0 ]]; then
        log_error "delete_pve_NFS_storage: No IP mapping found for storage '$pve_storage_name'"
        return 1
    fi
    local storage_ip="$selected_nfs_ip"

    log_info "Storage '$pve_storage_name' mapped to IP '$storage_ip'"

    # Step 3: Find which pool this storage belongs to
    local pool_name
    pool_name=$(get_dataset_pool_name "$pve_storage_name" "$storage_ip")
    if [[ $? -ne 0 ]]; then
        # echo "Error: Storage '$pve_storage_name' not found in any pool" >&2
        return 1
    fi

    log_info "Storage '$pve_storage_name' found in pool '$pool_name'"

    # Step 4: Check if this storage is a clone dataset
    local volume_name="$pool_name/$pve_storage_name"
    if ! is_clone "$volume_name" "$storage_ip"; then
        log_error "Storage '$pve_storage_name' is not a clone dataset. Refusing to delete original dataset."
        return 1
    fi

    log_info "Confirmed: Storage '$pve_storage_name' is a clone dataset"

    # Step 5: Delete the cloned dataset via REST API
    log_info "Deleting cloned dataset..."
    if ! delete_cloned_dataset "$pve_storage_name" "$storage_ip"; then
        log_error "Failed to delete cloned dataset '$pve_storage_name'"
        return 1
    fi

    log_info "Successfully deleted cloned dataset '$pve_storage_name'"

    # Step 6: Remove PVE storage configuration
    log_info "Removing PVE storage configuration..."
    if ! pvesm remove "$pve_storage_name"; then
        log_error "Failed to remove PVE storage '$pve_storage_name'"
        return 1
    fi

    log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

    # Step 7: Run cleanup of inactive NFS storage
    log_info "Running cleanup of inactive NFS storage..."
    if ! cleanup_inactive_nfs_storage; then
        log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
    else
        log_info "NFS storage cleanup complete"
    fi

    log_info "Storage '$pve_storage_name' deleted"
    return 0
}

# Function: delete_unused_pve_nfs_storage - Delete unused NFS storage (not necessarily cloned)
delete_unused_pve_nfs_storage() {
    local pve_storage_name="$1"

    if [[ -z "$pve_storage_name" ]]; then
        log_error "delete_unused_pve_nfs_storage: pve_storage_name is required"
        return 1
    fi

    log_info "Processing unused PVE NFS storage: $pve_storage_name"

    # Step 1: Check if given storage is a PVE NFS storage
    local pve_nfs_storages
    pve_nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        log_error "delete_unused_pve_nfs_storage: Failed to get PVE NFS storages"
        return 1
    fi

    local is_pve_nfs_storage=false
    while IFS= read -r storage; do
        if [[ "$storage" == "$pve_storage_name" ]]; then
            is_pve_nfs_storage=true
            break
        fi
    done <<< "$pve_nfs_storages"

    if [[ "$is_pve_nfs_storage" == "false" ]]; then
        log_error "'$pve_storage_name' is not a PVE NFS storage"
        return 1
    fi

    log_info "Confirmed: '$pve_storage_name' is a PVE NFS storage"

    # Step 2: Get the IP address for this storage
    get_storage_ip "$pve_storage_name" >/dev/null
    if [[ $? -ne 0 ]]; then
        log_error "delete_unused_pve_nfs_storage: No IP mapping found for storage '$pve_storage_name'"
        return 1
    fi
    local storage_ip="$selected_nfs_ip"

    log_info "Storage '$pve_storage_name' mapped to IP '$storage_ip'"

    # Step 3: Get export path from PVE storage configuration
    local export_path
    export_path=$(get_pve_storage_export_path "$pve_storage_name")
    local export_result=$?
    
    if [[ $export_result -ne 0 ]]; then
        log_error "delete_unused_pve_nfs_storage: Could not get export path for storage '$pve_storage_name'"
        log_info "This might be a storage configuration that exists in PVE but the dataset was already deleted from JovianDSS"
        log_info "Proceeding to remove only the PVE storage configuration"
        
        # Skip dataset deletion, only remove PVE storage configuration
        log_info "Removing PVE storage configuration..."
        if ! pvesm remove "$pve_storage_name"; then
            log_error "Failed to remove PVE storage '$pve_storage_name'"
            return 1
        fi

        log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

        # Run cleanup of inactive NFS storage
        log_info "Running cleanup of inactive NFS storage..."
        if ! cleanup_inactive_nfs_storage; then
            log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
        else
            log_info "NFS storage cleanup complete"
        fi

        log_info "Unused storage '$pve_storage_name' (PVE-only) deleted successfully"
        return 0
    fi

    log_info "Storage '$pve_storage_name' has export path '$export_path'"

    # Step 4: Convert export path to share name (remove leading /)
    local share_name="${export_path#/}"
    log_info "Looking for share name '$share_name'"

    # Step 5: Get dataset path from share using REST API
    local dataset_path
    dataset_path=$(get_dataset_from_share_name "$share_name" "$storage_ip")
    local dataset_result=$?
    
    if [[ $dataset_result -ne 0 ]]; then
        log_info "Could not find dataset via NFS shares API (share may be inactive)"
        log_info "Attempting direct dataset lookup by constructing path: Pool-0/$share_name"
        
        # Fallback: Try direct dataset path construction and verification
        local constructed_path="Pool-0/$share_name"
        
        # Verify the constructed path exists by querying nas-volumes directly
        local url="https://${storage_ip}:${rest_api_port}/api/v4/pools/Pool-0/nas-volumes"
        local temp_response=$(mktemp)
        local temp_status=$(mktemp)
        
        curl -k -s -X GET \
            -u "${rest_api_user}:${rest_api_password}" \
            -w '%{http_code}' \
            "$url" > "$temp_response" 2>/dev/null
        
        local http_status=$(tail -n1 "$temp_response")
        sed -i '$ d' "$temp_response"  # Remove status line from response
        local response_body=$(cat "$temp_response")
        rm -f "$temp_response" "$temp_status"
        
        if [[ "$http_status" == "200" ]]; then
            # Check if our dataset exists in the nas-volumes list
            if echo "$response_body" | grep -q "\"name\":\s*\"$share_name\""; then
                dataset_path="$constructed_path"
                log_info "Found dataset '$dataset_path' via direct nas-volumes lookup"
            else
                log_error "Dataset '$constructed_path' not found in nas-volumes either"
                log_info "Dataset was likely already deleted from JovianDSS"
                log_info "Proceeding to remove only the PVE storage configuration"
                
                # Skip dataset deletion, only remove PVE storage configuration
                log_info "Removing PVE storage configuration..."
                if ! pvesm remove "$pve_storage_name"; then
                    log_error "Failed to remove PVE storage '$pve_storage_name'"
                    return 1
                fi

                log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

                # Run cleanup of inactive NFS storage
                log_info "Running cleanup of inactive NFS storage..."
                if ! cleanup_inactive_nfs_storage; then
                    log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
                else
                    log_info "NFS storage cleanup complete"
                fi

                log_info "Unused storage '$pve_storage_name' (PVE-only) deleted successfully"
                return 0
            fi
        else
            log_error "Failed to query nas-volumes. HTTP status: $http_status"
            log_info "Cannot verify dataset existence, proceeding to remove only PVE storage configuration"
            
            # Skip dataset deletion, only remove PVE storage configuration
            log_info "Removing PVE storage configuration..."
            if ! pvesm remove "$pve_storage_name"; then
                log_error "Failed to remove PVE storage '$pve_storage_name'"
                return 1
            fi

            log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

            # Run cleanup of inactive NFS storage
            log_info "Running cleanup of inactive NFS storage..."
            if ! cleanup_inactive_nfs_storage; then
                log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
            else
                log_info "NFS storage cleanup complete"
            fi

            log_info "Unused storage '$pve_storage_name' (PVE-only) deleted successfully"
            return 0
        fi
    fi

    log_info "Storage '$pve_storage_name' corresponds to dataset path '$dataset_path'"

    # Step 6: Check if this is a clone or regular dataset
    local is_clone_dataset=false
    
    if is_clone "$dataset_path" "$storage_ip"; then
        is_clone_dataset=true
        log_info "Storage '$pve_storage_name' is a clone dataset - will delete cloned dataset"
    else
        log_info "Storage '$pve_storage_name' is a regular dataset - will delete dataset"
    fi

    # Step 7: Remove PVE storage configuration FIRST (to unmount and free the dataset)
    log_info "Removing PVE storage configuration first to free dataset..."
    if ! pvesm remove "$pve_storage_name"; then
        log_error "Failed to remove PVE storage '$pve_storage_name'"
        return 1
    fi
    log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

    # Step 8: Now delete the dataset (clone or regular) via REST API
    log_info "Deleting dataset from JovianDSS..."
    
    if [[ "$is_clone_dataset" == "true" ]]; then
        # Delete as cloned dataset - need to extract dataset name from path for this function
        local dataset_name="${dataset_path##*/}"
        if ! delete_cloned_dataset "$dataset_name" "$storage_ip"; then
            log_error "Failed to delete cloned dataset '$dataset_name'"
            return 1
        fi
        log_info "Successfully deleted cloned dataset '$dataset_name'"
    else
        # Delete as regular dataset  
        if ! delete_dataset "$dataset_path" "$storage_ip"; then
            log_error "Failed to delete dataset '$dataset_path'"
            return 1
        fi
        log_info "Successfully deleted dataset '$dataset_path'"
    fi

    # Step 9: Run cleanup of inactive NFS storage
    log_info "Running cleanup of inactive NFS storage..."
    if ! cleanup_inactive_nfs_storage; then
        log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
    else
        log_info "NFS storage cleanup complete"
    fi

    log_info "Unused storage '$pve_storage_name' deleted successfully"
    return 0
}

# Function: get_cloned_storage_list_with_ips
get_cloned_storage_list_with_ips() {
    log_debug "get_cloned_storage_list_with_ips: Function started"
    
    # Get all NFS storages from PVE
    log_debug "get_cloned_storage_list_with_ips: About to call get_pve_NFS_storage"
    local nfs_storages
    nfs_storages=$(get_pve_NFS_storage)
    local get_pve_exit=$?
    log_debug "get_cloned_storage_list_with_ips: get_pve_NFS_storage returned with exit code: $get_pve_exit"
    
    if [[ $get_pve_exit -ne 0 ]]; then
        log_error "get_cloned_storage_list_with_ips: Failed to get NFS storages from PVE"
        return 1
    fi

    if [[ -z "$nfs_storages" ]]; then
        log_debug "get_cloned_storage_list_with_ips: No NFS storages found, returning successfully"
        # No NFS storages found, return successfully but with no output
        return 0
    fi
    
    log_debug "get_cloned_storage_list_with_ips: Found NFS storages: $nfs_storages"

    # Check each storage and build list with IPs
    log_debug "get_cloned_storage_list_with_ips: Starting storage processing loop"
    local cloned_storages_with_ips=()
    local storage_count=0
    while IFS= read -r storage_name; do
        ((storage_count++))
        log_debug "get_cloned_storage_list_with_ips: Processing storage #$storage_count: '$storage_name'"
        
        if [[ -z "$storage_name" ]]; then
            log_debug "get_cloned_storage_list_with_ips: Skipping empty storage name"
            continue
        fi

        # Get NFS IP for this storage
        log_debug "get_cloned_storage_list_with_ips: Getting IP for storage '$storage_name'"
        local nfs_ip
        nfs_ip=$(get_storage_ip "$storage_name")
        local get_ip_exit=$?
        log_debug "get_cloned_storage_list_with_ips: get_storage_ip returned exit code: $get_ip_exit, IP: '$nfs_ip'"
        
        if [[ $get_ip_exit -ne 0 ]]; then
            log_error "get_cloned_storage_list_with_ips: No IP mapping found for storage '$storage_name'"
            continue
        fi

        # Find which pool this storage belongs to
        log_debug "get_cloned_storage_list_with_ips: About to call get_dataset_pool_name for '$storage_name' on IP '$nfs_ip'"
        local pool_name
        pool_name=$(get_dataset_pool_name "$storage_name" "$nfs_ip" 2>&1)
        local get_pool_exit=$?
        log_debug "get_cloned_storage_list_with_ips: get_dataset_pool_name returned exit code: $get_pool_exit, pool: '$pool_name'"
        
        if [[ $get_pool_exit -eq 10 ]]; then
            log_debug "get_cloned_storage_list_with_ips: get_dataset_pool_name returned authentication failure (401), propagating code 10"
            return 10
        elif [[ $get_pool_exit -eq 11 ]]; then
            log_debug "get_cloned_storage_list_with_ips: get_dataset_pool_name returned access forbidden (403), propagating code 11"
            return 11
        elif [[ $get_pool_exit -ne 0 ]]; then
            log_debug "get_cloned_storage_list_with_ips: get_dataset_pool_name failed with code $get_pool_exit, skipping storage '$storage_name'"
            # Storage not found in any pool, skip it
            continue
        fi

        # Check if this storage is a clone
        log_debug "get_cloned_storage_list_with_ips: Checking if '$storage_name' is a clone (volume: '$pool_name/$storage_name')"
        local volume_name="$pool_name/$storage_name"
        if is_clone "$volume_name" "$nfs_ip"; then
            log_debug "get_cloned_storage_list_with_ips: '$storage_name' is a clone, adding to list"
            cloned_storages_with_ips+=("$storage_name:$nfs_ip")
        else
            log_debug "get_cloned_storage_list_with_ips: '$storage_name' is not a clone, skipping"
        fi

    done <<< "$nfs_storages"
    
    log_debug "get_cloned_storage_list_with_ips: Finished processing all storages, found ${#cloned_storages_with_ips[@]} cloned storages"

    # Output results
    for item in "${cloned_storages_with_ips[@]}"; do
        log_debug "get_cloned_storage_list_with_ips: Outputting: '$item'"
        echo "$item"
    done
    
    log_debug "get_cloned_storage_list_with_ips: Function completed successfully"
}

# Function: check_vmct_using_storage
check_vmct_using_storage() {
    local storage_name="$1"
    local temp_file=$(mktemp)
    
    log_debug "check_vmct_using_storage: Checking if VMs/CTs are using storage '$storage_name'"
    
    # Check all VM and CT configuration files
    for conf_file in $ALL_PVE_VM_CONF $ALL_PVE_CT_CONF; do
        if [[ -f "$conf_file" ]]; then
            # Get VM/CT ID from filename
            local vmct_id=$(basename "$conf_file" .conf)
            
            # Check if this VM/CT uses the specified storage
            local storage_match=false
            # Use comprehensive device pattern for both LXC and VM device detection
            if grep -P -q "^${DEVICE_PATTERN}:.*${storage_name}:" "$conf_file" 2>/dev/null; then
                storage_match=true
                log_debug "check_vmct_using_storage: Found device match in $conf_file"
                log_debug "check_vmct_using_storage: Found disk match in $conf_file"
            fi
            
            if [[ "$storage_match" == "true" ]]; then
                
                # Determine type (VM or CT)
                local vmct_type="VM"
                if [[ "$conf_file" == */lxc/* ]]; then
                    vmct_type="CT"
                fi
                
                # Get VM/CT name from config (handle both formats: "name: value" and "name=value")
                # For VMs (QEMU), use 'name:' field; for containers (LXC), use 'hostname:' field
                local vmct_name
                vmct_name=$(grep -E "^name[[:space:]]*[:=]" "$conf_file" 2>/dev/null | sed -E 's/^name[[:space:]]*[:=][[:space:]]*//' | head -1)
                if [[ -z "$vmct_name" ]]; then
                    # Try hostname field for LXC containers
                    vmct_name=$(grep -E "^hostname[[:space:]]*[:=]" "$conf_file" 2>/dev/null | sed -E 's/^hostname[[:space:]]*[:=][[:space:]]*//' | head -1)
                fi
                if [[ -z "$vmct_name" ]]; then
                    vmct_name="(no name)"
                fi
                
                # Get tags from config (handle both formats: "tags: value" and "tags=value")
                local tags=$(grep -E "^tags[[:space:]]*[:=]" "$conf_file" 2>/dev/null | sed -E 's/^tags[[:space:]]*[:=][[:space:]]*//' | head -1)
                if [[ -z "$tags" ]]; then
                    tags="(no tags)"
                fi
                
                # Write to temp file in format: ID|TYPE|NAME|TAGS (using pipe delimiter to avoid space issues)
                echo "$vmct_id|$vmct_type|$vmct_name|$tags" >> "$temp_file"
                
                log_debug "check_vmct_using_storage: Found $vmct_type $vmct_id ($vmct_name) using storage '$storage_name'"
            fi
        fi
    done
    
    # Log temp file contents for debugging
    if [[ -s "$temp_file" ]]; then
        log_debug "check_vmct_using_storage: Found usage data in temp file:"
        while IFS= read -r line; do
            log_debug "check_vmct_using_storage: $line"
        done < "$temp_file"
        log_debug "check_vmct_using_storage: Returning temp file path: $temp_file"
        printf '%s\n' "$temp_file"
        return 0
    else
        log_debug "check_vmct_using_storage: No VMs/CTs found using storage '$storage_name'"
        rm -f "$temp_file"
        return 1
    fi
}

# Function: show_vmct_usage_warning
show_vmct_usage_warning() {
    local storage_name="$1"
    local usage_file="$2"
    
    log_debug "show_vmct_usage_warning: Showing warning for storage '$storage_name'"
    log_debug "show_vmct_usage_warning: Usage file path: $usage_file"
    
    # Check if usage file exists and has content
    if [[ ! -f "$usage_file" ]]; then
        log_error "show_vmct_usage_warning: Usage file '$usage_file' does not exist"
        return 1
    fi
    
    if [[ ! -s "$usage_file" ]]; then
        log_error "show_vmct_usage_warning: Usage file '$usage_file' is empty"
        return 1
    fi
    
    log_debug "show_vmct_usage_warning: Usage file contents:"
    while IFS= read -r line; do
        log_debug "show_vmct_usage_warning: File line: '$line'"
    done < "$usage_file"
    
    # Build warning message
    local warning_msg="WARNING: The following VMs/CTs are using storage '$storage_name':\n\n"
    local vm_count=0
    local has_vms=false
    local has_cts=false
    
    while IFS='|' read -r vmct_id vmct_type vmct_name tags; do
        log_debug "show_vmct_usage_warning: Read line: ID='$vmct_id', Type='$vmct_type', Name='$vmct_name', Tags='$tags'"
        
        # Skip empty lines
        if [[ -z "$vmct_id" ]]; then
            log_debug "show_vmct_usage_warning: Skipping empty line"
            continue
        fi
        
        # Track which types we have
        if [[ "$vmct_type" == "vm" ]]; then
            has_vms=true
        elif [[ "$vmct_type" == "ct" ]]; then
            has_cts=true
        fi
        
        warning_msg="${warning_msg}ID: $vmct_id | Type: $vmct_type | Name: $vmct_name | Tags: $tags\n"
        ((vm_count++))
        
        log_debug "show_vmct_usage_warning: Added VM/CT to warning: ID=$vmct_id, Type=$vmct_type, Name=$vmct_name, Tags=$tags"
    done < "$usage_file"
    
    log_debug "show_vmct_usage_warning: Total VMs/CTs processed: $vm_count"
    
    # Build appropriate type description
    local type_desc=""
    if [[ "$has_vms" == true && "$has_cts" == true ]]; then
        type_desc="VMs/CTs"
    elif [[ "$has_vms" == true ]]; then
        type_desc="VMs"
    elif [[ "$has_cts" == true ]]; then
        type_desc="CTs"
    else
        type_desc="VMs/CTs"
    fi
    
    warning_msg="${warning_msg}\nDeleting this storage will make these $type_desc unusable!\n\nDo you want to continue with deletion?"
    
    log_debug "show_vmct_usage_warning: Final warning message:"
    log_debug "show_vmct_usage_warning: $warning_msg"
    
    # Show warning dialog with Yes/No/Back options
    local dialog_result=$(mktemp)
    dialog --keep-tite \
        --title "Storage In Use Warning" \
        --yes-label "Continue Delete" \
        --no-label "Cancel" \
        --extra-button --extra-label "Back" \
        --defaultno \
        --yesno "$warning_msg" 20 80
    
    local exit_code=$?
    rm -f "$dialog_result"
    
    # Return appropriate exit code
    # 0 = Yes (Continue Delete), 1 = No (Cancel), 3 = Extra (Back)
    # With --defaultno, Cancel is highlighted but Continue Delete is still "Yes"
    case $exit_code in
        0) return 0 ;;  # Continue Delete
        1) return 1 ;;  # Cancel
        3) return 3 ;;  # Back
        *) return 1 ;;  # Default to cancel
    esac
}

# Get list of storages currently being used by VMs and CTs
get_storages_used_by_vms_cts() {
    local used_storages=""
    
    log_debug "get_storages_used_by_vms_cts: Scanning VM/CT configurations for storage usage"
    
    # Check VM configurations
    for conf_file in /etc/pve/nodes/*/qemu-server/*.conf; do
        if [[ -f "$conf_file" ]]; then
            # Extract storage names from disk configurations
            # Lines like: scsi0: storage-name:vm-100-disk-0.qcow2,size=32G
            # or: virtio0: storage-name:100/vm-100-disk-0.raw,size=32G
            while IFS= read -r line; do
                if [[ "$line" =~ ^(scsi|virtio|ide|sata|efidisk)[0-9]+:[[:space:]]*([^:]+): ]]; then
                    local storage_name="${BASH_REMATCH[2]}"
                    if [[ -n "$storage_name" && ! "$used_storages" =~ (^|$'\n')"$storage_name"($'\n'|$) ]]; then
                        used_storages+="$storage_name"$'\n'
                    fi
                fi
            done < "$conf_file"
        fi
    done
    
    # Check CT configurations  
    for conf_file in /etc/pve/nodes/*/lxc/*.conf; do
        if [[ -f "$conf_file" ]]; then
            # Extract storage names from rootfs and mp configurations
            # Lines like: rootfs: storage-name:subvol-100-disk-0,size=8G
            # or: mp0: storage-name:100/vm-100-disk-0.raw,mp=/mnt/data
            while IFS= read -r line; do
                if [[ "$line" =~ ^(rootfs|mp[0-9]+):[[:space:]]*([^:,]+) ]]; then
                    local storage_name="${BASH_REMATCH[2]}"
                    if [[ -n "$storage_name" && ! "$used_storages" =~ (^|$'\n')"$storage_name"($'\n'|$) ]]; then
                        used_storages+="$storage_name"$'\n'
                    fi
                fi
            done < "$conf_file"
        fi
    done
    
    # Remove trailing newline and return
    echo -n "$used_storages" | sed '/^$/d'
}

# Get list of unused NFS storages (not used by any VM/CT)
get_unused_nfs_storages() {
    local all_nfs_storages=""
    local used_storages=""
    local unused_storages=""
    
    log_debug "get_unused_nfs_storages: Getting list of all NFS storages"
    
    # Get all NFS storages from PVE storage configuration
    while IFS= read -r line; do
        if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
            local storage_name="${BASH_REMATCH[1]}"
            all_nfs_storages+="$storage_name"$'\n'
            log_debug "get_unused_nfs_storages: Found NFS storage: '$storage_name'"
        fi
    done < /etc/pve/storage.cfg
    
    log_debug "get_unused_nfs_storages: All NFS storages found: $(echo "$all_nfs_storages" | tr '\n' ' ')"
    
    # Get storages that are in use
    used_storages=$(get_storages_used_by_vms_cts)
    
    log_debug "get_unused_nfs_storages: Used storages: $(echo "$used_storages" | tr '\n' ' ')"
    
    # Find unused storages
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            if [[ ! "$used_storages" =~ (^|$'\n')"$storage"($'\n'|$) ]]; then
                unused_storages+="$storage"$'\n'
                log_debug "get_unused_nfs_storages: Storage '$storage' is unused"
            else
                log_debug "get_unused_nfs_storages: Storage '$storage' is in use"
            fi
        fi
    done <<< "$all_nfs_storages"
    
    log_debug "get_unused_nfs_storages: Unused storages: $(echo "$unused_storages" | tr '\n' ' ')"
    
    # Sort and remove trailing newline and return
    echo -n "$unused_storages" | sed '/^$/d' | sort
}

# Get storage details including file count and size
get_storage_file_details() {
    local storage_name="$1"
    local mount_path="/mnt/pve/$storage_name"
    local images_path="$mount_path/images"
    local file_count=0
    local total_size="0"
    
    if [[ -d "$images_path" ]]; then
        # Count files
        file_count=$(find "$images_path" -type f 2>/dev/null | wc -l)
        
        # Get total size
        if [[ $file_count -gt 0 ]]; then
            total_size=$(du -sh "$images_path" 2>/dev/null | awk '{print $1}')
        fi
    fi
    
    echo "$file_count|$total_size"
}

# Display detailed file listing for a storage and get confirmation for deletion
show_storage_file_listing_and_confirm() {
    local storage_name="$1"
    local images_path="/mnt/pve/$storage_name/images"
    
    if [[ ! -d "$images_path" ]]; then
        dialog --msgbox "Error: Storage directory not found at $images_path" 8 60
        return 1
    fi
    
    log_debug "show_storage_file_listing_and_confirm: Generating file listing for $storage_name"
    
    # Generate detailed file listing with proper formatting
    local file_count=0
    local temp_file=$(mktemp)
    
    log_debug "show_storage_file_listing_and_confirm: Created temp file: $temp_file"
    
    # Build file listing in a temp file to preserve formatting
    {
        echo "Storage: $storage_name"
        echo "Path: /mnt/pve/$storage_name/images"
        echo ""
        echo "Files:"
        echo "------"
        
        # List each file with proper formatting
        while IFS= read -r file; do
            if [[ -f "$file" ]]; then
                ((file_count++))
                # Get file size in human-readable format
                local size=$(ls -lh "$file" 2>/dev/null | awk '{print $5}')
                # Remove base path for cleaner display
                local relative_path="${file#/mnt/pve/$storage_name/images/}"
                # Use non-breaking spaces for column alignment in dialog
                local nbsp=$'\u00A0'  # non-breaking space
                local filename_display="  $relative_path"
                # Create aligned display using non-breaking spaces
                local padding_needed=$((65 - ${#filename_display}))
                local padding=""
                for ((i=0; i<padding_needed; i++)); do
                    padding="${padding}${nbsp}"
                done
                printf "%s%s(%s)\n" "$filename_display" "$padding" "$size"
            fi
        done < <(find "$images_path" -type f 2>/dev/null | sort)
        
        # Get total size
        local total_size
        total_size=$(du -sh "$images_path" 2>/dev/null | awk '{print $1}')
        
        echo ""
        echo "Summary:"
        echo "--------"
        echo "  Total files: $file_count"
        echo "  Total size: $total_size"
        echo ""
        echo "WARNING: Deleting this storage will permanently remove all listed files!"
        echo ""
        echo "Do you want to proceed with deletion?"
    } > "$temp_file"
    
    # Debug: Log temp file content for debugging
    log_debug "show_storage_file_listing_and_confirm: Temp file content preview:"
    log_debug "$(head -20 "$temp_file" 2>/dev/null | while IFS= read -r line; do echo "  $line"; done)"
    log_debug "show_storage_file_listing_and_confirm: Full temp file saved at: $temp_file"
    
    # Show file listing with confirmation dialog
    dialog --keep-tite \
        --title "Storage Contains Files - Confirm Deletion" \
        --yes-label "Delete All Files" --no-label "Back to Main" --extra-button --extra-label "Back" \
        --yesno "$(<"$temp_file")" 30 100
    
    local result=$?
    
    # For debugging - don't delete temp file yet, log its location
    log_debug "show_storage_file_listing_and_confirm: DEBUG MODE - Temp file preserved at: $temp_file"
    log_debug "show_storage_file_listing_and_confirm: To examine formatting: cat '$temp_file'"
    # TODO: Uncomment next line after debugging: rm -f "$temp_file"
    
    log_debug "show_storage_file_listing_and_confirm: User selection result: $result"
    
    return $result
}

# Function: delete_cloned_nfs_storage_wizard
delete_cloned_nfs_storage_wizard() {
    # Check if REST API is configured
    if [[ -z "${rest_api_user}" || -z "${rest_api_password}" || -z "${rest_api_port}" ]]; then
        dialog --msgbox "REST API configuration is incomplete. Please use Setup to configure it first." 8 60
        return 1
    fi

    # Show progress message while fetching data
    log_debug "delete_cloned_nfs_storage_wizard: Showing fetching dialog"
    dialog --infobox "Fetching Cloned Volumes ..." 5 35

    # Get list of cloned storages with their IPs
    log_debug "delete_cloned_nfs_storage_wizard: About to call get_cloned_storage_list_with_ips"
    local cloned_storages_with_ips
    cloned_storages_with_ips=$(get_cloned_storage_list_with_ips 2>&1)
    local get_list_exit=$?
    log_debug "delete_cloned_nfs_storage_wizard: get_cloned_storage_list_with_ips returned with exit code: $get_list_exit"
    log_debug "delete_cloned_nfs_storage_wizard: get_cloned_storage_list_with_ips output length: ${#cloned_storages_with_ips}"
    
    # Clear the fetching dialog in case of any error
    dialog --clear >/dev/null 2>&1
    
    # Check for specific error types
    if [[ $get_list_exit -eq 10 ]]; then
        log_debug "delete_cloned_nfs_storage_wizard: Authentication failure detected, showing dialog"
        # Now we can safely show the dialog since we're at the top level
        dialog --msgbox "REST API Authentication Failed!\n\nThe REST API credentials are incorrect or have been changed on the $PRODUCT.\n\nPlease go to Setup → Configure REST API and update your credentials.\n\nCurrent user: $rest_api_user" 15 70
        return 1
    elif [[ $get_list_exit -eq 11 ]]; then
        log_debug "delete_cloned_nfs_storage_wizard: Access forbidden detected, showing dialog"
        dialog --msgbox "REST API Access Forbidden!\n\nThe user '$rest_api_user' does not have permission to perform this operation.\n\nPlease check user permissions on the $PRODUCT." 10 70
        return 1
    elif [[ $get_list_exit -ne 0 ]]; then
        log_debug "delete_cloned_nfs_storage_wizard: Other error occurred (code: $get_list_exit)"
        dialog --msgbox "Failed to get cloned storage list. Please check your NFS server connection and configuration." 8 60
        return 1
    fi

    if [[ -z "$cloned_storages_with_ips" ]]; then
        dialog --msgbox "No cloned NFS storages found." 8 40
        return 1
    fi

    # Build menu options
    local menu_items=()
    local index=1
    while IFS= read -r storage_with_ip; do
        if [[ -n "$storage_with_ip" ]]; then
            local storage_name="${storage_with_ip%:*}"
            local nfs_ip="${storage_with_ip#*:}"
            menu_items+=("$index" "$storage_name ($nfs_ip)")
            ((index++))
        fi
    done <<< "$cloned_storages_with_ips"

    # Show selection dialog
    local dialog_selected=$(mktemp)

    # Storage selection loop to handle back button
    while true; do
        # Show storage selection menu
        dialog --keep-tite \
            --title "Select Cloned NFS Storage to Delete" \
            --ok-label "Select" --cancel-label "Cancel" \
            --menu "Choose a cloned NFS storage to delete:" 18 100 10 \
            "${menu_items[@]}" 2> "$dialog_selected"

        exit_code=$?
        local selected_index=$(cat "$dialog_selected")
        rm -f "$dialog_selected"

        if [[ $exit_code -ne 0 || -z "$selected_index" ]]; then
            return 1
        fi

        # Get the selected storage name and its NFS IP
        local selected_storage
        local selected_nfs_ip_final
        local current_index=1
        while IFS= read -r storage_with_ip; do
            if [[ -n "$storage_with_ip" ]]; then
                if [[ "$current_index" == "$selected_index" ]]; then
                    selected_storage="${storage_with_ip%:*}"
                    selected_nfs_ip_final="${storage_with_ip#*:}"
                    break
                fi
                ((current_index++))
            fi
        done <<< "$cloned_storages_with_ips"

        # Set the NFS IP for the selected storage
        selected_nfs_ip="$selected_nfs_ip_final"

        if [[ -z "$selected_storage" ]]; then
            dialog --msgbox "Error: Could not determine selected storage." 8 50
            return 1
        fi

        # Check if VMs/CTs are using this storage
        local usage_file
        if usage_file=$(check_vmct_using_storage "$selected_storage"); then
            # Storage is in use, show warning
            log_info "Storage '$selected_storage' is in use by VMs/CTs"
            
            show_vmct_usage_warning "$selected_storage" "$usage_file"
            local warning_result=$?
            
            # Clean up temp file
            rm -f "$usage_file"
            
            case $warning_result in
                0)  # Continue Delete
                    log_info "User chose to continue deletion despite VM/CT usage"
                    break
                    ;;
                1)  # Cancel
                    log_info "User cancelled deletion due to VM/CT usage"
                    return 1
                    ;;
                3)  # Back
                    log_info "User chose to go back to storage selection"
                    continue
                    ;;
            esac
        else
            # No VMs/CTs using this storage, proceed
            log_info "No VMs/CTs are using storage '$selected_storage'"
            break
        fi
    done

    # Final confirmation dialog - user must type "delete"
    local confirm_input=$(mktemp)
    dialog --inputbox "WARNING: This action will delete the cloned NFS storage '$selected_storage' in Proxmox VE, as well as the corresponding cloned dataset in $VENDOR $PRODUCT on which it is based.\n\nThis operation is irreversible!\n\nTo confirm deletion, type 'delete':" 18 80 2> "$confirm_input"
    exit_code=$?
    local confirmation=$(trimq "$(cat "$confirm_input")")
    rm -f "$confirm_input"

    if [[ $exit_code -ne 0 || "$confirmation" != "delete" ]]; then
        dialog --msgbox "Deletion cancelled." 8 40
        return 1
    fi

    # Show progress dialog
    dialog --infobox "Deleting cloned NFS storage '$selected_storage'...

This may take a moment." 8 50

    # Execute deletion
    if delete_pve_NFS_storage "$selected_storage"; then
        dialog --msgbox "Successfully deleted cloned NFS storage '$selected_storage'.\n\nCleanup finished.\nReview logs in:\n$LOGS_DIR/pve-nfs-cleanup-*.log\nfor details." 12 70
        return 0
    else
        dialog --msgbox "Error: Failed to delete cloned NFS storage '$selected_storage'. Check the log file for details." 8 70
        return 1
    fi
}

# Function: delete_unused_nfs_storage_wizard
delete_unused_nfs_storage_wizard() {
    # Check if REST API is configured
    if [[ -z "${rest_api_user}" || -z "${rest_api_password}" || -z "${rest_api_port}" ]]; then
        dialog --msgbox "REST API configuration is incomplete. Please use Setup to configure it first." 8 60
        return 1
    fi

    # Show progress message while fetching data
    log_debug "delete_unused_nfs_storage_wizard: Getting unused NFS storages"
    dialog --infobox "Scanning for unused NFS storages..." 5 40

    # Get list of unused NFS storages
    local unused_storages
    unused_storages=$(get_unused_nfs_storages)
    log_debug "delete_unused_nfs_storage_wizard: get_unused_nfs_storages returned: '$unused_storages'"
    
    # Clear the fetching dialog
    dialog --clear >/dev/null 2>&1

    if [[ -z "$unused_storages" ]]; then
        log_debug "delete_unused_nfs_storage_wizard: No unused storages found"
        dialog --msgbox "No unused NFS storages found.\n\nAll NFS storages are currently being used by VMs or CTs." 8 60
        return 1
    fi
    
    log_debug "delete_unused_nfs_storage_wizard: Found unused storages: $unused_storages"

    # Build menu options with file count and size information
    local menu_items=()
    local storage_details=()
    local index=1
    
    log_debug "delete_unused_nfs_storage_wizard: Building menu with file details"
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            # Get file details for this storage
            local file_info
            file_info=$(get_storage_file_details "$storage")
            local file_count="${file_info%|*}"
            local total_size="${file_info#*|}"
            
            # Format menu item with storage name and file info using printf alignment
            local menu_text
            printf -v menu_text "%-45s Files: %s, Size: %s" "$storage" "$file_count" "$total_size"
            menu_items+=("$index" "$menu_text")
            storage_details+=("$storage|$file_count|$total_size")
            ((index++))
        fi
    done <<< "$unused_storages"

    # Show selection dialog
    local dialog_selected=$(mktemp)

    # Storage selection loop
    while true; do
        # Show storage selection menu
        dialog --keep-tite \
            --title "Select Unused NFS Storage to Delete" \
            --ok-label "Select" --cancel-label "Cancel" \
            --default-item "$DELETE_UNUSED_NFS_LAST_SELECTED" \
            --menu "Choose an unused NFS storage to delete:" 18 100 10 \
            "${menu_items[@]}" 2> "$dialog_selected"

        exit_code=$?
        local selected_index=$(cat "$dialog_selected")
        rm -f "$dialog_selected"

        if [[ $exit_code -ne 0 || -z "$selected_index" ]]; then
            return 1
        fi

        # Remember the selected option for next time
        DELETE_UNUSED_NFS_LAST_SELECTED="$selected_index"

        # Get the selected storage details
        local selected_detail="${storage_details[$((selected_index - 1))]}"
        local selected_storage="${selected_detail%%|*}"
        local remaining="${selected_detail#*|}"
        local file_count="${remaining%|*}"
        local total_size="${remaining#*|}"

        if [[ -z "$selected_storage" ]]; then
            dialog --msgbox "Error: Could not determine selected storage." 8 50
            return 1
        fi

        log_info "Selected unused storage: $selected_storage (Files: $file_count, Size: $total_size)"

        # If storage has files, show detailed file listing and get confirmation
        if [[ "$file_count" -gt 0 ]]; then
            show_storage_file_listing_and_confirm "$selected_storage"
            local confirm_result=$?
            
            case $confirm_result in
                0)  # Confirmed deletion
                    log_info "User confirmed deletion of storage with files: $selected_storage"
                    break
                    ;;
                1)  # Cancelled
                    log_info "User cancelled deletion of storage: $selected_storage"
                    return 1
                    ;;
                3)  # Back to selection
                    log_info "User went back to storage selection"
                    continue
                    ;;
            esac
        else
            # Empty storage, proceed directly to final confirmation
            log_info "Storage is empty, proceeding to final confirmation: $selected_storage"
            break
        fi
    done

    # Get NFS server IP for the selected storage
    log_debug "delete_unused_nfs_storage_wizard: About to get IP for storage '$selected_storage'"
    log_debug "delete_unused_nfs_storage_wizard: Current storage_ip_map has ${#storage_ip_map[@]} entries"
    
    # Ensure storage IP mapping is built
    if [[ ${#storage_ip_map[@]} -eq 0 ]]; then
        log_debug "delete_unused_nfs_storage_wizard: Storage IP map is empty, building it"
        build_storage_ip_map
        log_debug "delete_unused_nfs_storage_wizard: After build_storage_ip_map, map has ${#storage_ip_map[@]} entries"
    fi
    
    # Debug: Show all storage mappings
    log_debug "delete_unused_nfs_storage_wizard: Available storage mappings:"
    for storage in "${!storage_ip_map[@]}"; do
        log_debug "  - '$storage' -> '${storage_ip_map[$storage]}'"
    done
    
    get_storage_ip "$selected_storage" >/dev/null
    local get_ip_result=$?
    log_debug "delete_unused_nfs_storage_wizard: get_storage_ip returned code: $get_ip_result"
    
    if [[ -z "$selected_nfs_ip" ]]; then
        log_error "delete_unused_nfs_storage_wizard: Failed to get IP for storage '$selected_storage'"
        dialog --msgbox "Error: Could not determine NFS server IP for storage '$selected_storage'.\n\nCheck logs for details: $LOG_FILE" 10 70
        return 1
    fi
    
    log_debug "delete_unused_nfs_storage_wizard: Successfully got IP '$selected_nfs_ip' for storage '$selected_storage'"

    # Final confirmation dialog - user must type "delete"
    local confirm_input=$(mktemp)
    dialog --inputbox "WARNING: This action will delete the unused NFS storage '$selected_storage' in Proxmox VE and the dataset in $VENDOR $PRODUCT.\n\nThis operation is irreversible!\n\nTo confirm deletion, type 'delete':" 16 80 2> "$confirm_input"
    exit_code=$?
    local confirmation=$(trimq "$(cat "$confirm_input")")
    rm -f "$confirm_input"

    if [[ $exit_code -ne 0 || "$confirmation" != "delete" ]]; then
        dialog --msgbox "Deletion cancelled." 8 40
        return 1
    fi

    # Show progress dialog
    dialog --infobox "Deleting unused NFS storage '$selected_storage'...

This may take a moment." 8 50

    # Execute deletion using the specialized function for unused storage
    if delete_unused_pve_nfs_storage "$selected_storage"; then
        dialog --msgbox "Successfully deleted unused NFS storage '$selected_storage'.\n\nCleanup finished.\nReview logs in:\n$LOGS_DIR/pve-nfs-cleanup-*.log\nfor details.\n\nNOTE: The backup destination dataset was NOT deleted. If the backup destination dataset needs to be deleted, please proceed with the WEB GUI." 15 70
        return 0
    else
        dialog --msgbox "Error: Failed to delete unused NFS storage '$selected_storage'. Check the log file for details." 8 70
        return 1
    fi
}

# VM/CT MANAGER - BULK ACTIONS

# VM/CT Manager variables
vm_manager_selected_vms=()
vm_manager_action_counts=()

# Get VMs with hostpci (passthrough) for VM/CT Manager
vm_manager_get_vms_with_hostpci() {
    grep -Rl hostpci $ALL_PVE_VM_CONF 2>/dev/null | \
        sed -nE 's#.*/([0-9]+)\.conf#\1#p' | sort -u
}

# Get all VMs and CTs from entire Proxmox VE cluster for VM/CT Manager
vm_manager_get_vm_ct_list() {
    local vm_list=()
    local hostpci_vms
    
    # Get VMs with hostpci for skipping
    readarray -t hostpci_vms < <(vm_manager_get_vms_with_hostpci)
    
    # Get cluster-wide resources using optimized file parsing
    while IFS='|' read -r vmid name type status node tags lock; do
        if [[ -n "$vmid" ]]; then
            # Skip VMs with hostpci (passthrough)
            if [[ "$type" == "vm" ]] && [[ " ${hostpci_vms[*]} " == *" $vmid "* ]]; then
                log_debug "Skipping vm:$vmid (hostpci present)"
                continue
            fi
            
            # Skip VMs/CTs with joviandss in name
            if [[ "$name" == *"joviandss"* ]]; then
                log_debug "Skipping $type:$vmid (joviandss in name: $name)"
                continue
            fi
            
            # Format output without lock field to match original format
            vm_list+=("$vmid|$name|$type|$status|$tags")
        fi
    done < <(get_all_vm_ct_info)
    
    printf '%s\n' "${vm_list[@]}"
}

# Find longest name and tag for formatting in VM/CT Manager
vm_manager_get_max_lengths() {
    local max_name_length=0
    local max_tag_length=0
    local vm_data
    
    while IFS='|' read -r vmid name type status tags; do
        local name_length=${#name}
        local tag_length=${#tags}
        
        if [[ $name_length -gt $max_name_length ]]; then
            max_name_length=$name_length
        fi
        
        if [[ $tag_length -gt $max_tag_length ]]; then
            max_tag_length=$tag_length
        fi
    done <<< "$1"
    
    # Minimum lengths
    if [[ $max_name_length -lt 10 ]]; then
        max_name_length=10
    fi
    
    if [[ $max_tag_length -lt 4 ]]; then
        max_tag_length=4
    fi
    
    echo "$max_name_length|$max_tag_length"
}

# VM/CT selection dialog for VM/CT Manager
vm_manager_select_vm_ct() {
    local vm_ct_data
    local select_all_mode=false
    local max_name_length
    local max_tag_length
    local menu_items=()
    
    # Get VM/CT data
    vm_ct_data=$(vm_manager_get_vm_ct_list)
    if [[ -z "$vm_ct_data" ]]; then
        dialog --title "Error" --msgbox "No VMs or CTs found in the cluster." 8 50
        return 1
    fi
    
    # Calculate max name and tag lengths for formatting
    local max_lengths=$(vm_manager_get_max_lengths "$vm_ct_data")
    IFS='|' read -r max_name_length max_tag_length <<< "$max_lengths"
    
    while true; do
        menu_items=()
        local select_label
        
        if $select_all_mode; then
            select_label="Deselect All"
        else
            select_label="Select All"
        fi
        
        # Build menu items
        while IFS='|' read -r vmid name type status tags; do
            if [[ -n "$vmid" ]]; then
                # Format display string with proper spacing
                local formatted_name=$(printf "%-${max_name_length}s" "$name")
                local formatted_tags=$(printf "%-${max_tag_length}s" "$tags")
                local display_string=$(printf "(%s) [%s] %-2s %s" "$formatted_name" "$formatted_tags" "$type" "$status")
                
                # Check if this VM/CT was previously selected
                local selection_status="off"
                for selected_vm in "${vm_manager_selected_vms[@]}"; do
                    if [[ "$selected_vm" == "$vmid" ]]; then
                        selection_status="on"
                        break
                    fi
                done
                
                menu_items+=("$vmid" "$display_string" "$selection_status")
            fi
        done <<< "$vm_ct_data"
        
        # Create dialog
        cmd=(dialog --keep-tite --title "VM/CT Manager - Select VM/CT"
             --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
             --checklist "Choose VM/CT to manage (use SPACE to select/deselect):" 20 120 12)
        options=("${menu_items[@]}")
        
        dialog_menu
        
        case $dialog_exit_code in
            0)  # Next
                if [[ -n "$selected_option" ]]; then
                    # Parse selected VMs/CTs
                    vm_manager_selected_vms=()
                    IFS=' ' read -ra ADDR <<< "$selected_option"
                    for vmid in "${ADDR[@]}"; do
                        vm_manager_selected_vms+=("$vmid")
                    done
                    log_info "Selected VMs/CTs: ${vm_manager_selected_vms[*]}"
                    return 0
                else
                    dialog --title "Warning" --msgbox "Please select at least one VM or CT." 8 50
                fi
                ;;
            1)  # Back
                return 1
                ;;
            3)  # Select All/Deselect All
                if $select_all_mode; then
                    # Deselect all
                    vm_manager_selected_vms=()
                    select_all_mode=false
                    log_debug "Deselected all VMs/CTs"
                else
                    # Select all
                    vm_manager_selected_vms=()
                    while IFS='|' read -r vmid name type status tags; do
                        if [[ -n "$vmid" ]]; then
                            vm_manager_selected_vms+=("$vmid")
                        fi
                    done <<< "$vm_ct_data"
                    select_all_mode=true
                    log_debug "Selected all VMs/CTs: ${vm_manager_selected_vms[*]}"
                fi
                ;;
        esac
    done
}

# Confirm critical action with detailed explanation for VM/CT Manager
vm_manager_confirm_critical_action() {
    local action="$1"
    local vm_count="${#vm_manager_selected_vms[@]}"
    local description=""
    local warning=""
    
    case "$action" in
        "shutdown")
            description="GRACEFUL SHUTDOWN - Sends shutdown signal to guest OS"
            warning="- Safe: Saves all data and applications\\n- Speed: Slower (waits for guest OS)\\n- Recommended for normal operations"
            ;;
        "stop")
            description="FORCE STOP - Immediately stops VM/CT without graceful shutdown"
            warning="- UNSAFE: May cause data loss or corruption\\n- Speed: Fast (immediate)\\n- Use only when shutdown fails"
            ;;
        "reboot")
            description="GRACEFUL REBOOT - Restarts VM/CT with proper shutdown first"
            warning="- Safe: Saves all data before restart\\n- Speed: Medium (graceful shutdown + restart)\\n- Recommended for normal restarts"
            ;;
        "reset")
            description="HARD RESET - Force restart like pressing physical reset button"
            warning="- VERY UNSAFE: High risk of data corruption\\n- Speed: Fastest (immediate reset)\\n- Use ONLY when system is completely frozen"
            ;;
        "suspend")
            description="SUSPEND - Pause VM/CT to memory, preserving current state"
            warning="- Safe: Preserves all running applications\\n- Speed: Fast pause/resume\\n- Use: Temporary resource management"
            ;;
        *)
            return 0  # No confirmation needed for non-critical actions
            ;;
    esac
    
    local prompt_text="CRITICAL ACTION:\n$description\\n\\nEffects on $vm_count VM/CT(s):\\n$warning\\n\\nTo confirm this action, type '$action':"
    
    while true; do
        # Confirmation input with description
        local confirm_input=$(mktemp)
        dialog --title "CONFIRM CRITICAL ACTION" --inputbox "$prompt_text" 18 80 2> "$confirm_input"
        local exit_code=$?
        local confirmation=$(trimq "$(cat "$confirm_input")")
        rm -f "$confirm_input"
        
        # User cancelled
        if [[ $exit_code -ne 0 ]]; then
            return 1
        fi
        
        # Correct input
        if [[ "$confirmation" == "$action" ]]; then
            return 0
        fi
        
        # Wrong input - show error and retry
        dialog --title "INPUT ERROR" --msgbox "You typed: '$confirmation'\\n\\nExpected: '$action'\\n\\nPlease try again or use Cancel to abort." 10 60
    done
}

# Action selection dialog for VM/CT Manager
vm_manager_select_action() {
    local actions=(
        "start" "Start selected VM/CT"
        "shutdown" "Graceful shutdown of selected VM/CT"
        "stop" "Force stop selected VM/CT"
        "reboot" "Reboot selected VM/CT"
        "reset" "Reset selected VM/CT"
        "suspend" "Suspend selected VM/CT to memory"
        "resume" "Resume suspended VM/CT"
    )
    
    cmd=(dialog --keep-tite --title "VM/CT Manager - Select Action"
         --ok-label "Execute" --cancel-label "Back"
         --menu "Choose action to perform on selected VM/CT:" 15 60 8)
    options=("${actions[@]}")
    
    dialog_menu
    
    case $dialog_exit_code in
        0)  # Execute
            if [[ -n "$selected_option" ]]; then
                log_info "Selected action: $selected_option"
                return 0
            fi
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Get VM/CT type, current status and node from cluster for VM/CT Manager
vm_manager_get_vm_type_status_node() {
    local vmid="$1"
    
    # Get VM/CT info using optimized function
    local vm_info=$(get_all_vm_ct_info | grep "^${vmid}|" | head -1)
    
    if [[ -n "$vm_info" ]]; then
        # Extract type, status, and node from the info
        # Format: vmid|name|type|status|node|tags|lock
        local type=$(echo "$vm_info" | cut -d'|' -f3)
        local status=$(echo "$vm_info" | cut -d'|' -f4)
        local node=$(echo "$vm_info" | cut -d'|' -f5)
        
        echo "${type}|${status}|${node}"
        return 0
    fi
    
    echo "unknown|unknown|unknown"
    return 1
}

# Get VM/CT detailed info including name and tags
vm_manager_get_vm_details() {
    local vmid="$1"
    
    # Get VM/CT info using optimized function
    local vm_info=$(get_all_vm_ct_info | grep "^${vmid}|" | head -1)
    
    if [[ -n "$vm_info" ]]; then
        # Extract details from the info
        # Format: vmid|name|type|status|node|tags|lock
        local name=$(echo "$vm_info" | cut -d'|' -f2)
        local type=$(echo "$vm_info" | cut -d'|' -f3)
        local status=$(echo "$vm_info" | cut -d'|' -f4)
        local node=$(echo "$vm_info" | cut -d'|' -f5)
        local tags=$(echo "$vm_info" | cut -d'|' -f6)
        
        echo "${type}|${status}|${node}|${name}|${tags}"
        return 0
    fi
    
    echo "unknown|unknown|unknown||"
    return 1
}

# Find which host/node a VM/CT is located on
# Usage: on_which_host_vmct_is <vmid>
# Returns: node name or empty if not found
on_which_host_vmct_is() {
    local vmid="$1"
    
    if [[ -z "$vmid" ]]; then
        log_error "on_which_host_vmct_is: VMID parameter is required"
        return 1
    fi
    
    log_debug "on_which_host_vmct_is: Looking for VM/CT $vmid"
    
    # Get VM/CT info using optimized function
    local vm_info=$(get_all_vm_ct_info | grep "^${vmid}|" | head -1)
    
    if [[ -n "$vm_info" ]]; then
        # Extract node from the info
        # Format: vmid|name|type|status|node|tags|lock
        local node=$(echo "$vm_info" | cut -d'|' -f5)
        
        log_debug "on_which_host_vmct_is: VM/CT $vmid found on node: $node"
        echo "$node"
        return 0
    fi
    
    log_debug "on_which_host_vmct_is: VM/CT $vmid not found"
    return 1
}

# Get IP address for a given hostname/node
# Usage: host_ip <hostname>
# Returns: IP address or empty if not found
host_ip() {
    local hostname="$1"
    
    if [[ -z "$hostname" ]]; then
        log_error "host_ip: hostname parameter is required"
        return 1
    fi
    
    log_debug "host_ip: Looking up IP for hostname '$hostname'"
    
    # First try to get IP from corosync configuration (for cluster nodes)
    local corosync_ip
    corosync_ip=$(awk -v target="$hostname" '
        $1 == "name:" {node=$2}
        $1 == "ring0_addr:" && node == target {print $2; exit}
    ' /etc/pve/corosync.conf 2>/dev/null)
    
    if [[ -n "$corosync_ip" ]]; then
        log_debug "host_ip: Found IP '$corosync_ip' for hostname '$hostname' in corosync.conf"
        echo "$corosync_ip"
        return 0
    fi
    
    # Fallback to getent for name resolution
    local getent_ip
    getent_ip=$(getent hosts "$hostname" 2>/dev/null | awk '{print $1; exit}')
    
    if [[ -n "$getent_ip" ]]; then
        log_debug "host_ip: Found IP '$getent_ip' for hostname '$hostname' via getent"
        echo "$getent_ip"
        return 0
    fi
    
    log_debug "host_ip: No IP found for hostname '$hostname'"
    return 1
}

# Execute action on VM/CT for VM/CT Manager
vm_manager_execute_action() {
    local vmid="$1"
    local action="$2"
    local vm_type_status_node
    local vm_type
    local current_status
    local vm_node
    local current_node
    local cmd_to_run
    local result
    
    # Get VM type, current status and node
    vm_type_status_node=$(vm_manager_get_vm_type_status_node "$vmid")
    IFS='|' read -r vm_type current_status vm_node <<< "$vm_type_status_node"
    
    # Get current node name
    current_node=$(hostname)
    
    log_debug "VM/CT $vmid: type=$vm_type, status=$current_status, node=$vm_node, action=$action"
    log_debug "Raw vm_type_status_node response: '$vm_type_status_node'"
    log_debug "Current node: $current_node, VM node: $vm_node"
    
    # Check if action is needed
    case "$action" in
        "start")
            if [[ "$current_status" == "running" ]]; then
                log_info "VM/CT $vmid already running, skipping start"
                return 2  # Skip
            elif [[ "$current_status" == "suspended" ]]; then
                log_info "VM/CT $vmid is suspended, skipping start (use resume instead)"
                return 2  # Skip
            fi
            ;;
        "stop"|"shutdown")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid already stopped, skipping $action"
                return 2  # Skip
            fi
            ;;
        "reboot"|"reset")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid is stopped, skipping $action (requires running VM/CT)"
                return 2  # Skip
            fi
            ;;
        "suspend")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid is stopped, skipping suspend (requires running VM/CT)"
                return 2  # Skip
            elif [[ "$current_status" == "suspended" ]]; then
                log_info "VM/CT $vmid already suspended, skipping suspend"
                return 2  # Skip
            fi
            ;;
        "resume")
            if [[ "$current_status" != "suspended" ]]; then
                log_info "VM/CT $vmid is not suspended, skipping resume (status: $current_status)"
                return 2  # Skip
            fi
            ;;
    esac
    
    # Build command
    if [[ "$vm_type" == "vm" ]]; then
        local base_cmd="qm $action $vmid"
        log_debug "Building VM command: $base_cmd"
    elif [[ "$vm_type" == "ct" ]]; then
        # For containers, reset should be reboot
        if [[ "$action" == "reset" ]]; then
            local base_cmd="pct reboot $vmid"
            log_debug "Building CT command (reset->reboot): $base_cmd"
        else
            local base_cmd="pct $action $vmid"
            log_debug "Building CT command: $base_cmd"
        fi
    else
        log_error "Unknown VM/CT type '$vm_type' for $vmid"
        return 1
    fi
    
    # Execute command on correct node using SSH
    local vm_node_ip
    vm_node_ip=$(host_ip "$vm_node")
    
    if [[ -z "$vm_node_ip" ]]; then
        log_error "Cannot resolve IP for node '$vm_node'"
        return 1
    fi
    
    # Always use SSH to execute on target node (even if it's the current node)
    cmd_to_run="ssh root@$vm_node_ip '$base_cmd'"
    log_debug "Executing via SSH on node $vm_node ($vm_node_ip): $cmd_to_run"
    log_info "Executing via SSH on node $vm_node: $base_cmd"
    
    result=$(ssh root@"$vm_node_ip" "$base_cmd" 2>&1 < /dev/null)
    local exit_code=$?
    
    log_debug "SSH command result: exit_code=$exit_code, output='$result'"
    
    if [[ $exit_code -eq 0 ]]; then
        log_info "Action $action completed successfully for VM/CT $vmid on node $vm_node"
        return 0
    else
        log_error "Action $action failed for VM/CT $vmid on node $vm_node: $result (exit code: $exit_code)"
        return 1
    fi
}

# Execute actions with progress for VM/CT Manager
vm_manager_execute_actions() {
    local action="$1"
    local total_count=${#vm_manager_selected_vms[@]}
    local success_count=0
    local skip_count=0
    local error_count=0
    local current=0
    
    log_debug "Starting batch action '$action' on $total_count VMs/CTs"
    log_debug "Selected VMs/CTs: ${vm_manager_selected_vms[*]}"
    
    # Initialize action counts
    vm_manager_action_counts=("$success_count" "$skip_count" "$error_count")
    
    for vmid in "${vm_manager_selected_vms[@]}"; do
        current=$((current + 1))
        local progress=$((current * 100 / total_count))
        
        # Get VM details for display
        local vm_details=$(vm_manager_get_vm_details "$vmid")
        local vm_type vm_name vm_tags
        IFS='|' read -r vm_type _ _ vm_name vm_tags <<< "$vm_details"
        
        # Convert vm_type to display format
        local display_type
        if [[ "$vm_type" == "vm" ]]; then
            display_type="VM"
        elif [[ "$vm_type" == "ct" ]]; then
            display_type="CT"
        else
            display_type="VM/CT"
        fi
        
        # Build display name with tags
        local display_name="$vmid"
        if [[ -n "$vm_name" ]]; then
            display_name="$vmid ($vm_name)"
        fi
        if [[ -n "$vm_tags" ]]; then
            display_name="$display_name [$vm_tags]"
        fi
        
        # Show progress
        echo "$progress" | dialog --title "Processing $display_type" --gauge "Executing $action on $display_type $display_name ($current/$total_count)" 8 90
        
        # Execute action
        vm_manager_execute_action "$vmid" "$action"
        local exit_code=$?
        
        case $exit_code in
            0)  # Success
                success_count=$((success_count + 1))
                ;;
            1)  # Error
                error_count=$((error_count + 1))
                ;;
            2)  # Skip
                skip_count=$((skip_count + 1))
                ;;
        esac
        
        # Small delay for user feedback
        sleep 0.5
    done
    
    # Update action counts
    vm_manager_action_counts=("$success_count" "$skip_count" "$error_count")
}

# Show final summary for VM/CT Manager
vm_manager_show_summary() {
    local action="$1"
    local success_count="${vm_manager_action_counts[0]}"
    local skip_count="${vm_manager_action_counts[1]}"
    local error_count="${vm_manager_action_counts[2]}"
    local total_count=${#vm_manager_selected_vms[@]}
    
    local summary_text="Action: $action\n\n"
    summary_text+="Total VMs/CTs processed: $total_count\n"
    summary_text+="Successful: $success_count\n"
    summary_text+="Skipped: $skip_count\n"
    summary_text+="Errors: $error_count\n\n"
    
    if [[ $error_count -gt 0 ]]; then
        summary_text+="Check log file for error details: $LOG_FILE"
    fi
    
    dialog --ok-label "Continue" --title "Action Summary" --msgbox "$summary_text" 15 60
    log_info "Action summary - Success: $success_count, Skipped: $skip_count, Errors: $error_count"
}

# VM/CT Manager - Bulk Actions main function
vm_ct_manager_bulk_actions() {
    # Check if running as root (required for Proxmox VE commands)
    if [[ $EUID -ne 0 ]]; then
        dialog --title "Error" --msgbox "VM/CT Manager must be run as root to access Proxmox VE commands." 8 60
        return 1
    fi
    
    # Check if Proxmox VE commands are available
    if ! command -v qm >/dev/null 2>&1 || ! command -v pct >/dev/null 2>&1; then
        dialog --title "Error" --msgbox "Proxmox VE commands (qm/pct) not found. Please run on a Proxmox VE node." 10 60
        return 1
    fi
    
    while true; do
        # Step 1: Select VMs/CTs
        if ! vm_manager_select_vm_ct; then
            return 0  # Back to main menu
        fi
        
        # Step 2: Select action
        if ! vm_manager_select_action; then
            continue  # Go back to VM selection
        fi
        
        # Step 3: Confirm critical actions and execute
        if ! vm_manager_confirm_critical_action "$selected_option"; then
            continue  # Go back to action selection if cancelled
        fi
        vm_manager_execute_actions "$selected_option"
        
        # Step 4: Show summary
        vm_manager_show_summary "$selected_option"
        
        return 0  # Back to main menu
    done
}

# PVE HA RESOURCES FUNCTIONS

# Configuration
GROUP="ha-nodes"

# Returns PVE major version (e.g., 8 or 9)
get_pve_major() {
  pveversion | awk -F'[/ ]' '/pve-manager/{split($2,a,"."); print a[1]; exit}'
}

# Ensure placement constraint exists for a set of resources
# Args:
#   1: name (group name on v8, rule name on v9+)  e.g. "ha-nodes"
#   2: nodes CSV "node1:2,node2:1,node3"          (same format old/new)
#   3: resources CSV "vm:101,ct:202"              (required for v9+ rule)
#   4: restricted flag "1|0"                      (1 -> strict on v9+)
#   5: comment                                    (optional)
ensure_ha_placement() {
  local NAME="$1" NODES="$2" RESLIST="$3" RESTRICTED="$4" COMMENT="${5:-Auto-created by pve-tools}"
  log_debug "DEBUG: ensure_ha_placement called with:"
  log_debug "  - NAME: '$NAME'"
  log_debug "  - NODES: '$NODES'"  
  log_debug "  - RESLIST: '$RESLIST'"
  log_debug "  - RESTRICTED: '$RESTRICTED'"
  log_debug "  - COMMENT: '$COMMENT'"
  
  local MAJOR
  MAJOR="$(get_pve_major)"

  if [ -z "$MAJOR" ]; then
    log_error "Failed to detect Proxmox VE version"
    return 1
  fi

  log_debug "DEBUG: Proxmox VE major version detected: $MAJOR"

  if [ "$MAJOR" -ge 9 ]; then
    # Node Affinity Rule (PVE 9+)
    log_info "Using PVE 9+ node affinity rules for placement constraint '$NAME'"
    # create if missing (handle Unicode table format)
    log_debug "DEBUG: Checking if rule '$NAME' exists..."
    local existing_rules
    existing_rules=$(ha-manager rules list --type node-affinity | tail -n +4 | head -n -1 | grep -v '^[[:space:]]*$' | sed 's/│//g' | awk '{print $1}')
    log_debug "DEBUG: Existing rules found: '$existing_rules'"
    
    if ! echo "$existing_rules" | grep -qx "$NAME"; then
      log_debug "DEBUG: Rule '$NAME' does not exist, creating new rule"
      # --strict 1 behaves like old --restricted 1
      local STRICT=0
      [ "$RESTRICTED" = "1" ] && STRICT=1
      log_info "Creating node affinity rule '$NAME' with strict=$STRICT"
      log_debug "DEBUG: Rule creation command will be:"
      log_debug "  ha-manager rules add node-affinity '$NAME' --resources '$RESLIST' --nodes '$NODES' --comment '$COMMENT' --strict '$STRICT'"
      
      if ha-manager rules add node-affinity "$NAME" \
        --resources "$RESLIST" \
        --nodes "$NODES" \
        --comment "$COMMENT" \
        --strict "$STRICT" 2>&1; then
        log_info "Successfully created node affinity rule '$NAME'"
      else
        log_error "Failed to create node affinity rule '$NAME'"
        # Let's capture the error output
        local error_output
        error_output=$(ha-manager rules add node-affinity "$NAME" \
          --resources "$RESLIST" \
          --nodes "$NODES" \
          --comment "$COMMENT" \
          --strict "$STRICT" 2>&1)
        log_error "DEBUG: Rule creation error output: $error_output"
        return 1
      fi
    else
      log_debug "DEBUG: Rule '$NAME' already exists, updating existing rule"
      # update existing rule (idempotent)
      local STRICT=0
      [ "$RESTRICTED" = "1" ] && STRICT=1
      log_info "Updating existing node affinity rule '$NAME' with strict=$STRICT"
      log_debug "DEBUG: Rule update command will be:"
      log_debug "  ha-manager rules set node-affinity '$NAME' --resources '$RESLIST' --nodes '$NODES' --comment '$COMMENT' --strict '$STRICT'"
      
      if ha-manager rules set node-affinity "$NAME" \
        --resources "$RESLIST" \
        --nodes "$NODES" \
        --comment "$COMMENT" \
        --strict "$STRICT" 2>&1; then
        log_info "Successfully updated node affinity rule '$NAME'"
      else
        log_error "Failed to update node affinity rule '$NAME'"
        # Let's capture the error output for update too
        local error_output
        error_output=$(ha-manager rules set node-affinity "$NAME" \
          --resources "$RESLIST" \
          --nodes "$NODES" \
          --comment "$COMMENT" \
          --strict "$STRICT" 2>&1)
        log_error "DEBUG: Rule update error output: $error_output"
        return 1
      fi
    fi
  else
    # HA Group (PVE 8 and older)
    log_info "Using PVE 8 HA groups for placement constraint '$NAME'"
    if ! ha-manager groupconfig | awk '{print $1}' | grep -qx "$NAME"; then
      log_info "Creating HA group '$NAME' with restricted=$RESTRICTED"
      if ha-manager groupadd "$NAME" \
        --nodes "$NODES" \
        --restricted "$RESTRICTED" \
        --comment "$COMMENT" 2>/dev/null; then
        log_info "Successfully created HA group '$NAME'"
      else
        log_error "Failed to create HA group '$NAME'"
        return 1
      fi
    else
      log_info "Updating existing HA group '$NAME' with restricted=$RESTRICTED"
      if ha-manager groupset "$NAME" \
        --nodes "$NODES" \
        --restricted "$RESTRICTED" \
        --comment "$COMMENT" 2>/dev/null; then
        log_info "Successfully updated HA group '$NAME'"
      else
        log_error "Failed to update HA group '$NAME'"
        return 1
      fi
    fi
  fi
}

# Function: get_groups
get_groups() {
  ha-manager groupconfig 2>/dev/null | grep -E '^group:' | awk '{print $2}'
}

# Function: get_vms_with_hostpci
get_vms_with_hostpci() {
  grep -Rl hostpci $ALL_PVE_VM_CONF 2>/dev/null | \
    sed -nE 's#.*/([0-9]+)\.conf#\1#p' | sort -u
}

# Get all PVE nodes
get_all_nodes() {
    local nodes_result
    nodes_result=$(timeout 30 pvesh get /nodes --output-format=json 2>/dev/null | python3 -c '
import sys, json
try:
    nodes = json.load(sys.stdin)
    # extract and sort the "node" values, then join with spaces
    print(" ".join(sorted(n["node"] for n in nodes)))
except:
    sys.exit(1)
' 2>/dev/null)
    
    if [ $? -eq 0 ] && [ -n "$nodes_result" ]; then
        echo "$nodes_result"
        return 0
    else
        log_error "Failed to get PVE nodes"
        return 1
    fi
}


# Confirm HA node selection
confirm_ha_nodes() {
    # Build dynamic nodes text
    local nodes_text=""
    local ordinals=("First" "Second" "Third" "Fourth" "Fifth" "Sixth" "Seventh" "Eighth")
    
    for ((i=0; i<${#SELECTED_HA_NODES[@]}; i++)); do
        local ordinal
        if [ $i -lt ${#ordinals[@]} ]; then
            ordinal="${ordinals[i]}"
        else
            ordinal="$((i+1))th"
        fi
        nodes_text+="$ordinal node: ${SELECTED_HA_NODES[i]}\n"
    done
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Confirm HA Node Selection"
           --yes-label "Next" --no-label "Back"
           --yesno "Selected nodes for HA group \"$GROUP\":\n\n$nodes_text\nProceed with these nodes?" 12 60)
    local options=()
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            # Copy SELECTED_HA_NODES to GROUP_NODES for compatibility
            GROUP_NODES=("${SELECTED_HA_NODES[@]}")
            NODES_CSV=$(IFS=, ; echo "${GROUP_NODES[*]}")
            log_info "Confirmed HA nodes: $NODES_CSV"
            return 0
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Get all VM/CT resources from cluster
get_all_vm_ct_resources() {
    # Use optimized function and format output to match expected format
    get_all_vm_ct_info | awk -F'|' '{print $1"|"$2"|"$3"|"$4"|"$5}'
}

# Calculate maximum field lengths for formatting
get_max_field_lengths() {
    local resources="$1"  # Accept resources as parameter to avoid duplicate calls
    
    local max_name_length=4  # minimum for "Name"
    local max_node_length=4  # minimum for "Node"
    
    while IFS='|' read -r vmid name vm_type status node; do
        if [ ${#name} -gt $max_name_length ]; then
            max_name_length=${#name}
        fi
        if [ ${#node} -gt $max_node_length ]; then
            max_node_length=${#node}
        fi
    done <<< "$resources"
    
    echo "$max_name_length|$max_node_length"
}

# Select VM/CT for HA Resources
select_vm_ct_for_ha() {
    local resources=() menu_items=() vmids=()
    
    # Get all VM/CT resources once
    local all_resources_data
    all_resources_data=$(get_all_vm_ct_resources)
    
    # Get HA manager status once to avoid multiple calls
    local ha_manager_status=$(ha-manager status)
    
    if [ -z "$all_resources_data" ]; then
        dialog --msgbox "No VMs/CTs found in the cluster." 8 40
        return 1
    fi
    
    # Parse resources data
    while IFS='|' read -r vmid name vm_type status node; do
        if [ -n "$vmid" ]; then
            resources+=("$vmid|$name|$vm_type|$status|$node")
            vmids+=("$vmid")
        fi
    done <<< "$all_resources_data"
    
    if [ "${#resources[@]}" -eq 0 ]; then
        dialog --msgbox "No VMs/CTs found in the cluster." 8 40
        return 1
    fi
    
    # Get max field lengths for formatting using cached data
    local max_lengths
    max_lengths=$(get_max_field_lengths "$all_resources_data")
    local max_name_length=$(echo "$max_lengths" | cut -d'|' -f1)
    local max_node_length=$(echo "$max_lengths" | cut -d'|' -f2)
    
    # Process each resource entry
    for resource in "${resources[@]}"; do
        IFS='|' read -r vmid name vm_type status node <<< "$resource"
        
        # Skip hostpci VMs
        local skip_hostpci=false
        if [[ "$vm_type" == "vm" ]] && [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            skip_hostpci=true
        fi
        
        # Skip if already in HA
        local skip_ha=false
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "${vm_type}:${vmid}"; then
            skip_ha=true
        fi
        
        # Format display string
        local formatted_name=$(printf "%-${max_name_length}s" "$name")
        local formatted_node=$(printf "%-${max_node_length}s" "$node")
        
        local display_string=$(printf "%s  %s  %s  %s" "$formatted_name" "$vm_type" "$status" "$formatted_node")
        
        # Add suffix if skipped using non-breaking spaces for dialog alignment
        local nbsp=$'\u00A0'  # non-breaking space
        if [ "$skip_hostpci" = true ]; then
            display_string+="${nbsp}${nbsp}(hostpci - will be skipped)"
        elif [ "$skip_ha" = true ]; then
            display_string+="${nbsp}${nbsp}(already in HA - will be skipped)"
        fi
        
        # Check if this VM/CT was previously selected
        local item_status="off"
        for selected_vmid in "${SELECTED_VMIDS[@]}"; do
            if [[ "$selected_vmid" == "$vmid" ]]; then
                item_status="on"
                break
            fi
        done
        
        menu_items+=("$vmid" "$display_string" "$item_status")
    done
    
    local dialog_selected=$(mktemp)
    local select_label="Select All"
    if [[ "$select_all_mode" == true ]]; then
        select_label="Deselect All"
    fi
    
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Select VM/CT for HA Resources"
         --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
         --checklist "Choose VM/CT to add to HA resources (use SPACE to select/deselect):" 20 120 10)
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select at least one VM/CT to add to HA resources." 8 50
                return 2  # Stay on same step
            fi
            
            # Parse selected VM IDs
            SELECTED_VMIDS=()
            IFS=' ' read -ra temp_vmids <<< "$selected_option"
            for vmid in "${temp_vmids[@]}"; do
                # Remove quotes if present
                vmid=$(echo "$vmid" | tr -d '"')
                SELECTED_VMIDS+=("$vmid")
            done
            return 0
            ;;
        1)  # Back
            return 1
            ;;
        3)  # Select All / Deselect All
            if [[ "$select_all_mode" == false ]]; then
                # Select all
                SELECTED_VMIDS=()
                for vmid in "${vmids[@]}"; do
                    SELECTED_VMIDS+=("$vmid")
                done
                select_all_mode=true
            else
                # Deselect all
                SELECTED_VMIDS=()
                select_all_mode=false
            fi
            return 2  # Stay on same step
            ;;
    esac
}

# Show confirmation dialog
show_confirmation_dialog() {
    local selected_count="${#SELECTED_VMIDS[@]}"
    local hostpci_count="${#HOSTPCI_VMS[@]}"
    
    # Get HA manager status once to avoid multiple calls
    local ha_manager_status=$(ha-manager status)
    
    # Count VMs/CTs that will be skipped
    local skip_count=0
    local skip_details=""
    
    for vmid in "${SELECTED_VMIDS[@]}"; do
        # Check if VM has hostpci
        if [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            skip_details+="- VM $vmid (hostpci present)\n"
            ((skip_count++))
        fi
        
        # Check if already in HA
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "vm:$vmid" || echo "$ha_manager_status" | awk '{print $1}' | grep -qx "ct:$vmid"; then
            skip_details+="- VM/CT $vmid (already in HA)\n"
            ((skip_count++))
        fi
    done
    
    local info_text="Summary of VM/CT selection for HA resources:\n\n"
    info_text+="Selected VM/CT count: $selected_count\n"
    # Use version-appropriate terminology (group for PVE <9, rule for PVE 9+)
    local MAJOR
    MAJOR="$(get_pve_major)"
    if [ "$MAJOR" -ge 9 ]; then
        info_text+="Target HA rule: $GROUP\n"
    else
        info_text+="Target HA group: $GROUP\n"
    fi
    info_text+="Target HA nodes: $NODES_CSV\n\n"
    
    if [ $skip_count -gt 0 ]; then
        info_text+="Items that will be skipped ($skip_count):\n$skip_details\n"
    fi
    
    local process_count=$((selected_count - skip_count))
    info_text+="Items to be processed: $process_count\n\n"
    info_text+="Do you want to continue?"
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: HA Resources Summary"
           --yes-label "Execute" --no-label "Back"
           --yesno "$info_text" 20 80)
    local options=()
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Execute
            log_info "User confirmed HA resources addition operation"
            return 0
            ;;
        1)  # Back
            log_info "User went back from HA resources addition confirmation"
            return 1
            ;;
    esac
}

# Process and add resources (PVE 8: groups, PVE 9+: node-affinity rules)
process_ha_resources() {
    local total_processed=0
    local total_added=0
    local total_skipped=0
    local results_text=""
    local MAJOR
    MAJOR="$(get_pve_major)"

    # Get HA manager status once to avoid multiple calls
    local ha_manager_status
    ha_manager_status=$(ha-manager status)

    # Get type information for selected VMs/CTs
    local vm_type_map
    vm_type_map=$(get_all_vm_ct_info | awk -F'|' '{print $1":"$3}')

    # Will hold SIDs that end up HA-managed (already in HA or newly added)
    # We'll use this to build the node-affinity rule on PVE >= 9
    local -a RESOURCES_FOR_RULE=()

    # Count total selected resources for progress
    local total_resources=${#SELECTED_VMIDS[@]}

    # Process each selected VM/CT
    for vmid in "${SELECTED_VMIDS[@]}"; do
        ((total_processed++))

        # Get VM/CT type
        local vm_type
        vm_type=$(echo "$vm_type_map" | grep "^$vmid:" | cut -d':' -f2)
        if [ -z "$vm_type" ]; then
            log_error "Cannot determine type for VM/CT $vmid"
            results_text+="Failed to determine type for $vmid\n"
            continue
        fi

        local sid="${vm_type}:${vmid}"

        # Update progress
        local progress=$((total_processed * 100 / total_resources))
        echo "$progress" | dialog --gauge "Processing selected VM/CT resources...\nCurrent: $sid" 8 60 0

        # Skip hostpci VMs
        if [[ "$vm_type" == "vm" ]] && [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            log_info "Skipping vm:$vmid (hostpci present)"
            results_text+="Skipped vm:$vmid (hostpci present)\n"
            ((total_skipped++))
            continue
        fi

        # Skip if already in HA (but remember for rule creation on PVE 9+)
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "$sid"; then
            log_info "Skipping $sid (already in HA)"
            log_debug "DEBUG: Adding already-HA resource '$sid' to RESOURCES_FOR_RULE array"
            results_text+="Skipped $sid (already in HA)\n"
            ((total_skipped++))
            RESOURCES_FOR_RULE+=("$sid")
            log_debug "DEBUG: RESOURCES_FOR_RULE array now contains: ${RESOURCES_FOR_RULE[*]}"
            continue
        fi

        # Add to HA
        if [ "$MAJOR" -ge 9 ]; then
            # PVE 9+: no --group, rules handle placement
            if ha-manager add "$sid" --state started \
                --comment "HA ${vmid} nodes=${NODES_CSV}" 2>/dev/null; then
                log_info "Added $sid to HA (v9+)"
                log_debug "DEBUG: Adding newly-added resource '$sid' to RESOURCES_FOR_RULE array"
                results_text+="Added $sid to HA\n"
                RESOURCES_FOR_RULE+=("$sid")
                log_debug "DEBUG: RESOURCES_FOR_RULE array now contains: ${RESOURCES_FOR_RULE[*]}"
                ((total_added++))
            else
                log_error "Failed to add $sid to HA"
                results_text+="Failed to add $sid\n"
            fi
        else
            # PVE 8: add to group
            if ha-manager add "$sid" --state started --group "$GROUP" \
                --comment "HA ${vmid} on ${NODES_CSV}" 2>/dev/null; then
                log_info "Added $sid to HA group \"$GROUP\""
                log_debug "DEBUG: Adding resource '$sid' to RESOURCES_FOR_RULE array (PVE 8 mode)"
                results_text+="Added $sid to HA group\n"
                RESOURCES_FOR_RULE+=("$sid")  # harmless if kept; ignored by groups
                log_debug "DEBUG: RESOURCES_FOR_RULE array now contains: ${RESOURCES_FOR_RULE[*]}"
                ((total_added++))
            else
                log_error "Failed to add $sid to HA group"
                results_text+="Failed to add $sid\n"
            fi
        fi
    done

    # DEBUG: Log rule creation decision parameters
    log_debug "DEBUG: Rule creation check - MAJOR='$MAJOR', RESOURCES_FOR_RULE count=${#RESOURCES_FOR_RULE[@]}"
    log_debug "DEBUG: RESOURCES_FOR_RULE content: ${RESOURCES_FOR_RULE[*]}"
    log_debug "DEBUG: GROUP='$GROUP', NODES_CSV='$NODES_CSV'"

    # On PVE 9+, create/update Node Affinity rule for the SIDs we actually manage
    if [ "$MAJOR" -ge 9 ]; then
        log_info "DEBUG: PVE version check passed (MAJOR=$MAJOR >= 9)"
        if [ "${#RESOURCES_FOR_RULE[@]}" -gt 0 ]; then
            log_info "DEBUG: Resources array not empty, proceeding with rule creation"
            # Build resources CSV like "vm:100,ct:200"
            local RESOURCES_CSV
            RESOURCES_CSV="$(IFS=,; echo "${RESOURCES_FOR_RULE[*]}")"
            log_info "Creating/updating node affinity rule for ${#RESOURCES_FOR_RULE[@]} resources: $RESOURCES_CSV"
            log_debug "DEBUG: About to call ensure_ha_placement with parameters:"
            log_debug "  - GROUP: '$GROUP'"
            log_debug "  - NODES_CSV: '$NODES_CSV'" 
            log_debug "  - RESOURCES_CSV: '$RESOURCES_CSV'"
            log_debug "  - RESTRICTED: '1'"
            log_debug "  - COMMENT: 'Auto-created by pve-tools'"
            
            # Map your old --restricted 1 to --strict 1 on rules
            ensure_ha_placement "$GROUP" "$NODES_CSV" "$RESOURCES_CSV" "1" "Auto-created by pve-tools"
            local ensure_result=$?
            log_debug "DEBUG: ensure_ha_placement returned exit code: $ensure_result"
        else
            log_info "DEBUG: Resources array is empty, skipping rule creation"
        fi
    else
        log_info "DEBUG: PVE version check failed (MAJOR=$MAJOR < 9), not creating node affinity rules"
    fi

    # Show results
    local summary="Operation completed:\n\n"
    summary+="Total processed: $total_processed\n"
    summary+="Successfully added: $total_added\n"
    summary+="Skipped: $total_skipped\n\n"
    summary+="Details:\n$results_text"

    dialog --keep-tite --title "HA Resources Addition Results" \
           --ok-label "OK" \
           --msgbox "$summary" 20 80

    log_info "HA resources processing completed: $total_added added, $total_skipped skipped"
}

# Select HA node (dynamic function for any node position)
select_ha_node() {
    local node_position=$1  # 1, 2, 3, etc.
    local nodes=() menu_items=()
    
    # Get available nodes
    local all_nodes
    all_nodes=$(get_all_nodes)
    if [ -z "$all_nodes" ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Split nodes into array
    readarray -t nodes <<< "$(echo "$all_nodes" | tr ' ' '\n')"
    
    if [ "${#nodes[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Build menu items excluding already selected nodes
    local i
    for ((i=0; i<${#nodes[@]}; i++)); do
        local node_already_selected=false
        # Check if this node is already in SELECTED_HA_NODES array
        for selected_node in "${SELECTED_HA_NODES[@]}"; do
            if [ "${nodes[i]}" = "$selected_node" ]; then
                node_already_selected=true
                break
            fi
        done
        
        if [ "$node_already_selected" = false ]; then
            menu_items+=("$i" "${nodes[i]}")
        fi
    done
    
    if [ "${#menu_items[@]}" -eq 0 ]; then
        dialog --msgbox "No additional nodes available for HA group." 8 50
        return 1
    fi
    
    local dialog_selected=$(mktemp)
    local ordinal
    case $node_position in
        1) ordinal="first" ;;
        2) ordinal="second" ;;
        3) ordinal="third" ;;
        4) ordinal="fourth" ;;
        5) ordinal="fifth" ;;
        6) ordinal="sixth" ;;
        7) ordinal="seventh" ;;
        8) ordinal="eighth" ;;
        *) ordinal="${node_position}th" ;;
    esac
    
    local cancel_label
    if [ $node_position -eq 1 ]; then
        cancel_label="Back to Main"
    else
        cancel_label="Back"
    fi
    
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Select ${ordinal^} HA Node"
           --ok-label "Next" --cancel-label "$cancel_label"
           --menu "Choose $ordinal PVE node for HA group \"$GROUP\":" 15 60 8)
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            if [ -n "$selected_option" ]; then
                local selected_node="${nodes[selected_option]}"
                # Store selected node in array at correct position (0-indexed)
                SELECTED_HA_NODES[$((node_position - 1))]="$selected_node"
                log_info "User selected HA node $node_position: $selected_node"
                return 0
            else
                return 1
            fi
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Main wizard function
add_vmct_to_ha_resources() {
    log_info "Starting Add VM/CT to HA Resources wizard"
    
    # Initialize wizard variables
    SELECTED_VMIDS=()
    SELECTED_HA_NODES=()  # Dynamic array for selected nodes
    current_step=1
    # Calculate total steps: number of HA nodes + 3 (confirm + select_vm/ct + execute)
    total_steps=$((number_of_pve_hosts_in_ha_cluster + 3))
    select_all_mode=false
    
    # Collect VMIDs using PCI passthrough (needed for selection display)
    readarray -t HOSTPCI_VMS < <(get_vms_with_hostpci)
    
    while true; do
        # Dynamic node selection steps (1 to number_of_pve_hosts_in_ha_cluster)
        if [ $current_step -le $number_of_pve_hosts_in_ha_cluster ]; then
            # Node selection step
            log_info "Wizard: Starting step $current_step - Select HA node $current_step"
            select_ha_node $current_step
            case $? in
                0) 
                    log_info "Wizard: Step $current_step completed successfully, moving to step $((current_step + 1))"
                    current_step=$((current_step + 1))
                    ;;
                1) 
                    if [ $current_step -eq 1 ]; then
                        log_info "User canceled first node selection"
                        return 0
                    else
                        log_info "User went back from step $current_step to step $((current_step - 1))"
                        current_step=$((current_step - 1))
                    fi
                    ;;
            esac
        else
            # Fixed steps after node selection
            case $current_step in
                $((number_of_pve_hosts_in_ha_cluster + 1)))
                    # Confirm node selection
                    confirm_ha_nodes
                    case $? in
                        0) current_step=$((current_step + 1)) ;;  # Next
                        1) current_step=$number_of_pve_hosts_in_ha_cluster ;;  # Back to last node selection
                    esac
                    ;;
                $((number_of_pve_hosts_in_ha_cluster + 2)))
                    # Select VM/CT
                    select_vm_ct_for_ha
                    case $? in
                        0) current_step=$((current_step + 1)) ;;  # Next
                        1) current_step=$((current_step - 1)) ;;  # Back
                        2) ;;                 # Stay on same step
                    esac
                    ;;
                $((number_of_pve_hosts_in_ha_cluster + 3)))
                    # Show confirmation dialog and execute
                show_confirmation_dialog
                case $? in
                    0) # Execute
                        # Ensure HA placement constraint exists (group on PVE 8, rule on PVE 9+)
                        local MAJOR
                        MAJOR="$(get_pve_major)"
                        if [ "$MAJOR" -ge 9 ]; then
                            # For PVE 9+, we'll create the rule inside process_ha_resources with actual resource list
                            log_info "PVE 9+ detected: Node affinity rule will be created after resource processing"
                        else
                            # For PVE 8, ensure group exists before processing
                            if ! get_groups | grep -qw "$GROUP"; then
                                log_info "PVE 8 Group '$GROUP' does not exist. Creating via ensure_ha_placement function"
                                # Note: resources CSV is empty for groups (not needed), but we pass it for compatibility
                                ensure_ha_placement "$GROUP" "$NODES_CSV" "" "1" "Auto-created by pve-tools"
                            fi
                        fi
                        
                        # Process resources
                        process_ha_resources
                        
                        log_info "Add VM/CT to HA Resources wizard completed"
                        return 0
                        ;;
                    1) current_step=$((current_step - 1)) ;;  # Back
                esac
                ;;
            esac
        fi
    done
}

# STORAGE VM STARTUP CONFIGURATION FUNCTIONS

# Get VMs/CTs that match the storage pattern
get_storage_vms() {
    local pattern="$storage_vm_name_contains"
    if [[ -z "$pattern" ]]; then
        pattern="$(lower "$PRODUCT")"
    fi
    local vm_ct_data=""
    
    # Get all VMs
    {
        for conf_file in $ALL_PVE_VM_CONF; do
        if [[ -f "$conf_file" ]]; then
            local vmid=$(basename "$conf_file" .conf)
            local name=""
            local onboot=""
            local startup=""
            
            # Extract VM name, onboot and startup settings
            while IFS= read -r line; do
                case "$line" in
                    name:*) name="${line#name: }" ;;
                    onboot:*) onboot="${line#onboot: }" ;;
                    startup:*) startup="${line#startup: }" ;;
                esac
            done < "$conf_file"
            
            # Check if name contains the pattern
            if [[ "$name" == *"$pattern"* ]]; then
                local start_at_boot="no"
                local order=""
                local up=""
                local down=""
                
                log_debug "get_storage_vms: Found VM $vmid ($name) - onboot='$onboot' startup='$startup'"
                # Parse onboot setting: onboot: 1 = yes, missing onboot = no
                if [[ "$onboot" == "1" ]]; then
                    start_at_boot="yes"
                fi
                
                # Parse startup settings
                if [[ -n "$startup" ]]; then
                    if [[ "$startup" =~ order=([0-9]+) ]]; then
                        order="${BASH_REMATCH[1]}"
                    fi
                    if [[ "$startup" =~ up=([0-9]+) ]]; then
                        up="${BASH_REMATCH[1]}"
                    fi
                    if [[ "$startup" =~ down=([0-9]+) ]]; then
                        down="${BASH_REMATCH[1]}"
                    fi
                fi
                
                log_debug "get_storage_vms: VM $vmid parsed - start_at_boot='$start_at_boot' order='$order' up='$up' down='$down'"
                vm_ct_data+="${vmid}|${name:-VM-$vmid}|vm|${start_at_boot}|${order:-}|${up:-}|${down:-}|$conf_file"$'\n'
            fi
        fi
        done
    } 2>/dev/null
    
    # Get all CTs
    {
        for conf_file in $ALL_PVE_CT_CONF; do
        if [[ -f "$conf_file" ]]; then
            local ctid=$(basename "$conf_file" .conf)
            local hostname=""
            local onboot=""
            local startup=""
            
            # Extract CT hostname, onboot and startup settings
            while IFS= read -r line; do
                case "$line" in
                    hostname:*) hostname="${line#hostname: }" ;;
                    onboot:*) onboot="${line#onboot: }" ;;
                    startup:*) startup="${line#startup: }" ;;
                esac
            done < "$conf_file"
            
            # Check if hostname contains the pattern
            if [[ "$hostname" == *"$pattern"* ]]; then
                local start_at_boot="no"
                local order=""
                local up=""
                local down=""
                
                # Parse onboot setting: onboot: 1 = yes, missing onboot = no
                if [[ "$onboot" == "1" ]]; then
                    start_at_boot="yes"
                fi
                
                # Parse startup settings  
                if [[ -n "$startup" ]]; then
                    if [[ "$startup" =~ order=([0-9]+) ]]; then
                        order="${BASH_REMATCH[1]}"
                    fi
                    if [[ "$startup" =~ up=([0-9]+) ]]; then
                        up="${BASH_REMATCH[1]}"
                    fi
                    if [[ "$startup" =~ down=([0-9]+) ]]; then
                        down="${BASH_REMATCH[1]}"
                    fi
                fi
                
                vm_ct_data+="${ctid}|${hostname:-CT-$ctid}|ct|${start_at_boot}|${order:-}|${up:-}|${down:-}|$conf_file"$'\n'
            fi
        fi
        done
    } 2>/dev/null
    
    echo "$vm_ct_data"
}

# Select storage VMs for startup configuration
select_storage_vms() {
    local vm_ct_data
    local select_all_mode=true  # Start with all selected
    local menu_items=()
    
    # Get storage VM/CT data
    vm_ct_data=$(get_storage_vms)
    if [[ -z "$vm_ct_data" ]]; then
        local pattern="$storage_vm_name_contains"
        if [[ -z "$pattern" ]]; then
            pattern="$(lower "$PRODUCT")"
        fi
        dialog --title "No Storage VMs Found" \
               --msgbox "No VMs or CTs found with name containing '$pattern'." 8 60
        return 1
    fi
    
    # Initialize selected list with all VMs if starting in select all mode
    if $select_all_mode; then
        storage_vm_selected=()
        while IFS='|' read -r vmid name type boot order up down conf_file; do
            if [[ -n "$vmid" ]]; then
                storage_vm_selected+=("$vmid")
            fi
        done <<< "$vm_ct_data"
    fi
    
    while true; do
        menu_items=()
        local select_label
        
        if $select_all_mode; then
            select_label="Deselect All"
        else
            select_label="Select All"
        fi
        
        # Calculate maximum VM name length for proper column alignment
        local max_name_len=8  # minimum width for "VM Name" header
        while IFS='|' read -r vmid name type boot order up down conf_file; do
            if [[ -n "$vmid" && -n "$name" ]]; then
                local name_len=${#name}
                if [[ $name_len -gt $max_name_len ]]; then
                    max_name_len=$name_len
                fi
            fi
        done <<< "$vm_ct_data"
        
        log_debug "select_storage_vms: Calculated max VM name length: $max_name_len"
        
        # Build header info using simple heredoc
        local header
        header=$(cat <<EOF
Legend:
order:  Start/shutdownorder
up:     Startup delay (sec)
down:   Shutdown timeout (sec)
EOF
)
        log_debug "select_storage_vms: Final header: '$header'"
        
        # Build menu items with formatted columns
        while IFS='|' read -r vmid name type boot order up down conf_file; do
            if [[ -n "$vmid" ]]; then
                # Format the display string with columns using calculated name width
                local name_part=$(printf "(%s)" "$name")
                local display_string=$(printf "%-*s [%s]  Start-at-boot: %-3s | order=%-3s | up=%-3s | down=%-3s" \
                    $((max_name_len + 2)) "$name_part" "$type" "$boot" "${order:-any}" "${up:-N/A}" "${down:-N/A}")
                
                # Check if this VM/CT is selected
                local selection_status="off"
                for selected_vm in "${storage_vm_selected[@]}"; do
                    if [[ "$selected_vm" == "$vmid" ]]; then
                        selection_status="on"
                        break
                    fi
                done
                
                menu_items+=("$vmid" "$display_string" "$selection_status")
            fi
        done <<< "$vm_ct_data"
        
        # Create dialog
        local product_name="$PRODUCT"
        local formatted_header="$(echo -e "$header")"
        log_debug "select_storage_vms: Header for dialog: '$header'"
        log_debug "select_storage_vms: Header length: ${#header}"
        cmd=(dialog --keep-tite --title "Setup Start-at-boot for $product_name VMs"
             --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
             --checklist "$formatted_header" 20 120 12)
        options=("${menu_items[@]}")
        
        dialog_menu
        
        case $dialog_exit_code in
            0)  # Next
                if [[ -n "$selected_option" ]]; then
                    # Parse selected VMs/CTs
                    storage_vm_selected=()
                    IFS=' ' read -ra ADDR <<< "$selected_option"
                    for vmid in "${ADDR[@]}"; do
                        storage_vm_selected+=("$vmid")
                    done
                    log_info "Selected storage VMs/CTs: ${storage_vm_selected[*]}"
                    return 0
                else
                    dialog --title "Warning" --msgbox "Please select at least one VM or CT." 8 50
                fi
                ;;
            1)  # Back
                return 1
                ;;
            3)  # Select All/Deselect All
                if $select_all_mode; then
                    # Deselect all
                    storage_vm_selected=()
                    select_all_mode=false
                else
                    # Select all
                    storage_vm_selected=()
                    while IFS='|' read -r vmid name type boot order up down conf_file; do
                        if [[ -n "$vmid" ]]; then
                            storage_vm_selected+=("$vmid")
                        fi
                    done <<< "$vm_ct_data"
                    select_all_mode=true
                fi
                ;;
        esac
    done
}

# Show startup configuration confirmation
confirm_startup_configuration() {
    log_debug "confirm_startup_configuration: Displaying values:"
    log_debug "confirm_startup_configuration: storage_vm_start_at_boot='${storage_vm_start_at_boot}'"
    log_debug "confirm_startup_configuration: storage_vm_start_shutdown_order='${storage_vm_start_shutdown_order}'"
    log_debug "confirm_startup_configuration: storage_vm_startup_deleay='${storage_vm_startup_deleay}'"
    log_debug "confirm_startup_configuration: storage_vm_shutdown_timeout='${storage_vm_shutdown_timeout}'"
    log_debug "confirm_startup_configuration: storage_vm_boot_order='${storage_vm_boot_order}'"
    
    local summary="Apply the following startup configuration to selected:\n\n"
    summary+="Start-at-boot: ${storage_vm_start_at_boot}\n"
    summary+="Start/shutdown order: ${storage_vm_start_shutdown_order}\n"
    summary+="Startup delay: ${storage_vm_startup_deleay} seconds\n"
    summary+="Shutdown timeout: ${storage_vm_shutdown_timeout} seconds\n"
    summary+="Boot order: ${storage_vm_boot_order}\n\n"
    summary+="\n\n"
    summary+="Selected: ${#storage_vm_selected[@]} items\n\n"
    
    # Get VM/CT data for name and tag information
    local vm_ct_data
    vm_ct_data=$(get_storage_vms)
    
    # Calculate maximum VM name length for proper column alignment
    local max_name_len=8  # minimum width for "VM Name" header
    while IFS='|' read -r vmid name type boot order up down conf_file; do
        if [[ -n "$vmid" && -n "$name" ]]; then
            # Check if this VM is in our selected list
            for selected_vmid in "${storage_vm_selected[@]}"; do
                if [[ "$selected_vmid" == "$vmid" ]]; then
                    local name_len=${#name}
                    if [[ $name_len -gt $max_name_len ]]; then
                        max_name_len=$name_len
                    fi
                    break
                fi
            done
        fi
    done <<< "$vm_ct_data"
    
    # Build the VM/CT list with proper alignment
    log_debug "confirm_startup_configuration: Building VM list with ${#storage_vm_selected[@]} selected VMs"
    log_debug "confirm_startup_configuration: Max name length calculated: $max_name_len"
    
    for selected_vmid in "${storage_vm_selected[@]}"; do
        local vm_name=""
        local vm_type=""
        log_debug "confirm_startup_configuration: Processing selected vmid: $selected_vmid"
        
        # Find the name and type for this vmid
        while IFS='|' read -r vmid name type boot order up down conf_file; do
            if [[ "$vmid" == "$selected_vmid" ]]; then
                vm_name="$name"
                vm_type="$type"
                log_debug "confirm_startup_configuration: Found vmid $vmid: name='$name', type='$type'"
                break
            fi
        done <<< "$vm_ct_data"
        
        # Format with proper alignment
        if [[ -n "$vm_name" ]]; then
            local name_part="($vm_name)"
            local formatted_line="$(printf "  - %-3s %-*s [%s]" "$selected_vmid" $((max_name_len + 2)) "$name_part" "$vm_type")"
            log_debug "confirm_startup_configuration: Adding formatted line: '$formatted_line'"
            summary+="$formatted_line\n"
        else
            log_debug "confirm_startup_configuration: VM name not found for vmid: $selected_vmid"
            summary+="  - $selected_vmid (name not found)\n"
        fi
    done
    
    log_debug "confirm_startup_configuration: Final summary length: ${#summary}"
    log_debug "confirm_startup_configuration: Summary content:"
    log_debug "$summary"
    
    dialog --keep-tite --title "Confirm Startup Configuration" \
           --ok-label "Confirm" --cancel-label "Cancel" \
           --yesno "$summary" 20 80
    
    return $?
}

# Apply startup settings to selected VMs/CTs
apply_startup_settings() {
    local total_processed=0
    local total_success=0
    local total_errors=0
    local results_text=""
    
    # Get VM/CT data for name information
    local vm_ct_data
    vm_ct_data=$(get_storage_vms)
    
    # Calculate max name length for formatting
    local max_name_len=8
    if [[ -n "$vm_ct_data" ]]; then
        while IFS='|' read -r vmid name type boot order up down conf_file; do
            if [[ -n "$vmid" && -n "$name" ]]; then
                local name_len=${#name}
                if [[ $name_len -gt $max_name_len ]]; then
                    max_name_len=$name_len
                fi
            fi
        done <<< "$vm_ct_data"
    fi
    
    # Helper function to get VM name and type from data
    get_vm_info() {
        local search_vmid="$1"
        local vm_name=""
        local vm_type=""
        
        if [[ -n "$vm_ct_data" ]]; then
            while IFS='|' read -r vmid name type boot order up down conf_file; do
                if [[ "$vmid" == "$search_vmid" ]]; then
                    vm_name="$name"
                    vm_type="$type"
                    break
                fi
            done <<< "$vm_ct_data"
        fi
        
        echo "$vm_name|$vm_type"
    }
    
    for vmid in "${storage_vm_selected[@]}"; do
        log_info "Processing startup configuration for VM/CT $vmid"
        local config_file=""
        local type=""
        
        # Get VM name and type information
        local vm_info
        vm_info=$(get_vm_info "$vmid")
        local vm_name=$(echo "$vm_info" | cut -d'|' -f1)
        local vm_data_type=$(echo "$vm_info" | cut -d'|' -f2)
        
        # Determine if it's a VM or CT
        if [[ -f "/etc/pve/nodes/$(hostname)/qemu-server/$vmid.conf" ]]; then
            config_file="/etc/pve/nodes/$(hostname)/qemu-server/$vmid.conf"
            type="VM"
        elif [[ -f "/etc/pve/nodes/$(hostname)/lxc/$vmid.conf" ]]; then
            config_file="/etc/pve/nodes/$(hostname)/lxc/$vmid.conf"
            type="CT"
        else
            # Try to find the config file on any node
            for node_conf in /etc/pve/nodes/*/qemu-server/"$vmid".conf; do
                if [[ -f "$node_conf" ]]; then
                    config_file="$node_conf"
                    type="VM"
                    break
                fi
            done
            if [[ -z "$config_file" ]]; then
                for node_conf in /etc/pve/nodes/*/lxc/"$vmid".conf; do
                    if [[ -f "$node_conf" ]]; then
                        config_file="$node_conf"
                        type="CT"
                        break
                    fi
                done
            fi
        fi
        
        if [[ -z "$config_file" ]]; then
            log_error "Configuration file not found for VM/CT $vmid"
            local name_part="($vm_name)"
            local formatted_result=$(printf "%-3s %-*s [%s]: ERROR - Config file not found" "$vmid" $((max_name_len + 2)) "$name_part" "$vm_data_type")
            results_text+="$formatted_result\n"
            ((total_errors++))
            ((total_processed++))
            continue
        fi
        
        # Create backup of config file
        local backup_file="${config_file}.backup.$(date +%Y%m%d_%H%M%S)"
        if ! cp "$config_file" "$backup_file" 2>/dev/null; then
            log_error "Failed to backup config file for VM/CT $vmid"
            local name_part="($vm_name)"
            local formatted_result=$(printf "%-3s %-*s [%s]: ERROR - Failed to backup config" "$vmid" $((max_name_len + 2)) "$name_part" "$vm_data_type")
            results_text+="$formatted_result\n"
            ((total_errors++))
            ((total_processed++))
            continue
        fi
        
        # Prepare new configuration lines
        local boot_line=""
        local startup_line=""
        
        log_debug "apply_startup_settings: Applying values for VM/CT $vmid:"
        log_debug "apply_startup_settings: storage_vm_start_at_boot='${storage_vm_start_at_boot}'"
        log_debug "apply_startup_settings: storage_vm_start_shutdown_order='${storage_vm_start_shutdown_order}'"
        log_debug "apply_startup_settings: storage_vm_startup_deleay='${storage_vm_startup_deleay}'"
        log_debug "apply_startup_settings: storage_vm_shutdown_timeout='${storage_vm_shutdown_timeout}'"
        log_debug "apply_startup_settings: storage_vm_boot_order='${storage_vm_boot_order}'"
        
        if [[ "$storage_vm_start_at_boot" == "yes" ]]; then
            boot_line="onboot: 1"
        else
            boot_line=""  # Missing onboot line means start at boot = no
        fi
        
        startup_line="startup: order=${storage_vm_start_shutdown_order},up=${storage_vm_startup_deleay},down=${storage_vm_shutdown_timeout}"
        
        log_debug "apply_startup_settings: boot_line='$boot_line'"
        log_debug "apply_startup_settings: startup_line='$startup_line'"
        
        # Read current config and prepare new one
        local temp_config=$(mktemp)
        local boot_updated=false
        local startup_updated=false
        
        while IFS= read -r line; do
            if [[ "$line" =~ ^onboot: ]]; then
                # Only add onboot line if we want start at boot = yes
                if [[ -n "$boot_line" ]]; then
                    echo "$boot_line" >> "$temp_config"
                fi
                boot_updated=true
            elif [[ "$line" =~ ^startup: ]]; then
                echo "$startup_line" >> "$temp_config"
                startup_updated=true
            else
                echo "$line" >> "$temp_config"
            fi
        done < "$config_file"
        
        # Add missing lines if not updated
        if ! $boot_updated && [[ -n "$boot_line" ]]; then
            echo "$boot_line" >> "$temp_config"
        fi
        if ! $startup_updated; then
            echo "$startup_line" >> "$temp_config"
        fi
        
        # Replace the original config file
        if mv "$temp_config" "$config_file" 2>/dev/null; then
            log_info "Successfully updated startup configuration for VM/CT $vmid"
            
            # Apply boot order for VMs only (not containers)
            if [[ "$type" == "VM" ]]; then
                log_debug "apply_startup_settings: Applying boot order '$storage_vm_boot_order' for VM $vmid"
                
                # Find which node the VM is on
                local vm_node vm_node_ip
                vm_node=$(on_which_host_vmct_is "$vmid")
                if [[ -z "$vm_node" ]]; then
                    log_error "Cannot find node for VM $vmid"
                    local formatted_result=$(printf "%-3s %-*s [%s]: ERROR - Cannot find node" "$vmid" $((max_name_len + 2)) "($vm_name)" "$vm_data_type")
                    results_text+="$formatted_result\n"
                    ((total_errors++))
                    continue
                fi
                
                vm_node_ip=$(host_ip "$vm_node")
                if [[ -z "$vm_node_ip" ]]; then
                    log_error "Cannot resolve IP for node '$vm_node'"
                    local formatted_result=$(printf "%-3s %-*s [%s]: ERROR - Cannot resolve node IP" "$vmid" $((max_name_len + 2)) "($vm_name)" "$vm_data_type")
                    results_text+="$formatted_result\n"
                    ((total_errors++))
                    continue
                fi
                
                # Set boot order using SSH
                if ssh root@"$vm_node_ip" "qm set '$vmid' --boot 'order=$storage_vm_boot_order'" < /dev/null >/dev/null 2>&1; then
                    log_info "Successfully set boot order to '$storage_vm_boot_order' for VM $vmid on node $vm_node"
                else
                    log_error "Failed to set boot order for VM $vmid on node $vm_node"
                fi
                
                # Delete ide2 (typically the CD-ROM) if it exists using SSH
                if ssh root@"$vm_node_ip" "qm set '$vmid' --delete ide2" < /dev/null >/dev/null 2>&1; then
                    log_info "Successfully removed ide2 device from VM $vmid on node $vm_node"
                else
                    log_debug "No ide2 device to remove or failed to remove ide2 for VM $vmid on node $vm_node"
                fi
            fi
            
            local name_part="($vm_name)"
            local formatted_result=$(printf "%-3s %-*s [%s]: SUCCESS" "$vmid" $((max_name_len + 2)) "$name_part" "$vm_data_type")
            results_text+="$formatted_result\n"
            ((total_success++))
        else
            log_error "Failed to update config file for VM/CT $vmid"
            local name_part="($vm_name)"
            local formatted_result=$(printf "%-3s %-*s [%s]: ERROR - Failed to update config" "$vmid" $((max_name_len + 2)) "$name_part" "$vm_data_type")
            results_text+="$formatted_result\n"
            # Restore backup
            cp "$backup_file" "$config_file" 2>/dev/null
            ((total_errors++))
        fi
        
        ((total_processed++))
        rm -f "$temp_config"
    done
    
    # Show results
    local summary="Startup configuration completed:\n\n"
    summary+="Total processed: $total_processed\n"
    summary+="Successfully configured: $total_success\n"
    summary+="Errors: $total_errors\n\n"
    summary+="Details:\n$results_text"
    
    dialog --keep-tite --title "Startup Configuration Results" \
           --ok-label "OK" \
           --msgbox "$summary" 20 80
    
    log_info "Startup configuration completed: $total_success configured, $total_errors errors"
}

# Configure startup settings wizard step
configure_startup_settings_wizard() {
    log_debug "configure_startup_settings_wizard: Displaying values:"
    log_debug "configure_startup_settings_wizard: storage_vm_start_at_boot='${storage_vm_start_at_boot}'"
    log_debug "configure_startup_settings_wizard: storage_vm_start_shutdown_order='${storage_vm_start_shutdown_order}'"
    log_debug "configure_startup_settings_wizard: storage_vm_startup_deleay='${storage_vm_startup_deleay}'"
    log_debug "configure_startup_settings_wizard: storage_vm_shutdown_timeout='${storage_vm_shutdown_timeout}'"
    log_debug "configure_startup_settings_wizard: storage_vm_boot_order='${storage_vm_boot_order}'"
    
    local summary
    summary=$(cat <<EOF
Note: Startup settings apply only to the $PRODUCT VM. 
All other VMs are managed by Proxmox VE HA, 
which uses its own logic to control startup/shutdown based on HA settings, 
not the VM's configuration.

Current startup configuration:

Start-at-boot: ${storage_vm_start_at_boot}
Start/shutdown order: ${storage_vm_start_shutdown_order}
Startup delay: ${storage_vm_startup_deleay} seconds
Shutdown timeout: ${storage_vm_shutdown_timeout} seconds
Boot order: ${storage_vm_boot_order}

These settings will be applied to all selected VMs.
EOF
)

    dialog --keep-tite --title "$PRODUCT Startup Configuration Settings" \
           --yes-label "Continue" --no-label "Back to Main" --extra-button --extra-label "Modify Settings" \
           --yesno "$summary" 20 100
    
    local exit_code=$?
    
    case $exit_code in
        0)  # Continue button - use current settings
            return 0
            ;;
        1)  # Back button
            return 1
            ;;
        3)  # Modify Settings button
            if setup_storage_vm_dialog; then
                # Settings were modified, show confirmation again
                configure_startup_settings_wizard
                return $?
            else
                # User cancelled the settings dialog
                configure_startup_settings_wizard
                return $?
            fi
            ;;
        *)
            return 1
            ;;
    esac
}

# Main wizard function for storage VM startup configuration
setup_storage_vm_startup_wizard() {
    log_info "Starting Setup Start-at-boot for $PRODUCT VMs wizard"
    
    # Check if running as root (required for config file modification)
    if [[ $EUID -ne 0 ]]; then
        dialog --title "Error" --msgbox "This function must be run as root to modify VM/CT configuration files." 8 70
        return 1
    fi
    
    # Initialize wizard variables
    storage_vm_selected=()
    
    while true; do
        # Step 1: Configure startup settings
        if ! configure_startup_settings_wizard; then
            return 0  # Back to main menu
        fi
        
        # Step 2: Select VMs/CTs
        if ! select_storage_vms; then
            continue  # Go back to configuration
        fi
        
        # Step 3: Confirm configuration
        if ! confirm_startup_configuration; then
            continue  # Go back to VM selection
        fi
        
        # Step 4: Apply settings
        apply_startup_settings
        
        return 0  # Back to main menu
    done
}

# MAIN SCRIPT EXECUTION

# Cleanup background processes and temporary files
cleanup_background_processes() {
    if [ -n "$PVE_CONFIG_BACKUP_PID" ]; then
        if kill -0 "$PVE_CONFIG_BACKUP_PID" 2>/dev/null; then
            log_info "Terminating background pve-config-backup check (PID: $PVE_CONFIG_BACKUP_PID)"
            kill "$PVE_CONFIG_BACKUP_PID" 2>/dev/null
        fi
    fi
    
    # Clean up temporary files
    rm -f /tmp/pve-config-backup-status /tmp/pve-config-backup-check.log
}

# Install/update PveToolsDiskMigration Perl module
install_disk_migration_module() {
    log_info "Checking PveToolsDiskMigration module installation"
    
    local module_path="/usr/share/perl5/PVE/API2/PveToolsDiskMigration.pm"
    local module_changed=false
    
    # Create the embedded module content
    local module_content
    module_content=$(cat << 'EOF_PERL_MODULE'
package PVE::API2::PveToolsDiskMigration;

use strict;
use warnings;

use POSIX qw(strftime);
use JSON;
use File::Path qw(make_path);
use File::Basename;

use PVE::SafeSyslog;
use PVE::Tools;
use PVE::Cluster;
# No REST handler needed for direct function calls
use PVE::RPCEnvironment;
use PVE::API2::Qemu;
use PVE::Storage;
use PVE::QemuServer;
use PVE::QemuConfig;
use PVE::INotify;
use Try::Tiny;

# Direct function call interface - no REST handler needed

# --------------------------------------------------------------------------
# Constructor
# --------------------------------------------------------------------------

sub new {
    my ($class, %p) = @_;

    my $self = {
        vmid          => $p{vmid},
        src_storage   => $p{src_storage},
        dst_storage   => $p{dst_storage},
        mode          => $p{mode},
        debug_level   => $p{debug_level},
        debug_logfile => $p{debug_logfile},
        authuser      => $p{authuser},
        state         => {
            phase        => 'INIT',
            completed    => 0,
            failed       => 0,
            current_file => undef,
            errors       => [],
        },
        start_time => time(),
    };

    bless $self, $class;
    return $self;
}

# --------------------------------------------------------------------------
# High‑level workflow
# --------------------------------------------------------------------------

sub execute_migration_workflow {
    my ($self) = @_;

    try {
        $self->debug_log(2, 'INIT', "Starting migration for VM $self->{vmid}");
        $self->phase_initialize();
        $self->phase_migrate_vm();
        $self->phase_verify_migration();
        $self->phase_cleanup();
        $self->debug_log(1, 'COMPLETE', 'Migration finished');
        print "### PVE::Worker::update_progress 100 Migration completed\n";
    } catch {
        my $err = $_;
        $self->debug_log(1, 'ERROR', "Migration failed: $err");
        $self->{state}->{phase} = 'ERROR';
        $self->save_state();
        die $err;
    };
}

# --------------------------------------------------------------------------
# Phase 1 – Initialise
# --------------------------------------------------------------------------

sub phase_initialize {
    my ($self) = @_;

    $self->{state}->{phase} = 'INIT';
    print "### PVE::Worker::update_progress 5 Initialising\n";

    my $src_path = $self->get_storage_path($self->{src_storage});
    my $dst_path = $self->get_storage_path($self->{dst_storage});
    die "Cannot resolve storage paths\n" unless $src_path && $dst_path;

    my $vmid = $self->{vmid};
    if (   $self->{mode} eq 'preserve-snapshots'
        && $self->get_vm_status($vmid) eq 'running')
    {
        die "VM $vmid is running – cannot preserve snapshots\n";
    }

    $self->{state}->{files} =
      $self->discover_vm_files( $vmid, $src_path );
    $self->debug_log(3, 'INIT',
        "Found " . @{ $self->{state}->{files} } . " files to migrate");
    $self->save_state();
}

# --------------------------------------------------------------------------
# Phase 2 – Migrate
# --------------------------------------------------------------------------

sub phase_migrate_vm {
    my ($self) = @_;

    $self->{state}->{phase} = 'MIGRATE';
    print "### PVE::Worker::update_progress 10 Migrating disks\n";

    my $vmid = $self->{vmid};

    try {
        if ( $self->{mode} eq 'preserve-snapshots' ) {
            $self->migrate_vm_preserve_snapshots($vmid);
        }
        else {
            $self->migrate_vm_standard($vmid);
        }
        $self->{state}->{completed} = 1;
        print "### PVE::Worker::update_progress 80 VM $vmid migrated\n";
    }
    catch {
        $self->{state}->{failed} = 1;
        die $_;
    };

    $self->save_state();
}

# --------------------------------------------------------------------------
# Phase 3 – Verify
# --------------------------------------------------------------------------

sub phase_verify_migration {
    my ($self) = @_;

    $self->{state}->{phase} = 'VERIFY';
    print "### PVE::Worker::update_progress 85 Verifying\n";

    my $cfg = $self->find_vm_config( $self->{vmid} ) or return;
    my $txt = $self->read_file($cfg);
    if ( $txt =~ /$self->{dst_storage}:/ ) {
        $self->debug_log(3, 'VERIFY', 'Configuration looks OK');
    }
    else {
        $self->debug_log(2, 'VERIFY', 'Configuration check failed');
    }
}

# --------------------------------------------------------------------------
# Phase 4 – Cleanup
# --------------------------------------------------------------------------

sub phase_cleanup {
    my ($self) = @_;

    $self->{state}->{phase} = 'CLEANUP';
    print "### PVE::Worker::update_progress 95 Finalising\n";

    my $dur = time() - $self->{start_time};
    $self->debug_log(2, 'CLEANUP', "Duration ${dur}s");
}

# --------------------------------------------------------------------------
# Preserve‑snapshot migration of a single VM
# --------------------------------------------------------------------------

sub migrate_vm_preserve_snapshots {
    my ( $self, $vmid ) = @_;

    # Lock VM configuration to prevent user from starting/modifying VM during migration
    my $lock_acquired = 0;
    my $lock_timeout = 300; # 5 minutes timeout for acquiring lock
    
    eval {
        # Try to acquire VM config lock with timeout
        PVE::QemuConfig->lock_config_full($vmid, $lock_timeout, sub {
            $lock_acquired = 1;
            $self->debug_log(1, 'LOCK', "VM $vmid configuration locked for migration");
            
            # Verify VM is still powered off after acquiring lock
            if ( $self->get_vm_status($vmid) eq 'running' ) {
                die "VM $vmid was started after lock acquisition - aborting migration for safety\n";
            }
            
            # Perform the actual migration with VM locked
            for my $fi ( @{ $self->{state}->{files} } ) {
                $self->copy_vm_file( $vmid, $fi );
            }
            $self->update_vm_configuration($vmid);
            
            $self->debug_log(1, 'LOCK', "VM $vmid migration completed, releasing lock");
        });
    };
    
    if ($@) {
        my $error = $@;
        $self->debug_log(1, 'ERROR', "Migration failed for VM $vmid: $error");
        
        # Provide user-friendly error messages
        if ($error =~ /timeout/i) {
            die "Failed to acquire VM $vmid configuration lock (timeout after ${lock_timeout}s). VM may be in use by another operation.\n";
        } elsif ($error =~ /was started after lock/i) {
            die "VM $vmid was started during migration setup. Migration aborted for data safety.\n";
        } else {
            die "Migration failed: $error\n";
        }
    }
    
    unless ($lock_acquired) {
        die "Failed to acquire VM $vmid configuration lock. VM may be locked by another operation.\n";
    }
}

# --------------------------------------------------------------------------
# Standard migration (delete snapshots, use qm move_disk)
# --------------------------------------------------------------------------

sub migrate_vm_standard {
    my ( $self, $vmid ) = @_;

    $self->delete_vm_snapshots($vmid);
    my $disks = $self->get_vm_disks($vmid);
    for my $d ( @$disks ) {
        $self->move_disk_standard( $vmid, $d );
    }
}

# --------------------------------------------------------------------------
# Copy a single file with its own worker (GUI description set)
# --------------------------------------------------------------------------

sub copy_vm_file {
    my ( $self, $vmid, $fi ) = @_;

    my $dst_root = $self->get_storage_path( $self->{dst_storage} );
    my $dst_dir  = "$dst_root/images/$vmid";
    make_path($dst_dir) unless -d $dst_dir;

    my $name = $fi->{filename};

    $name =~ s/^vm-\d+/vm-$vmid/ if $name =~ /^vm-\d+/;
    $name = "$vmid.conf" if $name =~ /^\d+\.conf$/;

    my $dst_file = "$dst_dir/$name";

    my $rpcenv     = PVE::RPCEnvironment::get();
    my $disk_descr = "VM $vmid - Disk migrate";

    my $upid = $rpcenv->fork_worker(
        'vm-disk-migrate',
        $vmid,
        $self->{authuser},
        sub {
            my $cmd = [
                'rsync', '-rlptv', '-P', '--timeout=300', '--partial',
                '--partial-dir=.rsync-partial', '--no-owner', '--no-group', 
                $fi->{src_path}, $dst_file
            ];
            my $exit_code = system(@$cmd);
            die "rsync failed with exit code: " . ($exit_code >> 8) if $exit_code != 0;
            die "size mismatch\n" unless -s $fi->{src_path} == -s $dst_file;
        },
        undef,
        $disk_descr,
    );

    $self->wait_for_task($upid);
}

# --------------------------------------------------------------------------
# Generic helpers (mostly unchanged)
# --------------------------------------------------------------------------

sub debug_log {
    my ( $self, $lvl, $phase, $msg, $ctx ) = @_;
    return if $lvl > $self->{debug_level};
    my $ts = strftime "%F %T", localtime;
    my $f  = $ctx && $ctx->{file} ? $ctx->{file} : '';
    my $ln = "[$ts] [L$lvl] [$phase] [VM$self->{vmid}] [$f] $msg\n";
    print $ln;
    if ( open my $fh, '>>', $self->{debug_logfile} ) {
        print $fh $ln;
        close $fh;
    }
}

sub save_state {
    my ($self) = @_;
    my $file = "/tmp/pve-migration-state-$self->{vmid}.json";
    if ( open my $fh, '>', $file ) {
        print $fh encode_json( $self->{state} );
        close $fh;
    }
}

sub get_storage_path {
    my ( $self, $sid ) = @_;
    my $path;
    eval {
        my $cfg  = PVE::Storage::config();
        my $scfg = PVE::Storage::storage_config( $cfg, $sid );
        $path = $scfg->{path} if $scfg && $scfg->{path};
    };
    return $path;
}

sub find_vm_config {
    my ( $self, $vmid ) = @_;
    for my $p (
        "/etc/pve/qemu-server/$vmid.conf",
        "/etc/pve/nodes/" . PVE::INotify::nodename() . "/qemu-server/$vmid.conf"
      )
    {
        return $p if -f $p;
    }
    return undef;
}

sub get_vm_status {
    my ( $self, $vmid ) = @_;
    eval { ( PVE::QemuServer::vmstatus($vmid) )->{$vmid}->{status} }
      || 'unknown';
}

sub discover_vm_files {
    my ( $self, $vmid, $src_root ) = @_;
    my @list;
    my $cfgfile = $self->find_vm_config($vmid) or return \@list;
    my $cfg     = $self->read_file($cfgfile);

    my %folders;
    for my $l ( split /\n/, $cfg ) {
        next unless $l =~ /^(virtio|sata|scsi|ide)\d+:\s*(.+)$/;
        my $ds = $2;
        if ( $ds =~ /^$self->{src_storage}:(\d+)\/(.+)$/ ) {
            $folders{$1} = 1;
        }
    }

    for my $f ( keys %folders ) {
        my $dir = "$src_root/images/$f";
        next unless -d $dir;
        opendir my $dh, $dir or next;
        while ( my $file = readdir $dh ) {
            next if $file =~ /^\./;
            next if $file =~ /\.conf$/;
            next unless -f "$dir/$file";
            push @list,
              {
                filename   => $file,
                src_path   => "$dir/$file",
                size       => -s "$dir/$file",
              };
        }
        closedir $dh;
    }
    return \@list;
}

sub wait_for_task {
    my ( $self, $upid ) = @_;
    my $timeout = 1800;
    my $int     = 5;
    my $cnt     = $timeout / $int;
    while ( $cnt-- ) {
        my $st = PVE::Tools::upid_read_status($upid);
        return if defined $st && $st eq 'OK';
        die "task failed\n" if defined $st && $st ne 'OK';
        sleep $int;
    }
    die "task timeout\n";
}

sub update_vm_configuration {
    my ( $self, $vmid ) = @_;
    my $file = $self->find_vm_config($vmid) or return;
    my $bak  = "$file.bak." . time();
    system( 'cp', $file, $bak ) == 0 or die "backup failed\n";

    my $txt   = $self->read_file($file);
    my $out   = '';
    my $chg   = 0;
    my $src   = $self->{src_storage};
    my $dst   = $self->{dst_storage};

    for my $l ( split /\n/, $txt ) {
        if ( $l =~ /^((?:virtio|sata|scsi|ide)\d+):\s*$src:(\d+)\/([^,]+)(.*)$/ )
        {
            my ( $dev, undef, $fn, $rest ) = ( $1, $2, $3, $4 );
            $fn =~ s/^vm-\d+/vm-$vmid/;
            $l = "$dev: $dst:$vmid/$fn$rest";
            $chg++;
        }
        elsif ( $l =~ /^vmstate:\s*$src:(\d+)\/(.+)$/ ) {
            my $fn = $2;
            $fn =~ s/^vm-\d+/vm-$vmid/;
            $l = "vmstate: $dst:$vmid/$fn";
            $chg++;
        }
        $out .= "$l\n";
    }

    $self->write_file( $file, $out ) if $chg;
}

sub delete_vm_snapshots {
    my ( $self, $vmid ) = @_;
    my $conf  = PVE::QemuConfig->load_config($vmid);
    my $snaps = $conf->{snapshots} || {};
    for my $sn ( keys %$snaps ) {
        next if $sn eq 'current';
        PVE::QemuServer::snapshot_delete( $vmid, $sn, 1 );
    }
}

sub get_vm_disks {
    my ( $self, $vmid ) = @_;
    my @d;
    my $c = PVE::QemuConfig->load_config($vmid);
    for my $k ( keys %$c ) {
        next unless $k =~ /^(virtio|sata|scsi|ide)\d+$/;
        push @d, $k if $c->{$k} =~ /^$self->{src_storage}:/;
    }
    return \@d;
}

sub move_disk_standard {
    my ( $self, $vmid, $disk ) = @_;
    my $rpc = PVE::RPCEnvironment::get();
    my $upid =
      $rpc->fork_worker(
        'qmmove', $vmid, $self->{authuser},
        sub {
            PVE::QemuServer::vm_move_disk( $vmid, $disk, $self->{dst_storage},
                undef, { delete => 0 } );
        }
      );
    $self->wait_for_task($upid);
}

# --------------------------------------------------------------------------
# file helpers
# --------------------------------------------------------------------------

sub read_file {
    my ( undef, $f ) = @_;
    open my $fh, '<', $f or die "cannot read $f: $!";
    local $/;
    my $txt = <$fh>;
    close $fh;
    return $txt;
}

sub write_file {
    my ( undef, $f, $txt ) = @_;
    open my $fh, '>', $f or die "cannot write $f: $!";
    print $fh $txt;
    close $fh;
}

1;
EOF_PERL_MODULE
)
    
    # Check if module exists and compare content
    if [ -f "$module_path" ]; then
        local current_content
        current_content=$(cat "$module_path" 2>/dev/null)
        
        if [ "$current_content" != "$module_content" ]; then
            log_info "PveToolsDiskMigration module needs update"
            module_changed=true
        else
            log_debug "PveToolsDiskMigration module is up to date"
        fi
    else
        log_info "PveToolsDiskMigration module not found, will install"
        module_changed=true
    fi
    
    # Install/update module if needed
    if [ "$module_changed" = true ]; then
        log_info "Installing/updating PveToolsDiskMigration module"
        
        # Create backup of existing module if present
        if [ -f "$module_path" ]; then
            local backup_path="${module_path}.bak.$(date +%Y%m%d-%H%M%S)"
            cp "$module_path" "$backup_path" 2>/dev/null
            log_info "Created backup: $backup_path"
        fi
        
        # Write the new module
        if echo "$module_content" > "$module_path"; then
            log_info "PveToolsDiskMigration module installed successfully"
        else
            log_error "Failed to install PveToolsDiskMigration module"
            return 1
        fi
    fi
    
    # Module installed successfully - ready for direct function calls (no API registration needed)
    log_info "PveToolsDiskMigration module ready for direct function calls"
    
    # Verify module is loadable
    if perl -e "use lib '/usr/share/perl5'; use PVE::API2::PveToolsDiskMigration;" 2>/dev/null; then
        log_info "PveToolsDiskMigration module verified successfully"
    else
        log_error "PveToolsDiskMigration module failed verification"
        return 1
    fi
    
    return 0
}

# Main script entry point
main() {
    # Initialize logging system first
    if ! init_logging; then
        echo "Error: Failed to initialize logging system" >&2
        exit 1
    fi
    
    log_info "Starting PVE Tools script"
    
    # Initialize debug error capture system if in debug mode
    init_debug_error_capture
    
    # Check for dialog dependency and install if missing
    check_for_dialog_and_install_if_missing
    
    # Initialize configuration
    if ! init_config; then
        log_error "Failed to initialize configuration"
        exit 1
    fi
    
    # Install/update PveToolsDiskMigration Perl module
    install_disk_migration_module
    
    # Check if any NFS storage exists
    if ! check_nfs_storage_exists; then
        dialog --title "No NFS Storage Found" \
               --msgbox "No NFS storage found in Proxmox VE. Please connect one first." \
               8 60 \
               --ok-label "Exit"
        log_info "Script terminated: No NFS storage found"
        exit 0
    fi
    
    # Build storage to IP mapping table
    build_storage_ip_map
    
    # Update connected storage servers information
    update_connected_storage_servers
    
    log_info "PVE Tools initialized"
    
    # Check for updates
    if check_for_updates; then
        prompt_for_update
    fi
    
    # Check for pve-config-backup updates
    check_and_update_pve_config_backup
    
    # Start pve-config-backup check in background
    PVE_CONFIG_BACKUP_STATUS="checking"
    PVE_CONFIG_BACKUP_RESULT=""
    (
        ensure_pve_config_backup_running > /tmp/pve-config-backup-check.log 2>&1
        echo "COMPLETED:$(date '+%H:%M:%S')" > /tmp/pve-config-backup-status
    ) &
    PVE_CONFIG_BACKUP_PID=$!
    
    log_info "Started pve-config-backup check in background (PID: $PVE_CONFIG_BACKUP_PID)"
    
    # Setup cleanup on exit
    trap 'cleanup_background_processes' EXIT
    
    # Main application loop
    clear
    while true; do
        main_menu
    done
}

# Only run main if this script is executed directly (not sourced)
if [ "${BASH_SOURCE[0]}" = "${0}" ]; then
    # Handle version argument before any terminal operations
    if [ "$1" = "version" ]; then
        VERSION_MODE=true
        echo "$VERSION"
        exit 0
    fi
    
    # Handle test-update argument for testing update functionality
    if [ "$1" = "--test-update" ]; then
        TEST_UPDATE_MODE=true
        # Initialize logging for test
        init_logging
        log_info "Running in TEST UPDATE MODE"
        
        # Test with a fake old version to see multiple commits
        echo "Testing update check with simulated old version..."
        
        # Temporarily set old version for testing
        OLD_VERSION="$VERSION"
        OLD_DATE="$RELEASE_DATE"
        VERSION="0.1"   # Simulate much older version  
        RELEASE_DATE="2025-08-04"  # Same as your original test
        
        echo "Simulated current version: $VERSION ($RELEASE_DATE)"
        echo "Actual version: $OLD_VERSION ($OLD_DATE)"
        echo ""
        echo "Fetching commits since simulated version..."
        
        # Test the function
        commits=$(get_commits_since_version "$VERSION" "$RELEASE_DATE" 2>&1)
        
        if [ -n "$commits" ]; then
            echo "Successfully retrieved commit history:"
            echo "=================================="
            echo -e "$commits"
            echo "=================================="
        else
            echo "No commits retrieved (this might be an error)"
        fi
        
        # Restore actual version
        VERSION="$OLD_VERSION"
        RELEASE_DATE="$OLD_DATE"
        
        echo ""
        echo "Test completed. Check /var/log/pve-tools/pve-tools.log for debug output."
        exit 0
    fi
    
    main "$@"
fi
