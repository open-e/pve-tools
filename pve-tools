#!/bin/bash
#    Copyright (c) 2025 Open-E, Inc.
#    All Rights Reserved.
#
#    This file is licensed under the GNU General Public License, Version 3.0 (the "License");
#    you may not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         https://www.gnu.org/licenses/gpl-3.0
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#    GNU General Public License for more details.

export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8
export NCURSES_NO_UTF8_ACS=1

# GLOBAL VARIABLES

# Version information
VERSION="0.20"
RELEASE_DATE="2025-07-19"

# Script mode flag
VERSION_MODE=false

# Product information
VENDOR="Open-E"
PRODUCT="JovianDSS"

CONFIG_DIR="/etc/$(basename "$0" .sh)"
CONFIG_FILE="$CONFIG_DIR/config.conf"
LOGS_DIR="/var/log/$(basename "$0" .sh)"
LOG_FILE="$LOGS_DIR/$(basename "$0" .sh).log"

# Script state variables
selected_confs=()  # Array of selected config files
selected_host=""
selected_host_index=""
current_step=1
total_steps=3
select_all_mode=false
storage_discovery_done=false
cached_config_list=()  # Cache config list to avoid repeated discovery

# Conflict resolution variables
new_vmct_id=""
conflict_selected_host=""

# API configuration
rest_api_user=admin
rest_api_password=admin
rest_api_port=82
selected_nfs_ip=""

# Storage to IP mapping table
declare -A storage_ip_map

# NFS wizard variables
selected_pool=""
selected_dataset=""
selected_snapshot=""
selected_nfs_ip_index=""
selected_pool_index=""
selected_dataset_index=""
selected_snapshot_index=""
nfs_current_step=1
nfs_total_steps=5

# NFS tuning variables
selected_nfs_storages=""
selected_nfs_nconnect=""
selected_nfs_sync=""

# Disk type constants
DISK_TYPES="(scsi|ide|sata|virtio|efidisk|tpmstate|nvme)"
DISK_PATTERN="^${DISK_TYPES}[0-9]+:[^:]+:[0-9]+/vm"

# CLEANUP FUNCTIONS

# Cleanup function to restore terminal settings and clear screen
cleanup_and_exit() {
    # Skip cleanup if we're just showing version
    if [ "$VERSION_MODE" = true ]; then
        exit 0
    fi
    
    # Kill any background dialog processes
    pkill -f "dialog.*Please wait" 2>/dev/null || true
    
    # Clear the screen
    clear
    
    # Reset terminal settings
    reset
    
    # Show cursor (in case it was hidden)
    tput cnorm 2>/dev/null || true
    
    # Restore terminal to normal mode
    stty sane 2>/dev/null || true
    
    log_info "Script cleanup completed - terminal restored"
    
    exit 0
}

# Setup trap handlers for cleanup
trap cleanup_and_exit EXIT
trap cleanup_and_exit SIGINT
trap cleanup_and_exit SIGTERM
trap cleanup_and_exit SIGTSTP

# STORAGE IP MAPPING

# Check if any NFS storage exists in Proxmox VE
check_nfs_storage_exists() {
    log_debug "check_nfs_storage_exists: Checking for NFS storage in /etc/pve/storage.cfg"
    
    if [[ ! -f "/etc/pve/storage.cfg" ]]; then
        log_error "check_nfs_storage_exists: /etc/pve/storage.cfg not found"
        return 1
    fi
    
    # Check if any NFS storage entries exist
    if grep -q "^nfs:" /etc/pve/storage.cfg; then
        log_debug "check_nfs_storage_exists: NFS storage found"
        return 0
    else
        log_debug "check_nfs_storage_exists: No NFS storage found"
        return 1
    fi
}

# Build storage name to IP address mapping table
build_storage_ip_map() {
    log_debug "build_storage_ip_map: Building storage to IP mapping table"
    
    # Clear existing mapping
    storage_ip_map=()
    
    # Read storage.cfg and extract NFS storage entries
    while IFS= read -r line; do
        if [[ "$line" =~ ^nfs:[[:space:]]*([^[:space:]]+) ]]; then
            local storage_name="${BASH_REMATCH[1]}"
            local server_ip=""
            
            # Read the next few lines to find the server IP
            while IFS= read -r config_line; do
                if [[ "$config_line" =~ ^[[:space:]]*server[[:space:]]+([^[:space:]]+) ]]; then
                    server_ip="${BASH_REMATCH[1]}"
                    break
                elif [[ "$config_line" =~ ^[[:space:]]*$ ]] || [[ "$config_line" =~ ^[a-zA-Z]+: ]]; then
                    # Empty line or start of new section, stop looking
                    break
                fi
            done
            
            if [[ -n "$server_ip" ]]; then
                storage_ip_map["$storage_name"]="$server_ip"
                log_debug "build_storage_ip_map: Mapped storage '$storage_name' to IP '$server_ip'"
            fi
        fi
    done < /etc/pve/storage.cfg
    
    log_info "build_storage_ip_map: Built mapping for ${#storage_ip_map[@]} storage entries"
}

# Get IP address for a specific storage name
get_storage_ip() {
    local storage_name="$1"
    
    if [[ -z "$storage_name" ]]; then
        log_error "get_storage_ip: storage_name is required"
        return 1
    fi
    
    if [[ -n "${storage_ip_map[$storage_name]}" ]]; then
        local ip="${storage_ip_map[$storage_name]}"
        log_debug "get_storage_ip: Found IP '$ip' for storage '$storage_name'"
        echo "$ip"
        return 0
    else
        log_error "get_storage_ip: No IP mapping found for storage '$storage_name'"
        return 1
    fi
}

# LOGGING FUNCTIONS

# Initialize logging system
init_logging() {
    if [ ! -d "$LOGS_DIR" ]; then
        mkdir -p "$LOGS_DIR" || {
            echo "Error: Cannot create log directory $LOGS_DIR" >&2
            return 1
        }
    fi
    
    if [ ! -f "$LOG_FILE" ]; then
        touch "$LOG_FILE" || {
            echo "Error: Cannot create log file $LOG_FILE" >&2
            return 1
        }
    fi
    
    return 0
}

# Base logging function with timestamp
log() {
    local level="$1"
    local message="$2"
    local timestamp
    timestamp=$(date '+[%Y-%m-%d %H:%M:%S]')
    
    echo "${timestamp} ${level}: ${message}" >> "$LOG_FILE"
}

# Log info message
log_info() {
    if [[ -f "$LOG_FILE" ]]; then
        log "INFO" "$1"
    fi
}

# Log error message
log_error() {
    if [[ -f "$LOG_FILE" ]]; then
        log "ERROR" "$1"
    fi
}

# Log debug message
log_debug() {
    if [[ -f "$LOG_FILE" ]]; then
        log "DEBUG" "$1"
    fi
}

# DIALOG DEPENDENCY CHECK

# Check for dialog and install if missing
check_for_dialog_and_install_if_missing() {
    log_info "Checking for dialog dependency"
    
    # Check if dialog is installed
    if command -v dialog >/dev/null 2>&1; then
        log_info "Dialog is already installed"
        return 0
    fi
    
    log_info "Dialog not found, attempting to install"
    echo "Installing missing dialog package. Please wait..."
    
    # Update package list
    if ! apt-get update >/dev/null 2>&1; then
        echo "Error: Failed to update package list. Please check your network connection." >&2
        log_error "Failed to update package list"
        exit 1
    fi
    
    # Install dialog
    if ! apt-get install -y dialog >/dev/null 2>&1; then
        echo "Error: Failed to install dialog package. Please install it manually: apt-get install dialog" >&2
        log_error "Failed to install dialog via apt-get"
        exit 1
    fi
    
    # Verify installation
    if ! command -v dialog >/dev/null 2>&1; then
        echo "Error: Dialog installation failed. Please install it manually." >&2
        log_error "Dialog installation verification failed"
        exit 1
    fi
    
    log_info "Dialog installed successfully"
    return 0
}

# CONFIGURATION MANAGEMENT

# Initialize configuration system
init_config() {
    log_info "Initializing configuration system"
    
    if [ ! -d "$CONFIG_DIR" ]; then
        mkdir -p "$CONFIG_DIR" || {
            log_error "Cannot create config directory $CONFIG_DIR"
            return 1
        }
        log_info "Created config directory: $CONFIG_DIR"
    fi

    if [ ! -f "$CONFIG_FILE" ]; then
        cat > "$CONFIG_FILE" << EOF
rest_api_user=admin
rest_api_password='admin'
rest_api_port=82
EOF
        log_info "Created default config file: $CONFIG_FILE"
    fi

    # Load configuration
    if [ -f "$CONFIG_FILE" ]; then
        log_debug "init_config: Config file exists, reading content"
        log_debug "init_config: Config file content:"
        while IFS= read -r line; do
            log_debug "init_config: $line"
        done < "$CONFIG_FILE"
        
        source "$CONFIG_FILE"
        log_info "Loaded configuration from $CONFIG_FILE"
        log_debug "init_config: rest_api_user='$rest_api_user'"
        log_debug "init_config: rest_api_password length=${#rest_api_password}"
        
        # Show masked password for debugging
        local masked_pwd="${rest_api_password:0:4}$(printf '%*s' $((${#rest_api_password} - 4)) | tr ' ' '*')"
        log_debug "init_config: rest_api_password preview='$masked_pwd'"
        log_debug "init_config: rest_api_port='$rest_api_port'"
    else
        log_error "Config file not found: $CONFIG_FILE"
        return 1
    fi
}

# UTILITY FUNCTIONS

# URL encode a string for safe use in curl credentials
url_encode() {
    local string="$1"
    local encoded=""
    local length=${#string}
    
    log_debug "url_encode: Input string length: $length"
    
    for (( i=0; i<length; i++ )); do
        local char="${string:$i:1}"
        case "$char" in
            [a-zA-Z0-9.~_-]) 
                encoded+="$char" ;;
            *)
                local hex_val=$(printf '%%%02X' "'$char")
                log_debug "url_encode: Encoding char '$char' as '$hex_val'"
                encoded+="$hex_val" ;;
        esac
    done
    
    log_debug "url_encode: Output string length: ${#encoded}"
    echo "$encoded"
}

# Trim leading and trailing quotes and spaces from user input
trimq() {
    echo "${1}" | tr -d " \"'"
}

# Convert string to uppercase
upper() {
    echo "${1}" | tr '[:lower:]' '[:upper:]'
}

# Cleanup inactive NFS storage on all cluster nodes
cleanup_inactive_nfs_storage() {
    log_info "Starting NFS cleanup on all cluster nodes"
    
    dialog --infobox "Cleaning up inactive NFS storage on all nodes..." 5 60
    sleep 1

    # Get node list from corosync configuration
    local node_list
    node_list=$(awk '
        $1 == "name:" {node=$2}
        $1 == "ring0_addr:" {print node, $2}
    ' /etc/pve/corosync.conf)
    
    if [ -z "$node_list" ]; then
        log_error "No nodes found in corosync configuration"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    log_info "Found cluster nodes: $(echo "$node_list" | wc -l) nodes"
    
    # Process each node
    echo "$node_list" | while read -r node ip; do
        log_info "Processing cleanup for node: $node ($ip)"
        (
        ssh -o StrictHostKeyChecking=no root@"$ip" "bash -s" << 'ENDSCRIPT'
log_info() { true; }
log_debug() { true; }
log_error() { true; }
log_info "Running cleanup on remote node"

# Get all mounted NFS directories
all_nfs_mounts=$(mount | grep -E '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+:/\S+\s.+\snfs\s' | awk '{print $3}' | sort)

# Get active PVE storage mounts
active_pve_mounts=$(pvesm status | awk '$2 == "nfs" && $3 == "active" {print "/mnt/pve/" $1}' | sort)

# Calculate inactive mounts
inactive_mounts=$(comm -23 <(echo "$all_nfs_mounts") <(echo "$active_pve_mounts"))
log_info "Inactive mounts to process: $(echo $inactive_mounts | wc -w) mounts"

if [ -z "$inactive_mounts" ]; then
  log_info "No inactive mounts found to clean up"
fi

for mount_point in $inactive_mounts; do
  [ -z "$mount_point" ] && continue
  log_info "Processing mount point: $mount_point"
  
  # Check if mount point is actually mounted
  if mount | grep -q "$mount_point"; then
    log_info "Unmounting $mount_point"
    if umount -l "$mount_point"; then
      log_info "Unmounted: $mount_point"
    else
      log_error "Failed to unmount: $mount_point"
      continue
    fi
  else
    log_info "Mount point already unmounted: $mount_point"
  fi
  
  # Check if directory exists
  if [ ! -d "$mount_point" ]; then
    continue
  fi
  
  # Attempt to remove directory
  log_info "Attempting to remove directory: $mount_point"
  if rmdir "$mount_point" 2>/dev/null; then
    log_info "Removed: $mount_point"
  else
    log_error "Failed to remove directory: $mount_point"
  fi
done

# Also check for orphaned clone directories in /mnt/pve/
log_info "Checking for orphaned clone directories in /mnt/pve/"
pve_directories=$(find /mnt/pve/ -maxdepth 1 -type d -name "*clone*" 2>/dev/null | sort)

echo "$pve_directories" | while read -r dir; do
  [ -z "$dir" ] || [ "$dir" = "/mnt/pve/" ] && continue
  
  # Check if this directory is in active PVE storage
  dir_name=$(basename "$dir")
  is_active=false
  echo "$active_pve_mounts" | while read -r active_mount; do
    if [ "$(basename "$active_mount")" = "$dir_name" ]; then
      is_active=true
      break
    fi
  done
  
  if [ "$is_active" = "true" ]; then
    continue
  fi
  
  # Check if directory is currently mounted
  if mount | grep -q "$dir"; then
    log_info "Directory is mounted but not active, unmounting: $dir"
    if umount -l "$dir"; then
      log_info "Unmounted orphaned directory: $dir"
    else
      log_error "Failed to unmount orphaned directory: $dir"
      continue
    fi
  else
    log_info "Directory not mounted: $dir"
  fi
  
  # Attempt to remove orphaned directory
  log_info "Attempting to remove orphaned directory: $dir"
  if rmdir "$dir" 2>/dev/null; then
    log_info "Removed orphaned: $dir"
  else
    log_error "Failed to remove orphaned directory: $dir"
  fi
done
ENDSCRIPT
        ) >> "$LOGS_DIR/pve-nfs-cleanup-$node.log" 2>&1 &
    done

    wait
    
    log_info "NFS cleanup complete"
    dialog --msgbox "Cleanup finished.\nReview logs in:\n$LOGS_DIR/pve-nfs-cleanup-*.log\nfor details." 8 70
}

# Check pve-config-backup daemon status on all cluster nodes
check_pve_config_backup_status() {
    log_info "Checking pve-config-backup daemon status on all cluster nodes"
    
    dialog --infobox "Checking pve-config-backup daemon status on all nodes..." 5 60

    # Get node list from corosync configuration (optimized)
    local node_list
    node_list=$(awk '$1 == "name:" {node=$2} $1 == "ring0_addr:" {print node, $2}' /etc/pve/corosync.conf)
    
    if [ -z "$node_list" ]; then
        log_error "No nodes found in corosync configuration"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    local nodes_need_install=()
    local nodes_need_start=()
    local nodes_ok=()
    
    # Check each node in parallel for faster execution
    echo "$node_list" | while read -r node ip; do
        (
            log_info "Checking pve-config-backup status on node: $node ($ip)"
            
            # Check if pve-config-backup is installed and get its status (with timeout)
            local status_output
            status_output=$(timeout 10 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@"$ip" "
                if [ -f /usr/local/sbin/pve-config-backup ]; then
                    pve-config-backup status 2>/dev/null || echo 'INSTALLED_BUT_ERROR'
                else
                    echo 'NOT_INSTALLED'
                fi
            " 2>/dev/null)
            
            if [ "$status_output" = "NOT_INSTALLED" ]; then
                echo "$node $ip" >> /tmp/nodes_need_install
                log_info "Node $node: pve-config-backup not installed"
            elif echo "$status_output" | grep -q "Active: inactive (dead)"; then
                echo "$node $ip" >> /tmp/nodes_need_start
                log_info "Node $node: pve-config-backup installed but inactive"
            elif echo "$status_output" | grep -q "Active: active (running)"; then
                echo "$node $ip" >> /tmp/nodes_ok
                log_info "Node $node: pve-config-backup running correctly"
            else
                echo "$node $ip" >> /tmp/nodes_need_install
                log_info "Node $node: pve-config-backup status unknown, will reinstall"
            fi
        ) &
    done
    
    # Wait for all parallel checks to complete
    wait
    
    # Read results from temp files
    if [ -f /tmp/nodes_need_install ]; then
        nodes_need_install=($(cat /tmp/nodes_need_install))
        rm -f /tmp/nodes_need_install
    fi
    
    if [ -f /tmp/nodes_need_start ]; then
        nodes_need_start=($(cat /tmp/nodes_need_start))
        rm -f /tmp/nodes_need_start
    fi
    
    if [ -f /tmp/nodes_ok ]; then
        nodes_ok=($(cat /tmp/nodes_ok))
        rm -f /tmp/nodes_ok
    fi
    
    # Report status
    local total_nodes=$(echo "$node_list" | wc -l)
    local install_count=$((${#nodes_need_install[@]} / 2))
    local start_count=$((${#nodes_need_start[@]} / 2))
    local ok_count=$((${#nodes_ok[@]} / 2))
    
    log_info "pve-config-backup status check complete - Total: $total_nodes, OK: $ok_count, Need install: $install_count, Need start: $start_count"
    
    # Install on nodes that need it
    if [ $install_count -gt 0 ]; then
        install_pve_config_backup_on_nodes "${nodes_need_install[@]}"
    fi
    
    # Start on nodes that need it
    if [ $start_count -gt 0 ]; then
        start_pve_config_backup_on_nodes "${nodes_need_start[@]}"
    fi
    
    # Final status check
    dialog --msgbox "pve-config-backup status check complete.\nTotal nodes: $total_nodes\nAlready running: $ok_count\nInstalled: $install_count\nStarted: $start_count" 12 60
}

# Install pve-config-backup on specified nodes
install_pve_config_backup_on_nodes() {
    local nodes=("$@")
    local node_count=$((${#nodes[@]} / 2))
    
    if [ $node_count -eq 0 ]; then
        return 0
    fi
    
    log_info "Installing pve-config-backup on $node_count nodes"
    
    # Process nodes in pairs (node_name node_ip)
    for ((i=0; i<${#nodes[@]}; i+=2)); do
        local node="${nodes[i]}"
        local ip="${nodes[i+1]}"
        
        log_info "Installing pve-config-backup on node: $node ($ip)"
        
        nohup bash -c "
        (
        timeout 30 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@'$ip' 'bash -s' << 'ENDSCRIPT'
# Install pve-config-backup
cd /tmp
wget https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup \
    -O /usr/local/sbin/pve-config-backup 2>/dev/null

if [ \$? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
    chmod +x /usr/local/sbin/pve-config-backup
    /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
    if [ \$? -eq 0 ]; then
        echo 'INSTALL_SUCCESS'
    else
        echo 'INSTALL_FAILED'
    fi
else
    echo 'DOWNLOAD_FAILED'
fi
ENDSCRIPT
        ) >> '$LOGS_DIR/pve-config-backup-install-$node.log' 2>&1
        " >/dev/null 2>&1 &
    done
    
    wait
    log_info "pve-config-backup installation complete on all nodes"
}

# Start pve-config-backup daemon on specified nodes
start_pve_config_backup_on_nodes() {
    local nodes=("$@")
    local node_count=$((${#nodes[@]} / 2))
    
    if [ $node_count -eq 0 ]; then
        return 0
    fi
    
    log_info "Starting pve-config-backup daemon on $node_count nodes"
    
    # Process nodes in pairs (node_name node_ip)
    for ((i=0; i<${#nodes[@]}; i+=2)); do
        local node="${nodes[i]}"
        local ip="${nodes[i+1]}"
        
        log_info "Starting pve-config-backup daemon on node: $node ($ip)"
        
        (
        timeout 15 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@"$ip" "bash -s" << 'ENDSCRIPT'
# Start pve-config-backup daemon
if [ -f /usr/local/sbin/pve-config-backup ]; then
    /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
    if [ $? -eq 0 ]; then
        echo "START_SUCCESS"
    else
        echo "START_FAILED"
    fi
else
    echo "NOT_INSTALLED"
fi
ENDSCRIPT
        ) >> "$LOGS_DIR/pve-config-backup-start-$node.log" 2>&1 &
    done
    
    wait
    log_info "pve-config-backup daemon start complete on all nodes"
}

# Ensure pve-config-backup is installed and running on HA nodes (or all nodes if no HA group)
ensure_pve_config_backup_running() {
    log_info "Ensuring pve-config-backup daemon is installed and running on HA nodes"

    # Get HA nodes list (fallback to all nodes if no HA group exists)
    local ha_nodes_list
    ha_nodes_list=$(get_ha_nodes)
    
    if [ -z "$ha_nodes_list" ]; then
        log_error "No nodes found in HA configuration or cluster"
        dialog --msgbox "Error: No cluster nodes found" 8 40
        return 1
    fi
    
    # Build node list with IPs by looking up each node in corosync.conf
    local node_list=""
    while IFS= read -r node_name; do
        [ -z "$node_name" ] && continue  # Skip empty lines
        local node_ip
        node_ip=$(awk -v node="$node_name" '
            $1 == "name:" && $2 == node {found=1; next}
            found && $1 == "ring0_addr:" {print $2; found=0}
        ' /etc/pve/corosync.conf)
        
        if [ -n "$node_ip" ]; then
            node_list+="$node_name $node_ip"$'\n'
        else
            log_error "Could not find IP for node: $node_name"
        fi
    done <<< "$ha_nodes_list"
    
    # Remove trailing newline
    node_list=$(echo "$node_list" | sed '/^$/d')
    
    if [ -z "$node_list" ]; then
        log_error "No valid node IPs found"
        dialog --msgbox "Error: No valid node IPs found" 8 40
        return 1
    fi
    
    local total_nodes=$(echo "$node_list" | wc -l)
    local processed=0
    local installed=0
    local started=0
    local already_running=0
    
    # Clean up any previous results
    rm -f /tmp/node_results
    
    # Process each node
    while IFS= read -r line; do
        [ -z "$line" ] && continue  # Skip empty lines
        
        local node=$(echo "$line" | cut -d' ' -f1)
        local ip=$(echo "$line" | cut -d' ' -f2)
        
        log_info "Ensuring pve-config-backup is ready on node: $node ($ip)"
        
        # Check and handle pve-config-backup on this node
        local result
        result=$(ssh -o StrictHostKeyChecking=no root@"$ip" "bash -s" << 'ENDSCRIPT'
# Check if pve-config-backup is installed
if [ ! -f /usr/local/sbin/pve-config-backup ]; then
    # Install pve-config-backup
    cd /tmp
    wget https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup \
        -O /usr/local/sbin/pve-config-backup >/dev/null 2>&1
    
    if [ $? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
        chmod +x /usr/local/sbin/pve-config-backup
        /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
        if [ $? -eq 0 ]; then
            echo "INSTALLED"
            exit 0
        else
            echo "INSTALL_FAILED"
            exit 1
        fi
    else
        echo "DOWNLOAD_FAILED"
        exit 1
    fi
fi

# Check daemon status
status_output=$(/usr/local/sbin/pve-config-backup status 2>/dev/null)
if echo "$status_output" | grep -q "Active: active (running)"; then
    echo "ALREADY_RUNNING"
elif echo "$status_output" | grep -q "Active: inactive (dead)"; then
    # Start the daemon
    /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
    if [ $? -eq 0 ]; then
        echo "STARTED"
    else
        echo "START_FAILED"
        exit 1
    fi
else
    echo "STATUS_UNKNOWN"
    exit 1
fi
ENDSCRIPT
        2>/dev/null)
        
        case "$result" in
            "INSTALLED")
                log_info "Node $node: pve-config-backup installed and started"
                echo "INSTALLED" >> /tmp/node_results
                ;;
            "STARTED")
                log_info "Node $node: pve-config-backup daemon started"
                echo "STARTED" >> /tmp/node_results
                ;;
            "ALREADY_RUNNING")
                log_info "Node $node: pve-config-backup already running"
                echo "ALREADY_RUNNING" >> /tmp/node_results
                ;;
            *)
                log_error "Node $node: Failed to ensure pve-config-backup is running"
                echo "FAILED" >> /tmp/node_results
                ;;
        esac
    done <<< "$node_list"
    
    # Count results
    if [ -f /tmp/node_results ]; then
        installed=$(grep -c "INSTALLED" /tmp/node_results)
        started=$(grep -c "STARTED" /tmp/node_results)
        already_running=$(grep -c "ALREADY_RUNNING" /tmp/node_results)
        rm -f /tmp/node_results
    fi
    
    log_info "pve-config-backup setup complete on HA nodes - Total: $total_nodes, Already running: $already_running, Installed: $installed, Started: $started"
    
    # Only show dialog if action was taken (installations or starts)
    if [ $installed -gt 0 ] || [ $started -gt 0 ]; then
        dialog --msgbox "pve-config-backup setup complete on HA nodes.\nTotal nodes: $total_nodes\nAlready running: $already_running\nInstalled: $installed\nStarted: $started" 12 60
    fi
    
    return 0
}

# Cleanup unused disks in VM/CT configs
cleanup_unused_disks_wizard() {
    log_info "Starting cleanup unused disks wizard"
    
    # Arrays to store found unused disks
    local unused_disks=()
    local unused_display=()
    
    # Scan all VM configs
    for config_file in /etc/pve/nodes/*/qemu-server/*.conf; do
        [ -f "$config_file" ] || continue
        
        local vmid=$(basename "$config_file" .conf)
        local node=$(basename "$(dirname "$(dirname "$config_file")")")
        
        # Search for unused disk lines
        while IFS= read -r line; do
            if [[ "$line" =~ ^unused[0-9]+: ]]; then
                unused_disks+=("vm:$vmid:$node:$line")
                unused_display+=("$vmid" "VM - $line")
            fi
        done < "$config_file"
    done
    
    # Scan all CT configs
    for config_file in /etc/pve/nodes/*/lxc/*.conf; do
        [ -f "$config_file" ] || continue
        
        local ctid=$(basename "$config_file" .conf)
        local node=$(basename "$(dirname "$(dirname "$config_file")")")
        
        # Search for unused disk lines
        while IFS= read -r line; do
            if [[ "$line" =~ ^unused[0-9]+: ]]; then
                unused_disks+=("ct:$ctid:$node:$line")
                unused_display+=("$ctid" "CT - $line")
            fi
        done < "$config_file"
    done
    
    # Check if any unused disks found
    if [ ${#unused_disks[@]} -eq 0 ]; then
        dialog --msgbox "No unused disks found in VM/CT configurations." 8 50
        return 0
    fi
    
    log_info "Found ${#unused_disks[@]} unused disk entries"
    
    # Create checklist dialog
    local dialog_selected=$(mktemp)
    local checklist_items=()
    local all_selected=false
    
    # Build initial checklist items
    for ((i=0; i<${#unused_display[@]}; i+=2)); do
        local vmid="${unused_display[i]}"
        local desc="${unused_display[i+1]}"
        checklist_items+=("$((i/2))" "$vmid: $desc" "off")
        log_debug "Added checklist item: index=$((i/2)), desc='$vmid: $desc', status=off"
    done
    
    log_debug "Total checklist_items array length: ${#checklist_items[@]}"
    
    while true; do
        local middle_button_label="Select All"
        if [ "$all_selected" = true ]; then
            middle_button_label="Deselect All"
        fi
        
        log_debug "About to show dialog with button: $middle_button_label"
        log_debug "Current all_selected state: $all_selected"
        
        dialog --keep-tite --title "Cleanup Unused Disks" \
               --ok-label "Remove Selected" --cancel-label "Back" \
               --extra-button --extra-label "$middle_button_label" \
               --checklist "Select unused disks to remove (use SPACE to select):" \
               20 90 10 "${checklist_items[@]}" 2> "$dialog_selected"
        
        local exit_code=$?
        log_debug "Dialog exit code: $exit_code"
        
        case $exit_code in
            0)  # Remove Selected
                log_debug "User pressed Remove Selected"
                local selected_indices=$(cat "$dialog_selected")
                log_debug "Selected indices: $selected_indices"
                if [ -z "$selected_indices" ]; then
                    dialog --msgbox "No items selected." 6 30
                    continue
                fi
                
                # Confirm removal
                dialog --keep-tite --title "Confirm Removal" \
                       --yes-label "Yes, Remove" --no-label "Cancel" \
                       --yesno "Are you sure you want to remove the selected unused disk entries?" 8 70
                
                if [ $? -eq 0 ]; then
                    cleanup_selected_unused_disks "$selected_indices" unused_disks
                fi
                break
                ;;
            1)  # Back
                log_debug "User pressed Back"
                rm -f "$dialog_selected"
                return 0
                ;;
            2)  # Toggle Select All / Deselect All (extra button)
                log_debug "User pressed toggle button (exit code 2)"
                if [ "$all_selected" = false ]; then
                    # Select all
                    log_info "Selecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="on"
                        log_debug "Set checklist_items[$i]=on"
                    done
                    all_selected=true
                else
                    # Deselect all
                    log_info "Deselecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="off"
                        log_debug "Set checklist_items[$i]=off"
                    done
                    all_selected=false
                fi
                continue
                ;;
            3)  # Toggle Select All / Deselect All (help button alternative)
                log_debug "User pressed toggle button (exit code 3)"
                if [ "$all_selected" = false ]; then
                    # Select all
                    log_info "Selecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="on"
                        log_debug "Set checklist_items[$i]=on"
                    done
                    all_selected=true
                else
                    # Deselect all
                    log_info "Deselecting all items"
                    for ((i=2; i<${#checklist_items[@]}; i+=3)); do
                        checklist_items[i]="off"
                        log_debug "Set checklist_items[$i]=off"
                    done
                    all_selected=false
                fi
                continue
                ;;
            *)  # Any other exit code
                log_debug "Unexpected dialog exit code: $exit_code"
                rm -f "$dialog_selected"
                return 1
                ;;
        esac
    done
    
    rm -f "$dialog_selected"
    return 0
}

# Remove selected unused disk entries
cleanup_selected_unused_disks() {
    local selected_indices="$1"
    local -n disk_array=$2
    local removed_count=0
    
    log_info "Starting removal of selected unused disk entries"
    
    # Process each selected index
    for index in $selected_indices; do
        local disk_info="${disk_array[index]}"
        IFS=':' read -ra parts <<< "$disk_info"
        local type="${parts[0]}"
        local vmid="${parts[1]}"
        local node="${parts[2]}"
        local unused_line="${parts[3]}"
        
        # Determine config file path
        local config_file
        if [ "$type" = "vm" ]; then
            config_file="/etc/pve/nodes/$node/qemu-server/$vmid.conf"
        else
            config_file="/etc/pve/nodes/$node/lxc/$vmid.conf"
        fi
        
        # Extract unused disk identifier (e.g., "unused0")
        local unused_key
        unused_key=$(echo "$unused_line" | cut -d':' -f1)
        
        log_info "Removing $unused_key from $type $vmid config"
        
        # Remove the unused line using sed
        if sed -i "/^$unused_key:/d" "$config_file"; then
            log_info "Successfully removed $unused_key from $config_file"
            ((removed_count++))
        else
            log_error "Failed to remove $unused_key from $config_file"
        fi
    done
    
    log_info "Cleanup complete. Removed $removed_count unused disk entries"
    dialog --msgbox "Cleanup complete!\nRemoved $removed_count unused disk entries." 8 50
}


# Execute dialog menu and capture result
dialog_menu() {
    "${cmd[@]}" "${options[@]}" 2> "$dialog_selected"
    dialog_exit_code=$?
    selected_option=$(< "$dialog_selected")
    dialog --infobox "Please wait..." 5 20 &
    # Give up to 0.1 second to suck out up to 10 000 bytes from keyboard buffer:
    read -t 0.2 -n 10000 discard
}

# Show VM/CT conflict dialog and get user decision
resolve_vmct_conflict() {
    local vmid="$1"
    local type="$2"
    
    if [ -z "$vmid" ] || [ -z "$type" ]; then
        log_error "resolve_vmct_conflict: Missing required parameters"
        return 1
    fi
    
    log_info "Resolving VM/CT conflict for ID $vmid ($type)"
    
    local vm_type_upper=$(upper "$type")
    local conflict_msg="$vm_type_upper ID $vmid already exists in PVE repository!\n\n"
    conflict_msg+="Choose an option:\n"
    conflict_msg+="- Cancel: Skip this $vm_type_upper restoration\n"
    conflict_msg+="- New ID: Provide a different ID for restoration"
    
    dialog --keep-tite --title "$vm_type_upper Conflict Detected" \
           --yes-label "New ID" --no-label "Cancel" \
           --yesno "$conflict_msg" 12 70
    
    local choice=$?
    
    
    if [ $choice -eq 0 ]; then
        # User chose "New ID", prompt for input
        log_info "User chose New ID - prompting for input"
        if get_new_vmct_id "$vmid" "$type"; then
            log_info "User provided new ID: $new_vmct_id"
            return 0
        else
            return 1
        fi
    else
        # User chose "Cancel"
        return 1
    fi
}

# Get new VM/CT ID from user with validation
get_new_vmct_id() {
    local original_id="$1"
    local type="$2"
    local input_file=$(mktemp)
    
    log_info "Prompting user for new $(upper "$type") ID for $type $original_id"
    
    while true; do
        dialog --keep-tite --title "Enter New $(upper "$type") ID" \
               --ok-label "OK" --cancel-label "Cancel" \
               --inputbox "Enter new ID for $type $original_id:" 8 50 2> "$input_file"
        
        local exit_code=$?
        local user_input=$(cat "$input_file")
        
        
        if [ $exit_code -ne 0 ]; then
            # User cancelled
            rm -f "$input_file"
            return 1
        fi
        
        # Validate input
        if [[ ! "$user_input" =~ ^[0-9]+$ ]]; then
            dialog --msgbox "Invalid input. Please enter a numeric ID." 8 40
            continue
        fi
        
        # Check if ID is already in use
        if check_vmct_exists "$user_input"; then
            dialog --msgbox "ID $user_input is already in use. Please choose another ID." 8 50
            continue
        fi
        
        # Valid new ID provided
        new_vmct_id="$user_input"
        rm -f "$input_file"
        log_info "User provided new $(upper "$type") ID: $new_vmct_id for original ID: $original_id"
        return 0
    done
}

# Select host for conflicting VM/CT restoration
select_host_for_conflict() {
    local vmid="$1"
    local type="$2"
    local line hosts=() menu_items=()
    
    if [ -z "$vmid" ] || [ -z "$type" ]; then
        log_error "select_host_for_conflict: Missing required parameters"
        return 1
    fi
    
    log_info "Selecting host for conflicting $(upper "$type") ID $vmid ($type)"
    
    # Get available hosts
    while IFS= read -r line; do
        hosts+=("$line")
    done < <(get_ha_nodes)
    
    if [ "${#hosts[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE hosts found." 8 40
        return 1
    fi
    
    # Build menu items
    local i
    for ((i=0; i<${#hosts[@]}; i++)); do
        menu_items+=("$i" "${hosts[i]}")
    done
    
    local dialog_selected=$(mktemp)
    dialog --keep-tite --title "Select Target Host for $(upper "$type") $vmid" \
           --ok-label "Select" --cancel-label "Cancel" \
           --menu "Choose PVE host to restore $(upper "$type") $vmid ($type) to:" 15 60 8 \
           "${menu_items[@]}" 2> "$dialog_selected"
    
    local exit_code=$?
    local selected_index=$(cat "$dialog_selected")
    rm -f "$dialog_selected"
    
    if [ $exit_code -eq 0 ] && [ -n "$selected_index" ]; then
        conflict_selected_host="${hosts[selected_index]}"
        log_info "User selected host: $conflict_selected_host for $(upper "$type") $vmid"
        return 0
    else
        return 1
    fi
}

# NFS TUNING FUNCTIONS

get_nfs_storages() {
    grep -Po '^nfs:\s*\K\S+' /etc/pve/storage.cfg 2>/dev/null || true
}

get_current_nconnect() {
    local storage="$1"
    local nconnect_val
    nconnect_val=$(awk -v storage="$storage" '
        /^nfs:/ && $2 == storage {
            found = 1
            next
        }
        found && /^[a-z]+:/ {
            found = 0
            next
        }
        found && /options/ {
            for(i=2; i<=NF; i++) {
                if($i ~ /nconnect=[0-9]+/) {
                    gsub(/.*nconnect=/, "", $i)
                    gsub(/,.*/, "", $i)
                    print $i
                    exit
                }
            }
        }
    ' /etc/pve/storage.cfg)
    echo "${nconnect_val:-1}"
}

get_mounted_nconnect() {
    local storage="$1"
    local current_nconnect
    local mount_output
    
    log_debug "Getting mounted nconnect for storage: $storage"
    mount_output=$(mount | grep nfs | grep "$storage")
    log_debug "Mount output for $storage: $mount_output"
    
    current_nconnect=$(echo "$mount_output" | grep -Eo "nconnect=[0-9]+" | cut -d= -f2)
    log_debug "Extracted nconnect value for $storage: ${current_nconnect:-1}"
    
    echo "${current_nconnect:-1}"
}

get_current_sync() {
    local storage="$1"
    local sync_val
    
    log_debug "Getting current sync option for storage: $storage"
    
    # Check storage.cfg for sync/async option in options field
    sync_val=$(awk -v storage="$storage" '
        $1 == "nfs:" && $2 == storage { in_block = 1; next }
        in_block && /^[[:space:]]*options[[:space:]]+/ {
            options_line = $0
            # Look for sync or async in the options line
            if (match(options_line, /sync/)) {
                if (match(options_line, /async/)) {
                    print "async"
                } else {
                    print "sync"
                }
            } else {
                print "sync"  # Default to sync for HA reliability
            }
            exit
        }
        /^nfs:/ && $2 != storage { in_block = 0 }
    ' /etc/pve/storage.cfg)
    
    log_debug "Extracted sync value for $storage: ${sync_val:-sync}"
    echo "${sync_val:-sync}"
}

get_mounted_sync() {
    local storage="$1"
    local current_sync
    local mount_output
    
    log_debug "Getting mounted sync option for storage: $storage"
    mount_output=$(mount | grep nfs | grep "$storage")
    log_debug "Mount output for $storage: $mount_output"
    
    # Check if sync is present in mount options
    if echo "$mount_output" | grep -q ",sync,\|,sync "; then
        current_sync="sync"
    else
        # NFS defaults to async if not specified, but we treat missing option as sync for HA
        current_sync="async"
    fi
    
    log_debug "Extracted sync value for $storage: $current_sync"
    echo "$current_sync"
}

nfs_tuning_step1_storage_selection() {
    local dialog_selected=$(mktemp)
    
    local nfs_storages
    nfs_storages=$(get_nfs_storages)
    
    if [[ -z "$nfs_storages" ]]; then
        dialog --title "Error" --msgbox "No NFS storage found in configuration" 8 50
        rm -f "$dialog_selected"
        return 1
    fi
    
    local dialog_args=()
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            local config_nconnect
            local mounted_nconnect
            local config_sync
            local mounted_sync
            config_nconnect=$(get_current_nconnect "$storage")
            mounted_nconnect=$(get_mounted_nconnect "$storage")
            config_sync=$(get_current_sync "$storage")
            mounted_sync=$(get_mounted_sync "$storage")
            dialog_args+=("$storage" "nconnect cfg=$(printf '%2s' "$config_nconnect") mnt=$(printf '%2s' "$mounted_nconnect") sync: cfg=$config_sync mnt=$mounted_sync" "on")
        fi
    done <<< "$nfs_storages"
    
    if [[ ${#dialog_args[@]} -eq 0 ]]; then
        dialog --title "Error" --msgbox "No valid NFS storage found" 8 40
        rm -f "$dialog_selected"
        return 1
    fi
    
    cmd=(dialog --keep-tite --title "Step 1/3: Storage Selection"
         --ok-label "Next" --cancel-label "Back"
         --checklist "Select storage to update (use SPACE to select):" 20 100 10)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select at least one storage." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_storages=$(echo "$selected_option" | tr -d '"' | tr ' ' '\n')
            log_info "Selected storages: $(echo "$selected_nfs_storages" | tr '\n' ' ')"
            return 0  # Proceed to next step
            ;;
        1)  # Cancel button (Back)
            return 1
            ;;
    esac
}

nfs_tuning_step2_nconnect_selection() {
    local dialog_selected=$(mktemp)
    
    local dialog_args=()
    
    for i in {1..16}; do
        if [[ $i -eq 4 ]]; then
            dialog_args+=("$i" "nconnect = $i (default)" "on")
        else
            dialog_args+=("$i" "nconnect = $i" "off")
        fi
    done
    
    cmd=(dialog --keep-tite --title "Step 2/3: nconnect Value"
         --ok-label "Next" --cancel-label "Back"
         --radiolist "Choose new nconnect value:" 20 100 10)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select an nconnect value." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_nconnect="$selected_option"
            log_info "Selected nconnect value: $selected_nfs_nconnect"
            return 0  # Proceed to next step
            ;;
        1)  # Cancel button (Back)
            return 2  # Go back to previous step
            ;;
    esac
}

nfs_tuning_step3_sync_selection() {
    local dialog_selected=$(mktemp)
    
    local dialog_args=()
    dialog_args+=("sync" "sync (default - guaranteed write completion for HA)" "on")
    dialog_args+=("async" "async (better performance - less reliable)" "off")
    
    cmd=(dialog --keep-tite --title "Step 3/3: Sync Option"
         --ok-label "Next" --cancel-label "Back"
         --radiolist "Choose sync option for NFS mounts:" 15 70 5)
    options=("${dialog_args[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # OK/Next button
            if [[ -z "$selected_option" ]]; then
                dialog --title "Error" --msgbox "Please select a sync option." 8 50
                return 2  # Stay on same step
            fi
            selected_nfs_sync="$selected_option"
            log_info "Selected sync option: $selected_nfs_sync"
            return 0  # Proceed to execute
            ;;
        1)  # Cancel button (Back)
            return 2  # Go back to previous step
            ;;
    esac
}

execute_nfs_tuning_update() {
    local temp_dir="/tmp/nfs-tuning-$$"
    local results_file="$temp_dir/results.txt"
    
    mkdir -p "$temp_dir"
    
    log_info "Starting NFS tuning update process"
    echo "Starting NFS tuning update process" > "$results_file"
    echo "Selected storages: $(echo "$selected_nfs_storages" | tr '\n' ' ')" >> "$results_file"
    echo "Selected nconnect value: $selected_nfs_nconnect" >> "$results_file"
    echo "Selected sync option: $selected_nfs_sync" >> "$results_file"
    echo "" >> "$results_file"
    
    # Count total storages for progress calculation
    local storage_count=0
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            storage_count=$((storage_count + 1))
        fi
    done <<< "$selected_nfs_storages"
    
    # Progress tracking
    local current_step=0
    local total_steps=$((storage_count * 2 + storage_count + 1))  # config+umount, mount, verify, +1 for stabilize
    
    # Create progress pipe
    local progress_pipe
    progress_pipe=$(mktemp -u)
    mkfifo "$progress_pipe"
    
    # Start progress dialog in background
    dialog --title "Updating NFS Tuning Settings" \
           --gauge "Initializing..." 8 60 0 < "$progress_pipe" &
    local dialog_pid=$!
    
    # Function to update progress
    update_progress() {
        local message="$1"
        local percentage=$((current_step * 100 / total_steps))
        echo "$percentage"
        echo "XXX"
        echo "$message"
        echo "XXX"
    }
    
    # Redirect to progress pipe
    exec 3>"$progress_pipe"
    
    # Step 1: Update storage config and umount for each storage
    log_info "Step 1: Updating storage configurations and unmounting"
    echo "Step 1: Updating storage configurations and unmounting" >> "$results_file"
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            # Update storage configuration
            current_step=$((current_step + 1))
            update_progress "Updating $storage configuration..." >&3
            
            log_info "Updating $storage configuration with nconnect=$selected_nfs_nconnect, sync=$selected_nfs_sync"
            
            # Build options string based on selected sync option
            local options_string="nconnect=$selected_nfs_nconnect"
            if [[ "$selected_nfs_sync" == "sync" ]]; then
                options_string="${options_string},sync"
            else
                options_string="${options_string},async"
            fi
            
            log_debug "Executing: pvesm set $storage --options $options_string"
            pvesm set "$storage" --options "$options_string" 2>/dev/null || true
            
            # Attempt to unmount
            current_step=$((current_step + 1))
            update_progress "Unmounting $storage..." >&3
            
            log_info "Attempting to unmount $storage"
            local mount_path="/mnt/pve/$storage"
            log_debug "Executing: umount $mount_path"
            umount "$mount_path" 2>/dev/null || true
        fi
    done <<< "$selected_nfs_storages"
    
    # Step 2: Mount all storages
    log_info "Step 2: Mounting all storages"
    echo "Step 2: Mounting all storages" >> "$results_file"
    
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            current_step=$((current_step + 1))
            update_progress "Mounting $storage..." >&3
            
            log_info "Mounting $storage"
            local escaped
            escaped=$(systemd-escape "$storage")
            local unit="mnt-pve-${escaped}.mount"
            log_debug "Executing: systemctl start $unit"
            systemctl start "$unit" 2>/dev/null || true
        fi
    done <<< "$selected_nfs_storages"
    
    # Step 3: Wait for mounts to stabilize
    current_step=$((current_step + 1))
    update_progress "Waiting for mounts to stabilize..." >&3
    
    log_info "Step 3: Waiting for mounts to stabilize"
    log_debug "Starting 5-second wait for mounts to stabilize"
    echo "Step 3: Waiting for mounts to stabilize, then verifying results" >> "$results_file"
    sleep 5
    log_debug "Mount stabilization wait completed"
    
    # Step 4: Verify results
    log_info "Step 4: Verifying results"
    echo "============================" >> "$results_file"
    
    local has_failures=false
    while IFS= read -r storage; do
        if [[ -n "$storage" ]]; then
            update_progress "Verifying $storage..." >&3
            
            local mounted_nconnect
            local mounted_sync
            local attempts=0
            local max_attempts=10
            
            log_debug "Starting verification for $storage (expected: nconnect=$selected_nfs_nconnect, sync=$selected_nfs_sync)"
            
            # Try multiple times to get the mount info, as it may take time to update
            while [[ $attempts -lt $max_attempts ]]; do
                attempts=$((attempts + 1))
                log_debug "Verification attempt $attempts/$max_attempts for $storage"
                
                mounted_nconnect=$(get_mounted_nconnect "$storage")
                mounted_sync=$(get_mounted_sync "$storage")
                log_debug "Got values for $storage: nconnect=$mounted_nconnect, sync=$mounted_sync"
                
                # Check if we got the expected values
                if [[ "$mounted_nconnect" == "$selected_nfs_nconnect" && "$mounted_sync" == "$selected_nfs_sync" ]]; then
                    log_debug "Expected values found for $storage, stopping retries"
                    break
                fi
                
                if [[ $attempts -lt $max_attempts ]]; then
                    log_debug "Sleeping 2 seconds before next attempt for $storage"
                    sleep 2
                fi
            done
            
            log_debug "Final verification for $storage: expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
            
            if [[ "$mounted_nconnect" == "$selected_nfs_nconnect" && "$mounted_sync" == "$selected_nfs_sync" ]]; then
                log_info "$storage: SUCCESS - nconnect=$mounted_nconnect, sync=$mounted_sync"
                log_debug "RESULT: $storage SUCCESS"
                echo "$storage: SUCCESS - nconnect=$mounted_nconnect, sync=$mounted_sync" >> "$results_file"
            else
                log_info "$storage: FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
                log_debug "RESULT: $storage FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync"
                echo "$storage: FAILED - expected nconnect=$selected_nfs_nconnect, actual=$mounted_nconnect; expected sync=$selected_nfs_sync, actual=$mounted_sync" >> "$results_file"
                has_failures=true
            fi
        fi
    done <<< "$selected_nfs_storages"
    
    # Complete progress
    echo "100" >&3
    echo "XXX" >&3
    echo "Update completed!" >&3
    echo "XXX" >&3
    
    # Close progress pipe and wait for dialog to finish
    exec 3>&-
    sleep 1
    kill "$dialog_pid" 2>/dev/null || true
    wait "$dialog_pid" 2>/dev/null || true
    
    # Clean up
    rm -f "$progress_pipe"
    
    # Add failure note if needed
    if [[ "$has_failures" == "true" ]]; then
        echo "" >> "$results_file"
        echo "NOTE: To apply and mount NFS with new nconnect and sync option values," >> "$results_file"
        echo " all VMs must be stopped or their disks migrated to other storage." >> "$results_file"
    fi
    
    dialog --exit-label "Continue" --title "Operation Summary" --textbox "$results_file" 20 80

    # Clean up temp directory
    rm -rf "$temp_dir"
}

nfs_tuning_wizard() {
    local nfs_current_step=1
    local nfs_total_steps=3
    
    # Clear previous selections
    selected_nfs_storages=""
    selected_nfs_nconnect=""
    selected_nfs_sync=""
    
    while true; do
        case $nfs_current_step in
            1)
                nfs_tuning_step1_storage_selection
                case $? in
                    0)  # Next - advance to step 2
                        nfs_current_step=2
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Stay on same step
                        ;;
                esac
                ;;
            2)
                nfs_tuning_step2_nconnect_selection
                case $? in
                    0)  # Next - advance to step 3
                        nfs_current_step=3
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Back - return to step 1
                        nfs_current_step=1
                        ;;
                esac
                ;;
            3)
                nfs_tuning_step3_sync_selection
                case $? in
                    0)  # Next - execute and return to main menu
                        execute_nfs_tuning_update
                        return 0
                        ;;
                    1)  # Back to main menu
                        return 0
                        ;;
                    2)  # Back - return to step 2
                        nfs_current_step=2
                        ;;
                esac
                ;;
        esac
    done
}

# POST KNOWN ISSUES FIXES AND TUNING

# Check ZFS import scan service status
check_zfs_import_scan_service() {
    local service_status
    service_status=$(systemctl is-enabled zfs-import-scan.service 2>/dev/null)
    
    if [ "$service_status" = "disabled" ]; then
        return 1  # Already disabled
    else
        return 0  # Enabled or other status
    fi
}

# Fix ZFS import scan service
fix_zfs_import_scan_service() {
    log_info "Disabling zfs-import-scan.service"
    
    if systemctl disable zfs-import-scan.service >> "$LOG_FILE" 2>&1; then
        log_info "Successfully disabled zfs-import-scan.service"
        return 0
    else
        log_error "Failed to disable zfs-import-scan.service"
        return 1
    fi
}

# Check HA policy shutdown setting
check_ha_policy_shutdown() {
    local ha_policy
    
    # Check only /etc/pve/datacenter.cfg for HA configuration
    if [ -f "/etc/pve/datacenter.cfg" ]; then
        ha_policy=$(grep -E "^ha:" /etc/pve/datacenter.cfg 2>/dev/null | grep -oE "shutdown_policy=[^, ]*" | cut -d= -f2)
    fi
    
    # If no explicit setting in datacenter.cfg, default to conditional
    if [ -z "$ha_policy" ]; then
        ha_policy="conditional"
    fi
    
    log_info "Current HA shutdown policy: $ha_policy"
    
    if [ "$ha_policy" = "migrate" ]; then
        return 1  # Already set to migrate
    else
        return 0  # Not set to migrate (includes missing line = default conditional)
    fi
}

# Fix HA policy shutdown setting
fix_ha_policy_shutdown() {
    log_info "Setting HA shutdown policy to migrate"
    
    local datacenter_cfg="/etc/pve/datacenter.cfg"
    
    if [ -f "$datacenter_cfg" ]; then
        # Check if ha section exists
        if grep -q "^ha:" "$datacenter_cfg"; then
            # Update existing ha section
            sed -i 's/^ha:.*$/ha: shutdown_policy=migrate/' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
        else
            # Add ha section
            echo "ha: shutdown_policy=migrate" >> "$datacenter_cfg" 2>> "$LOG_FILE"
        fi
        
        if [ $? -eq 0 ]; then
            log_info "Successfully set HA shutdown policy to migrate in datacenter.cfg"
            return 0
        else
            log_error "Failed to set HA shutdown policy to migrate"
            return 1
        fi
    else
        log_error "datacenter.cfg not found"
        return 1
    fi
}

# Restore HA policy shutdown setting to conditional (default)
restore_ha_policy_shutdown() {
    log_info "Restoring HA shutdown policy to conditional (default)"
    
    local datacenter_cfg="/etc/pve/datacenter.cfg"
    
    if [ -f "$datacenter_cfg" ]; then
        # Check if ha section exists
        if grep -q "^ha:" "$datacenter_cfg"; then
            # Check if it only contains shutdown_policy
            local ha_line=$(grep "^ha:" "$datacenter_cfg")
            if [[ "$ha_line" =~ ^ha:[[:space:]]*shutdown_policy=[^,]*$ ]]; then
                # Only shutdown_policy is set, remove the entire line
                sed -i '/^ha:[[:space:]]*shutdown_policy=/d' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
                log_info "Removed ha: shutdown_policy line from datacenter.cfg (restored to default)"
            else
                # Other HA settings exist, just remove shutdown_policy
                sed -i 's/,\?[[:space:]]*shutdown_policy=[^,]*//g; s/shutdown_policy=[^,]*,\?[[:space:]]*//g' "$datacenter_cfg" >> "$LOG_FILE" 2>&1
                log_info "Removed shutdown_policy from ha line in datacenter.cfg (restored to default)"
            fi
            
            if [ $? -eq 0 ]; then
                log_info "Successfully restored HA shutdown policy to conditional (default) in datacenter.cfg"
                return 0
            else
                log_error "Failed to restore HA shutdown policy to conditional (default)"
                return 1
            fi
        else
            # No ha section exists, already at default
            log_info "No ha section found in datacenter.cfg, already at default (conditional)"
            return 0
        fi
    else
        log_error "datacenter.cfg not found"
        return 1
    fi
}

# Check if hardware watchdog is configured
check_watchdog_module() {
    local pve_ha_config="/etc/default/pve-ha-manager"
    
    if [ ! -f "$pve_ha_config" ]; then
        log_error "pve-ha-manager config file not found: $pve_ha_config"
        return 1
    fi
    
    # Check if WATCHDOG_MODULE line exists and is uncommented
    if grep -q "^WATCHDOG_MODULE=" "$pve_ha_config"; then
        return 1  # Already configured (uncommented)
    else
        return 0  # Not configured (commented or missing)
    fi
}

# Fix hardware watchdog configuration
fix_watchdog_module() {
    log_info "Configuring hardware watchdog (ipmi_watchdog)"
    
    local pve_ha_config="/etc/default/pve-ha-manager"
    
    if [ ! -f "$pve_ha_config" ]; then
        log_error "pve-ha-manager config file not found: $pve_ha_config"
        return 1
    fi
    
    # Check if commented line exists
    if grep -q "^#WATCHDOG_MODULE=" "$pve_ha_config"; then
        # Uncomment the line
        sed -i 's/^#WATCHDOG_MODULE=/WATCHDOG_MODULE=/' "$pve_ha_config" >> "$LOG_FILE" 2>&1
        log_info "Uncommented WATCHDOG_MODULE line in $pve_ha_config"
    else
        # Add the line if it doesn't exist
        echo "WATCHDOG_MODULE=ipmi_watchdog" >> "$pve_ha_config"
        log_info "Added WATCHDOG_MODULE=ipmi_watchdog to $pve_ha_config"
    fi
    
    if [ $? -eq 0 ]; then
        log_info "Successfully configured hardware watchdog (ipmi_watchdog)"
        return 0
    else
        log_error "Failed to configure hardware watchdog"
        return 1
    fi
}

# Main function for post known issues fixes and tuning
post_known_issues_fixes() {
    local dialog_selected=$(mktemp)
    local fixes_applied=0
    
    # ZFS Import Scan Service Fix
    if check_zfs_import_scan_service; then
        # Service is enabled, offer to disable it
        dialog --title "ZFS Import Scan Service Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "Proxmox Host Attempts to Import Open-E JovianDSS ZFS Pool After Reboot\n\nDescription:\nWhen running Open-E JovianDSS as a virtual machine on a Proxmox host, the host system may attempt to import a ZFS pool that belongs to the Open-E JovianDSS VM. This typically occurs if the Open-E JovianDSS VM accesses its data disks via PCI passthrough.\n\nObserved Behavior:\nAfter rebooting the Proxmox host, the system tries to auto-import the ZFS pool from the passthrough disks, which may lead to errors.\n\nWorkaround:\nDisable the zfs-import-scan.service on the Proxmox host to prevent automatic import of ZFS pools that belong to the Open-E JovianDSS VM.\n\nDo you want to apply this fix?" 22 80
        
        if [ $? -eq 0 ]; then
            if fix_zfs_import_scan_service; then
                dialog --msgbox "ZFS Import Scan Service Fix Applied Successfully!\n\nThe zfs-import-scan.service has been disabled." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply ZFS Import Scan Service fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "ZFS Import Scan Service Fix Already Applied\n\nThe zfs-import-scan.service is already disabled." 10 60
    fi
    
    # HA Policy Shutdown Fix
    if check_ha_policy_shutdown; then
        # Policy is not set to migrate, offer to fix it
        dialog --title "HA Policy Shutdown Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "How is the HA policy for a node shutdown configured?\n\nDescription:\nThe shutdown policy can be configured in the Web UI (Datacenter → Options → HA Settings). By default, the setting is 'Conditional' for backward compatibility.\n\nTo achieve the expected behavior—starting VMs/CTs on another cluster host—you must change the setting to 'Migrate'.\n\nThis fix will set the HA shutdown policy to 'Migrate' to ensure VMs/CTs are properly migrated to other nodes during shutdown.\n\nDo you want to apply this fix?" 18 80
        
        if [ $? -eq 0 ]; then
            if fix_ha_policy_shutdown; then
                dialog --msgbox "HA Policy Shutdown Fix Applied Successfully!\n\nThe HA shutdown policy has been set to 'Migrate'." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply HA Policy Shutdown fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "HA Policy Shutdown Fix Already Applied\n\nThe HA shutdown policy is already set to 'Migrate'." 10 60
    fi
    
    # Hardware Watchdog Configuration Fix
    if check_watchdog_module; then
        # Hardware watchdog is not configured, offer to configure it
        dialog --title "Hardware Watchdog Configuration Fix" \
               --yes-label "Confirm" \
               --no-label "Skip" \
               --yesno "Hardware Watchdog Configuration\n\nDescription:\nHardware watchdogs are preferred for reliability, as they operate independently of the operating system. Software watchdogs, which are kernel-based, act as a fallback when hardware options are unavailable.\n\nTo configure the watchdog module, edit the configuration file:\n /etc/default/pve-ha-manager\n\nToggle the setting by (un)commenting the line:\n#WATCHDOG_MODULE=ipmi_watchdog    Commented => PVE keeps the default softdog software watchdog.\nWATCHDOG_MODULE=ipmi_watchdog    Uncommented => watchdog-mux auto-loads the ipmi_watchdog driver, turning on hardware fencing.\n\nRemoving the # activates the specified hardware watchdog, replacing the software-based fallback.\n\nDo you want to apply this fix?" 24 80
        
        if [ $? -eq 0 ]; then
            if fix_watchdog_module; then
                dialog --msgbox "Hardware Watchdog Configuration Fix Applied Successfully!\n\nThe WATCHDOG_MODULE has been configured to use ipmi_watchdog." 10 60
                fixes_applied=$((fixes_applied + 1))
            else
                dialog --msgbox "Error: Failed to apply Hardware Watchdog Configuration fix.\nCheck the log file for details." 10 60
            fi
        fi
    else
        dialog --msgbox "Hardware Watchdog Configuration Fix Already Applied\n\nThe WATCHDOG_MODULE is already configured." 10 60
    fi
    
    # Summary
    if [ $fixes_applied -gt 0 ]; then
        dialog --msgbox "Post Known Issues Fixes Complete!\n\nApplied $fixes_applied fix(es) successfully." 10 50
    else
        dialog --msgbox "Post Known Issues Fixes Complete!\n\nNo fixes were needed or applied." 10 50
    fi
    
    rm -f "$dialog_selected"
    log_info "Post known issues fixes completed - $fixes_applied fixes applied"
}

# AUTO-UPDATE FUNCTIONS

# Check for script updates from GitHub
check_for_updates() {
    local current_version="$VERSION"
    local remote_version
    
    log_info "Checking for updates - current version: $current_version"
    
    # Get remote version from GitHub
    remote_version=$(wget -qO- https://raw.githubusercontent.com/open-e/pve-tools/main/pve-tools | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote version"
        return 1
    fi
    
    log_info "Remote version: $remote_version"
    
    # Compare versions (simple string comparison)
    if [ "$remote_version" != "$current_version" ]; then
        # Version is different, check if it's newer
        if [ "$remote_version" \> "$current_version" ]; then
            log_info "New version available: $remote_version"
            return 0  # Update available
        else
            log_info "Current version is newer or equal"
            return 1  # No update needed
        fi
    else
        log_info "Already running latest version"
        return 1  # No update needed
    fi
}

# Update the script from GitHub
update_script() {
    local current_version="$VERSION"
    local script_path
    
    # Get the path of the currently running script
    script_path=$(readlink -f "$0")
    
    log_info "Starting script update from $script_path"
    
    # Create backup of current script
    if ! cp "$script_path" "${script_path}.ver.${current_version}" >> "$LOG_FILE" 2>&1; then
        log_error "Failed to create backup of current script"
        dialog --msgbox "Error: Failed to create backup of current script" 8 50
        return 1
    fi
    
    log_info "Created backup: ${script_path}.ver.${current_version}"
    
    # Download new version
    if ! wget https://raw.githubusercontent.com/open-e/pve-tools/main/pve-tools -O "$script_path" >> "$LOG_FILE" 2>&1; then
        log_error "Failed to download new version"
        dialog --msgbox "Error: Failed to download new version\nRestoring backup..." 8 50
        
        # Restore backup on failure
        cp "${script_path}.ver.${current_version}" "$script_path" >> "$LOG_FILE" 2>&1
        return 1
    fi
    
    # Make executable
    chmod +x "$script_path" >> "$LOG_FILE" 2>&1
    
    log_info "Script updated successfully"
    
    # Show success message and exit
    dialog --msgbox "Update completed successfully!\n\nPlease run pve-tools again to use the new version." 10 60
    
    exit 0
}

# Prompt user for update
prompt_for_update() {
    local remote_version
    
    # Get remote version for display
    remote_version=$(wget -qO- https://raw.githubusercontent.com/open-e/pve-tools/main/pve-tools | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    dialog --title "Update Available" \
           --yes-label "Update" \
           --no-label "Continue" \
           --yesno "A new version of pve-tools is available!\n\nCurrent version: $VERSION\nNew version: $remote_version\n\nWould you like to update now?" 12 60
    
    case $? in
        0)  # Yes - update
            update_script
            ;;
        1)  # No - continue
            log_info "User declined update"
            return 1
            ;;
        *)  # Error or ESC
            log_info "Update dialog cancelled"
            return 1
            ;;
    esac
}

# Check for pve-config-backup updates from GitHub
check_pve_config_backup_updates() {
    local current_version
    local remote_version
    
    log_info "Checking for pve-config-backup updates"
    
    # Get current version from local file
    if [ ! -f /usr/local/sbin/pve-config-backup ]; then
        log_info "pve-config-backup not installed locally"
        return 2  # Not installed
    fi
    
    current_version=$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '"' | head -1)
    
    if [ -z "$current_version" ]; then
        log_error "Failed to retrieve current pve-config-backup version"
        return 1
    fi
    
    log_info "Current pve-config-backup version: $current_version"
    
    # Get remote version from GitHub
    remote_version=$(wget -qO- https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote pve-config-backup version"
        return 1
    fi
    
    log_info "Remote pve-config-backup version: $remote_version"
    
    # Compare versions (simple string comparison)
    if [ "$remote_version" != "$current_version" ]; then
        # Version is different, check if it's newer
        if [ "$remote_version" \> "$current_version" ]; then
            log_info "New pve-config-backup version available: $remote_version"
            return 0  # Update available
        else
            log_info "Current pve-config-backup version is newer or equal"
            return 1  # No update needed
        fi
    else
        log_info "Already running latest pve-config-backup version"
        return 1  # No update needed
    fi
}

# Update pve-config-backup on all cluster nodes
update_pve_config_backup() {
    local remote_version
    
    log_info "Starting pve-config-backup update on all cluster nodes"
    
    # Get remote version for logging
    remote_version=$(wget -qO- https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
    
    if [ -z "$remote_version" ]; then
        log_error "Failed to retrieve remote pve-config-backup version for update"
        return 1
    fi
    
    log_info "Updating to pve-config-backup version: $remote_version"
    
    # Get HA nodes list (fallback to all nodes if no HA group exists)
    local ha_nodes_list
    ha_nodes_list=$(get_ha_nodes)
    
    if [ -z "$ha_nodes_list" ]; then
        log_error "No nodes found in HA configuration or cluster"
        dialog --msgbox "Error: No cluster nodes found for update" 8 40
        return 1
    fi
    
    # Build node list with IPs
    local node_list=""
    while IFS= read -r node_name; do
        [ -z "$node_name" ] && continue
        local node_ip
        node_ip=$(awk -v node="$node_name" '
            $1 == "name:" && $2 == node {found=1; next}
            found && $1 == "ring0_addr:" {print $2; found=0}
        ' /etc/pve/corosync.conf)
        
        if [ -n "$node_ip" ]; then
            node_list+="$node_name $node_ip"$'\n'
        else
            log_error "Could not find IP for node: $node_name"
        fi
    done <<< "$ha_nodes_list"
    
    # Remove trailing newline
    node_list=$(echo "$node_list" | sed '/^$/d')
    
    if [ -z "$node_list" ]; then
        log_error "No valid node IPs found for update"
        dialog --msgbox "Error: No valid node IPs found for update" 8 40
        return 1
    fi
    
    local total_nodes=$(echo "$node_list" | wc -l)
    local updated=0
    local failed=0
    
    dialog --infobox "Updating pve-config-backup on $total_nodes nodes..." 5 60
    
    # Process each node
    while IFS= read -r line; do
        [ -z "$line" ] && continue
        
        local node=$(echo "$line" | cut -d' ' -f1)
        local ip=$(echo "$line" | cut -d' ' -f2)
        
        log_info "Updating pve-config-backup on node: $node ($ip)"
        
        # Update pve-config-backup on this node
        local result
        result=$(ssh -o StrictHostKeyChecking=no root@"$ip" "bash -s" << 'ENDSCRIPT'
# Stop the daemon before update
if [ -f /usr/local/sbin/pve-config-backup ]; then
    /usr/local/sbin/pve-config-backup stop >/dev/null 2>&1
fi

# Create backup of current version
if [ -f /usr/local/sbin/pve-config-backup ]; then
    current_ver=$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '"' | head -1)
    if [ -n "$current_ver" ]; then
        cp /usr/local/sbin/pve-config-backup "/usr/local/sbin/pve-config-backup.ver.${current_ver}" 2>/dev/null
    fi
fi

# Download new version
cd /tmp
wget https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup \
    -O /usr/local/sbin/pve-config-backup >/dev/null 2>&1

if [ $? -eq 0 ] && [ -f /usr/local/sbin/pve-config-backup ]; then
    chmod +x /usr/local/sbin/pve-config-backup
    # Reinstall and restart the service
    /usr/local/sbin/pve-config-backup install >/dev/null 2>&1
    if [ $? -eq 0 ]; then
        /usr/local/sbin/pve-config-backup start >/dev/null 2>&1
        if [ $? -eq 0 ]; then
            echo "UPDATE_SUCCESS"
        else
            echo "START_FAILED"
        fi
    else
        echo "INSTALL_FAILED" 
    fi
else
    echo "DOWNLOAD_FAILED"
fi
ENDSCRIPT
        2>/dev/null)
        
        case "$result" in
            "UPDATE_SUCCESS")
                log_info "Node $node: pve-config-backup updated successfully"
                ((updated++))
                ;;
            *)
                log_error "Node $node: pve-config-backup update failed - $result"
                ((failed++))
                ;;
        esac
    done <<< "$node_list"
    
    log_info "pve-config-backup update complete - Total: $total_nodes, Updated: $updated, Failed: $failed"
    
    if [ $updated -gt 0 ]; then
        dialog --msgbox "pve-config-backup update complete!\nTotal nodes: $total_nodes\nUpdated: $updated\nFailed: $failed\n\nNew version: $remote_version" 12 60
    else
        dialog --msgbox "pve-config-backup update failed on all nodes.\nPlease check logs for details." 10 50
    fi
    
    return 0
}

# Check and prompt for pve-config-backup updates
check_and_update_pve_config_backup() {
    local update_status
    update_status=$(check_pve_config_backup_updates)
    local check_result=$?
    
    case $check_result in
        0)  # Update available
            local current_version remote_version
            current_version=$(grep 'VERSION=' /usr/local/sbin/pve-config-backup | cut -d= -f2 | tr -d '"' | head -1)
            remote_version=$(wget -qO- https://raw.githubusercontent.com/open-e/pve-config-backup/main/pve-config-backup | grep '^VERSION=' | cut -d= -f2 | tr -d '"')
            
            dialog --title "pve-config-backup Update Available" \
                   --yes-label "Update" \
                   --no-label "Skip" \
                   --yesno "A new version of pve-config-backup is available!\n\nCurrent version: $current_version\nNew version: $remote_version\n\nWould you like to update now?" 12 60
            
            case $? in
                0)  # Yes - update
                    update_pve_config_backup
                    ;;
                1)  # No - skip
                    log_info "User declined pve-config-backup update"
                    ;;
                *)  # Error or ESC
                    log_info "pve-config-backup update dialog cancelled"
                    ;;
            esac
            ;;
        1)  # No update needed
            log_info "pve-config-backup is up to date"
            ;;
        2)  # Not installed
            log_info "pve-config-backup not installed, will be handled by ensure_pve_config_backup_running"
            ;;
        *)  # Error
            log_error "Failed to check pve-config-backup updates"
            ;;
    esac
}

# MAIN MENU SYSTEM

# Check background pve-config-backup status
check_background_status() {
    if [ -f /tmp/pve-config-backup-status ]; then
        local status_info=$(cat /tmp/pve-config-backup-status)
        if [[ "$status_info" == COMPLETED:* ]]; then
            local completed_time="${status_info#COMPLETED:}"
            PVE_CONFIG_BACKUP_STATUS="completed"
            PVE_CONFIG_BACKUP_RESULT="✓ pve-config-backup check completed at $completed_time"
        fi
    elif [ -n "$PVE_CONFIG_BACKUP_PID" ]; then
        if ! kill -0 "$PVE_CONFIG_BACKUP_PID" 2>/dev/null; then
            PVE_CONFIG_BACKUP_STATUS="failed"
            PVE_CONFIG_BACKUP_RESULT="✗ pve-config-backup check failed"
        else
            PVE_CONFIG_BACKUP_STATUS="checking"
            PVE_CONFIG_BACKUP_RESULT="⏳ pve-config-backup check in progress..."
        fi
    fi
}

# Display main menu and handle user selections
main_menu() {
    local dialog_selected=$(mktemp)
    
    # Check background status
    check_background_status
    
    # Build menu title
    local menu_title="$VENDOR $PRODUCT Proxmox VE (PVE) Tools - Main Menu"

    cmd=(dialog --keep-tite --title "$menu_title"
         --ok-label "Select" --cancel-label "Exit" --extra-button --extra-label "Setup"
         --menu "Choose an option:" 18 80 10)
    options=("0" "Add Cloned NFS Storage from $PRODUCT Snapshot"
             "1" "Restore VM/CT from Cloned NFS Storage"
             "2" "Delete Cloned NFS Storage in PVE & cloned dataset in $PRODUCT     "
             "3" "Cleanup inactive NFS storage"
             "4" "Cleanup unused disks in VM/CT config"
             "5" "Bulk VM/CT Operations – Start, Stop, Reboot, and More"
             "6" "NFS tuning - set nconnect, sync options"
             "7" "Add VM/CT to Proxmox VE HA Resources"
             "8" "Post known issues fixes and tuning"
             "9" "Help & About pve-tools")

    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Select
            case "$selected_option" in
                0)  # Add Cloned NFS Storage from JovianDSS Snapshot
                    add_nfs_storage_wizard
                    ;;
                1)  # Restore VM/CT from Cloned NFS Storage
                    restore_from_clone_wizard
                    ;;
                2)  # Delete Cloned NFS Storage
                    delete_cloned_nfs_storage_wizard
                    ;;
                3)  # Cleanup inactive NFS storage
                    cleanup_inactive_nfs_storage
                    ;;
                4)  # Cleanup unused disks in VM/CT config
                    cleanup_unused_disks_wizard
                    ;;
                5)  # VM/CT Manager - Bulk Actions
                    vm_ct_manager_bulk_actions
                    ;;
                6)  # NFS tuning - set nconnect, sync options
                    nfs_tuning_wizard
                    ;;
                7)  # Add VM/CT to Proxmox VE HA Resources
                    add_vmct_to_ha_resources
                    ;;
                8)  # Post known issues fixes and tuning
                    post_known_issues_fixes
                    ;;
                9)  # Help & About pve-tools
                    help_about_dialog
                    ;;
            esac
            ;;
        1)  # Cancel
            clear
            exit 0
            ;;
        3)  # Setup
            setup_dialog
            ;;
    esac
    
    return 0
}

# Display Help & About dialog
help_about_dialog() {
    local help_text=""
    
    # Version and release info
    help_text+="pve-tools: version $VERSION\n"
    help_text+="Release Date: $RELEASE_DATE\n"
    help_text+="\n"
    
    # Log paths information
    help_text+="LOG PATHS:\n"
    help_text+="----------\n"
    help_text+="Main log file: $LOG_FILE\n"
    help_text+="Log directory: $LOGS_DIR\n"
    help_text+="Config file: $CONFIG_FILE\n"
    help_text+="\n"
    
    # Information from CLAUDE.md
    help_text+="PROGRAM OVERVIEW:\n"
    help_text+="-----------------\n"
    help_text+="PVE Tools is a bash script that provides Proxmox VE (PVE) integration\n"
    help_text+="with $VENDOR $PRODUCT storage. This tool enables management of NFS storage\n"
    help_text+="clones from $PRODUCT snapshots for VM/CT restore if missing, or clone VM/CT\n"
    help_text+="if original one is existing. The OODP (On Off-site Data Protection) must be\n"
    help_text+="configured and running in $PRODUCT, additionally the pve-config-backup\n"
    help_text+="daemon must be running on every Proxmox VE host. The pve-tools will\n"
    help_text+="automatically install and start the pve-config-backup daemon if missing.\n"
    help_text+="The pve-config-backup daemon provides a unique mechanism for instant backup\n"
    help_text+="of VM/CT configuration files. These backed-up configuration files are stored\n"
    help_text+="in the same directory as the associated virtual disks, simplifying backup\n"
    help_text+="management. Administrators familiar with virtualization platforms where all\n"
    help_text+="VM components reside in a single directory will appreciate the similar\n"
    help_text+="convenience provided in Proxmox VE. OODP provides continuous backup and\n"
    help_text+="disaster recovery, with intervals as short as one minute. Its highly\n"
    help_text+="efficient architecture minimizes system resource usage, enabling\n"
    help_text+="uninterrupted backup operations even under heavy workloads and in\n"
    help_text+="environments hosting thousands of virtual machines (VMs) or containers (CTs).\n"
    help_text+="\n"
    help_text+="MAIN FUNCTIONS:\n"
    help_text+="---------------\n"
    help_text+="0. Add Cloned NFS Storage from $PRODUCT Snapshot\n"
    help_text+="1. Restore VM/CT from Cloned NFS Storage\n"
    help_text+="2. Delete Cloned NFS Storage in PVE & Cloned Dataset in $PRODUCT\n"
    help_text+="3. Clean up inactive NFS storage\n"
    help_text+="4. Clean up unused disks in VM/CT config\n"
    help_text+="5. Bulk VM/CT Operations – Start, Stop, Reboot, and More\n"
    help_text+="6. NFS tuning - Set nconnect, sync options\n"
    help_text+="7. Add VM/CT to Proxmox VE HA Resources\n"
    help_text+="8. Apply known issue fixes and tuning\n"
    help_text+="9. Help & About pve-tools\n"
    help_text+="\n"
    help_text+="REQUIREMENTS:\n"
    help_text+="-------------\n"
    help_text+="- Proxmox VE cluster environment\n"
    help_text+="- $VENDOR $PRODUCT storage system\n"
    help_text+="- Dialog package for TUI interface\n"
    help_text+="- REST API access to $PRODUCT (default port 82)\n"
    help_text+="\n"
    help_text+="STARTUP FEATURES:\n"
    help_text+="-----------------\n"
    help_text+="- Automatic software update checking from GitHub repository\n"
    help_text+="- Automatic pve-config-backup daemon detection on startup\n"
    help_text+="- Auto-installation of pve-config-backup from GitHub when not found\n"
    help_text+="- Background status monitoring of cluster backup daemons\n"
    
    dialog --keep-tite --title "Help & About pve-tools" \
           --ok-label "Close" \
           --msgbox "$help_text" 30 80
    
    return 0
}

# Display setup dialog for REST API configuration
setup_dialog() {
    local dialog_selected=$(mktemp)
    local form_data=$(mktemp)
    local temp_user="$rest_api_user"
    local temp_password="$rest_api_password"
    local temp_port="$rest_api_port"

    # Step 1: Get username and port
    dialog --keep-tite --title "Setup Configuration - Step 1/2" \
           --ok-label "Next" --cancel-label "Back" \
           --form "Configure $PRODUCT REST API settings:" 10 60 2 \
           "User:" 1 1 "$temp_user" 1 20 20 0 \
           "Port:" 2 1 "$temp_port" 2 20 10 0 \
           2> "$form_data"

    local exit_code=$?

    if [ $exit_code -eq 0 ]; then
        # Parse username and port
        local line_count=1
        while IFS= read -r line; do
            case $line_count in
                1) temp_user="$line" ;;
                2) temp_port="$line" ;;
            esac
            ((line_count++))
        done < "$form_data"

        # Step 2: Get password with masking
        local masked_password=$(printf '%*s' ${#temp_password} | tr ' ' '*')
        dialog --keep-tite --title "Setup Configuration - Step 2/2" \
               --ok-label "Save" --cancel-label "Back" \
               --insecure --passwordbox "Enter REST API password (supports copy&paste):\n" 12 70 \
               2> "$form_data"

        exit_code=$?

        if [ $exit_code -eq 0 ]; then
            temp_password=$(cat "$form_data")
            
            # Only update if password is not empty
            if [ -n "$temp_password" ]; then
                rest_api_user="$temp_user"
                rest_api_password="$temp_password"
                rest_api_port="$temp_port"

                log_info "User saved configuration changes"
                log_debug "setup_config: Parsed user='$rest_api_user'"
                log_debug "setup_config: Parsed password length=${#rest_api_password}"
                log_debug "setup_config: Parsed port='$rest_api_port'"

                # Save to config file - properly escape the password to handle special characters
                cat > "$CONFIG_FILE" << EOF
rest_api_user=$rest_api_user
rest_api_password='$rest_api_password'
rest_api_port=$rest_api_port
EOF

                log_info "Configuration saved to $CONFIG_FILE"
                dialog --msgbox "Configuration saved successfully!" 6 40
            else
                dialog --msgbox "Password cannot be empty. Configuration not saved." 6 50
            fi
        fi
    fi
    
    rm -f "$form_data"
    rm -f "$dialog_selected"
    return 0
}

# RESTORE FROM CLONE WIZARD FUNCTIONS

get_ha_nodes() {
    local ha_nodes
    ha_nodes=$(sed -n '/^group: ha-nodes$/,/^group:/p' /etc/pve/ha/groups.cfg | \
        grep -E '\s+nodes\s' | \
        cut -d' ' -f2 | tr ',' '\n' | sort)
    if [[ -n "$ha_nodes" ]]; then
        echo "$ha_nodes"
    else
        pvecm nodes 2>/dev/null | grep -E '\s+[0-9]+\s+[0-9]+\s+\S+' | awk '{print $3}' | sort
    fi
}

# Determine configuration file type (VM or Container)
config_file_type() {
    local config_file="$1"
    
    if [ -z "$config_file" ]; then
        log_error "config_file_type: No file path provided"
        return 1
    fi
    
    if [ ! -f "$config_file" ]; then
        log_error "config_file_type: File not found: $config_file"
        return 1
    fi
    
    # Check for VM-specific configuration
    if grep -q '^vmgenid:' "$config_file"; then
        echo "vm"
    # Check for Container-specific configuration
    elif grep -q '^rootfs:' "$config_file"; then
        echo "ct"
    else
        echo "unknown"
    fi
    
    return 0
}

# List existing VM/LXC IDs from Proxmox configuration files
existing_id() {
    log_debug "Getting existing VM/CT IDs from Proxmox configuration"
    
    find /etc/pve/nodes/*/{qemu-server,lxc} -name '*.conf' \
        -printf '%f\n' 2>/dev/null | \
    sed 's/\.conf$//' | \
    sort -n | \
    uniq
}

# Check if VM/CT ID exists in PVE repository
check_vmct_exists() {
    local vmid="$1"
    
    if [ -z "$vmid" ]; then
        log_error "check_vmct_exists: No VM/CT ID provided"
        return 1
    fi
    
    log_debug "Checking if VM/CT ID $vmid exists in PVE repository"
    
    # Check if config file exists in any node
    if find /etc/pve/nodes/*/{qemu-server,lxc} -name "${vmid}.conf" 2>/dev/null | grep -q .; then
        log_debug "VM/CT ID $vmid exists in PVE repository"
        return 0
    else
        log_debug "VM/CT ID $vmid does not exist in PVE repository"
        return 1
    fi
}

# Get next available VM/CT ID
get_available_vmct_id() {
    local start_id="${1:-100}"
    local current_id="$start_id"
    
    log_debug "Finding next available VM/CT ID starting from $start_id"
    
    # Get list of existing IDs
    local existing_ids
    existing_ids=$(existing_id)
    
    # Find first available ID
    while check_vmct_exists "$current_id"; do
        ((current_id++))
    done
    
    log_debug "Next available VM/CT ID: $current_id"
    echo "$current_id"
    return 0
}

# List all configuration files in mounted PVE storage
all_mnt_confs() {
    log_debug "Getting all mounted configuration files"
    
    find /mnt/pve/*/images/* -maxdepth 1 -name '*.conf' 2>/dev/null | sort
    
    return 0
}


get_id_from_conf_file() {
  local conf="$1"
  awk -F/ '{ sub(/\.conf$/, "", $NF); print $NF }' <<< "$conf"
}

get_storage_name_from_mnt_conf_file() {
  local conf="$1"
  # the storage name is the 3rd path component (after “mnt” and “pve”), which is $4 when splitting on “/”
  awk -F/ '{ print $4 }' <<< "$conf"
}

get_etc_conf_file_for_id() {
    local vmid="$1"
    local conf

    # Check in both qemu-server and lxc dirs
    for conf in /etc/pve/nodes/*/qemu-server/"${vmid}".conf /etc/pve/nodes/*/lxc/"${vmid}".conf; do
        if [[ -f "$conf" ]]; then
            echo "$conf"
            return 0
        fi
    done

    # If we get here, no file was found
    log_error "get_etc_conf_file_for_id: no config file found for VMID ${vmid}"
    return 1
}

get_storage_name_of_id() {
    local id="$1"
    local etc_conf conf_type storage_name

    # 1) locate the /etc/pve config file
    etc_conf=$(get_etc_conf_file_for_id "$id") || return 1

    # 2) determine if it's a CT or VM
    conf_type=$(config_file_type "$etc_conf") || return 1

    case "$conf_type" in
        ct)
            # parse the storage name from rootfs:
            storage_name=$(
                awk -F: '/^rootfs:/ {print $2}' "$etc_conf" \
                | awk -F: '{print $1}' \
                | xargs
            )
            echo "$storage_name"
            ;;
        vm)
            # use the more specific pattern to find the first VM disk line
            storage_name=$(
                grep -E "$DISK_PATTERN" "$etc_conf" \
                | head -n1 \
                | awk -F: '{print $2}' \
                | xargs
            )
            echo "$storage_name"
            ;;
        *)
            log_error "get_storage_name_of_id: unknown config type '$conf_type' in $etc_conf"
            return 1
            ;;
    esac

    return 0
}

is_mnt_conf_file_form_existing_vmct() {
    local mnt_conf_file="$1"
    local vmid etc_conf storage_etc storage_mnt

    # 1) sanity check
    if [[ -z "$mnt_conf_file" ]]; then
        log_error "get_storage_name_from_mnt_conf_file: Usage: $FUNCNAME /mnt/pve/.../<vmid>.conf"
        return 2
    fi
    if [[ ! -f "$mnt_conf_file" ]]; then
        log_error "get_storage_name_from_mnt_conf_file: File not found: $mnt_conf_file"
        return 1
    fi

    # 2) extract ID
    vmid=$(get_id_from_conf_file "$mnt_conf_file")

    # 3) find /etc/pve conf for that ID
    etc_conf=$(get_etc_conf_file_for_id "$vmid") || return 1

    # 4) get the storage name from the /etc/pve config
    storage_etc=$(get_storage_name_of_id "$vmid") || return 1

    # 5) get the storage name from the mounted .conf path
    storage_mnt=$(get_storage_name_from_mnt_conf_file "$mnt_conf_file")

    # 6) compare and return
    if [[ "$storage_etc" == "$storage_mnt" ]]; then
        return 0
    else
        return 1
    fi
}


# this is NEW get to restore func
get_cloned_config_file_list() {
    local config_files=()
    
    for storage in $(get_cloned_storage_list); do
        # Find VM disk files and extract VM IDs
        for disk_file in /mnt/pve/"$storage"/images/*/vm-[0-9]*-disk-*; do
            if [[ -f "$disk_file" ]]; then
                # Extract VM ID from disk filename (vm-201-disk-0.qcow2 -> 201)
                local vm_id=$(basename "$disk_file" | sed -n 's/vm-\([0-9]\+\)-disk-.*/\1/p')
                if [[ -n "$vm_id" ]]; then
                    # Look for corresponding config file in the same directory
                    local vm_dir=$(dirname "$disk_file")
                    local config_file="$vm_dir/$vm_id.conf"
                    if [[ -f "$config_file" ]]; then
                        config_files+=("$config_file")
                    fi
                fi
            fi
        done
    done
    
    # Remove duplicates and sort
    printf '%s\n' "${config_files[@]}" | sort | uniq
}


# Step 1: Select configuration files for restoration
step1_select_config() {
    local line confs=() menu_items=() vmids=()

    # Use cached config list if available, otherwise get fresh list
    if [ "${#cached_config_list[@]}" -eq 0 ]; then
        while IFS= read -r line; do
            cached_config_list+=("$line")
            log_debug "Added config file to cache: $line"
        done < <(get_cloned_config_file_list)
    fi
    
    # Copy cached list to working array
    confs=("${cached_config_list[@]}")

    if [ "${#confs[@]}" -eq 0 ]; then
        dialog --msgbox "No VMs/CTs found to restore." 8 40
        return 1
    fi

    # First pass: collect all storage names and VM/CT names to calculate max lengths
    local storage_names=() vmct_names=()
    for ((i=0; i<${#confs[@]}; i++)); do
        local storage_name=$(get_storage_name_from_mnt_conf_file "${confs[i]}")
        storage_names+=("$storage_name")
        
        # Get VM/CT name from config file
        local vmct_name=$(grep -E "^name[[:space:]]*[:=]" "${confs[i]}" 2>/dev/null | sed -E 's/^name[[:space:]]*[:=][[:space:]]*//' | head -1)
        if [[ -z "$vmct_name" ]]; then
            vmct_name="(no name)"
        fi
        vmct_names+=("$vmct_name")
    done

    # Calculate maximum storage name length for alignment
    local max_storage_len=0
    for storage_name in "${storage_names[@]}"; do
        if [ ${#storage_name} -gt $max_storage_len ]; then
            max_storage_len=${#storage_name}
        fi
    done
    
    # Calculate maximum VM/CT name length for alignment
    local max_vmct_name_len=0
    for vmct_name in "${vmct_names[@]}"; do
        if [ ${#vmct_name} -gt $max_vmct_name_len ]; then
            max_vmct_name_len=${#vmct_name}
        fi
    done

    local i vmid seen_vmids=() vmid_to_conf_map=()
    for ((i=0; i<${#confs[@]}; i++)); do
        # Extract VM/CT ID from path like /mnt/pve/storage/images/103/103.conf
        local dir_name="$(dirname "${confs[i]}")"
        vmid=$(basename "$dir_name")
        
        # Skip if we've already seen this VM ID
        local skip=false
        for seen_vmid in "${seen_vmids[@]}"; do
            if [[ "$seen_vmid" == "$vmid" ]]; then
                skip=true
                break
            fi
        done
        
        if [[ "$skip" == true ]]; then
            continue
        fi
        
        seen_vmids+=("$vmid")
        vmids+=("$vmid")
        # Map vmid to original config file path
        vmid_to_conf_map+=("${confs[i]}")

        # Get VM/CT type
        local vm_type=$(config_file_type "${confs[i]}")
        
        # Get VM/CT name from pre-calculated array
        local vmct_name="${vmct_names[i]}"
        
        # Get storage name
        local storage_name="${storage_names[i]}"
        
        # Get NFS IP address
        local nfs_ip=$(get_nfs_server_ip_of_given_storage "$storage_name")
        
        # Format storage name with proper alignment
        local formatted_storage=$(printf "%-${max_storage_len}s" "$storage_name")
        
        # Calculate spacing after parentheses to align columns
        local name_with_parens="($vmct_name)"
        local total_name_width=$((max_vmct_name_len + 2))  # +2 for parentheses
        local padding_needed=$((total_name_width - ${#name_with_parens}))
        local padding=$(printf "%*s" $padding_needed "")
        
        # Create display string with parentheses aligned to name: "vm/ct (vmct_name)  storage_name  nfs_ip"
        local display_string=$(printf "%-2s %s%s  %s  %s" "$vm_type" "$name_with_parens" "$padding" "$formatted_storage" "$nfs_ip")

        # Check if this config was previously selected
        local status="off"
        for selected_conf in "${selected_confs[@]}"; do
            if [[ "$selected_conf" == "${confs[i]}" ]]; then
                status="on"
                break
            fi
        done

        menu_items+=("$vmid" "$display_string" "$status")
    done

    local dialog_selected=$(mktemp)
    local select_label="Select All"
    if [[ "$select_all_mode" == true ]]; then
        select_label="Deselect All"
    fi

    cmd=(dialog --keep-tite --title "Step 1/$total_steps: Select VM/CT for Restore"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "$select_label"
         --checklist "Choose VM/CT for restore (use SPACE to select/deselect):" 20 100 10)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select at least one VM/CT to restore." 8 50
                return 2  # Stay on same step
            fi

            # Parse selected VM IDs and find corresponding config files
            selected_confs=()
            IFS=' ' read -ra selected_vmids <<< "$selected_option"
            for selected_vmid in "${selected_vmids[@]}"; do
                # Remove quotes if present
                selected_vmid=$(echo "$selected_vmid" | tr -d '"')
                for ((i=0; i<${#vmids[@]}; i++)); do
                    if [[ "${vmids[i]}" == "$selected_vmid" ]]; then
                        selected_confs+=("${vmid_to_conf_map[i]}")
                        break
                    fi
                done
            done
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Select All / Deselect All
            if [[ "$select_all_mode" == false ]]; then
                # Select all
                selected_confs=("${confs[@]}")
                select_all_mode=true
            else
                # Deselect all
                selected_confs=()
                select_all_mode=false
            fi
            return 2  # Stay on same step
            ;;
    esac
}

step2_select_host() {
    local line hosts=() menu_items=()
    while IFS= read -r line; do
        hosts+=("$line")
    done < <(get_ha_nodes)

    if [ "${#hosts[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE hosts found." 8 40
        return 1
    fi

    local i
    for ((i=0; i<${#hosts[@]}; i++)); do
        menu_items+=("$i" "${hosts[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_host_index" ]]; then
        extra_button="--default-item $selected_host_index"
    fi

    cmd=(dialog --keep-tite --title "Step 2/$total_steps: Select Target Host"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose PVE host to restore to:" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_host_index="$selected_option"
            selected_host="${hosts[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

step3_summary() {
    local summary_text="Summary of selections:\n\n"
    summary_text+="Target host: $selected_host\n"
    summary_text+="Selected VM/CT configs (${#selected_confs[@]} total):\n\n"

    local i=1
    for conf in "${selected_confs[@]}"; do
        local dir_name="$(dirname "$conf")"
        local vmid=$(basename "$dir_name")
        local config_type=$(config_file_type "$conf")
        summary_text+="  $i) VM/CT ID: $vmid ($config_type)\n"
        summary_text+="     Config: $conf\n"
        ((i++))
    done

    summary_text+="\nClick 'Apply' to proceed with restoration or 'Back' to modify selections."

    dialog --keep-tite --title "Step 3/$total_steps: Confirmation" \
           --ok-label "Apply" --cancel-label "Back to Main" --extra-button --extra-label "Back" \
           --msgbox "$summary_text" 25 100

    case $? in
        0)  # Apply
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

restore_ct() {
    local src_conf="$1"
    local host_name="$2"
    local new_ctid="$3"  # Optional new CT ID
    local storage_name
    storage_name="$(echo "$src_conf" | awk -F'/' '{print $(NF-3)}')"
    local ctid
    
    # Use new ID if provided, otherwise use original ID from config path
    if [ -n "$new_ctid" ]; then
        ctid="$new_ctid"
        log_info "Using new CT ID: $ctid for restoration"
    else
        ctid="$(echo "$src_conf" | awk -F'/' '{print $(NF-1)}')"
        log_info "Using original CT ID: $ctid for restoration"
    fi
    
    local dst_conf="/etc/pve/nodes/${host_name}/lxc/${ctid}.conf"
    
    log_info "Restoring CT $ctid from $src_conf to $dst_conf"
    
    # Copy configuration file
    if ! cp "$src_conf" "$dst_conf"; then
        log_error "Failed to copy CT configuration from $src_conf to $dst_conf"
        return 1
    fi
    
    # Update storage reference in config
    local current_storage_name
    current_storage_name=$(awk -F: '/^rootfs:/ {print $2}' "$dst_conf" | awk -F: '{print $1}' | awk '{$1=$1};1')
    
    if [ -n "$current_storage_name" ]; then
        sed -i "s|^rootfs: *${current_storage_name}:|rootfs: ${storage_name}:|" "$dst_conf"
        log_info "Updated storage reference from $current_storage_name to $storage_name in CT $ctid config"
    else
        log_error "Could not find rootfs storage reference in CT $ctid config"
        return 1
    fi
    
    # Start the start-and-stop sequence in background
    # Use nohup to fully detach the job
    nohup bash -c "pct start $ctid; pct stop $ctid" > "$LOGS_DIR/restore_ct_${ctid}.log" 2>&1 < /dev/null &
    
    log_info "CT $ctid restored"
    return 0
}


# Regenerate MAC addresses for network interfaces in VM config
regenerate_mac_addresses() {
    local config_file="$1"
    
    if [ ! -f "$config_file" ]; then
        log_error "regenerate_mac_addresses: Config file not found: $config_file"
        return 1
    fi
    
    log_info "Regenerating MAC addresses for VM config: $config_file"
    
    # Find all network interface lines (net0, net1, etc.) with MAC addresses
    # Handle both formats: macaddr=XX:XX:XX:XX:XX:XX and hwaddr=XX:XX:XX:XX:XX:XX
    local mac_count=0
    
    # Process each network interface line
    while IFS= read -r line; do
        if [[ "$line" =~ ^net[0-9]+: ]]; then
            local net_num=$(echo "$line" | grep -oE "^net[0-9]+" | grep -oE "[0-9]+")
            local old_mac=""
            local mac_param=""
            
            # Check for virtio=MAC pattern (Proxmox format)
            if [[ "$line" =~ virtio=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="virtio"
            # Check for e1000=MAC pattern
            elif [[ "$line" =~ e1000=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="e1000"
            # Check for rtl8139=MAC pattern
            elif [[ "$line" =~ rtl8139=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="rtl8139"
            # Check for macaddr= pattern (alternative format)
            elif [[ "$line" =~ macaddr=([0-9A-Fa-f:]+) ]]; then
                old_mac="${BASH_REMATCH[1]}"
                mac_param="macaddr"
            fi
            
            if [[ -n "$old_mac" ]]; then
                # Generate new random MAC address (using Proxmox's range)
                # Proxmox uses prefix BC:24:11 for auto-generated MACs
                local new_mac=$(printf "BC:24:11:%02X:%02X:%02X" $((RANDOM % 256)) $((RANDOM % 256)) $((RANDOM % 256)))
                
                # Replace the MAC address in the config file
                sed -i "s/${mac_param}=${old_mac}/${mac_param}=${new_mac}/g" "$config_file"
                
                log_info "Regenerated MAC address for net${net_num}: ${old_mac} -> ${new_mac}"
                ((mac_count++))
            fi
        fi
    done < "$config_file"
    
    log_info "Total MAC addresses regenerated: $mac_count"
    
    return 0
}

restore_vm() {
    local src_conf="$1"
    local host_name="$2"
    local new_vmid="$3"  # Optional new VM ID
    local storage_name
    storage_name="$(echo "$src_conf" | awk -F'/' '{print $(NF-3)}')"
    local vmid
    
    # Use new ID if provided, otherwise use original ID from config path
    if [ -n "$new_vmid" ]; then
        vmid="$new_vmid"
        log_info "Using new VM ID: $vmid for restoration"
    else
        vmid="$(echo "$src_conf" | awk -F'/' '{print $(NF-1)}')"
        log_info "Using original VM ID: $vmid for restoration"
    fi
    
    local dst_conf="/etc/pve/nodes/${host_name}/qemu-server/${vmid}.conf"
    
    log_info "Restoring VM $vmid from $src_conf to $dst_conf"
    
    # Copy configuration file
    if ! cp "$src_conf" "$dst_conf"; then
        log_error "Failed to copy VM configuration from $src_conf to $dst_conf"
        return 1
    fi

    # Update storage references in config
    local current_storage_name
    current_storage_name="$(grep -E "^${DISK_TYPES}[0-9]+:\s+[^:]+:[0-9]+/vm" "$dst_conf" | head -1 | awk -F':' '{print $2}' | awk '{$1=$1};1')"

    if [[ -n "$current_storage_name" ]]; then
        sed -i -E "s@^(${DISK_TYPES}[0-9]+:) *${current_storage_name}:@\1${storage_name}:@g" "$dst_conf"
        log_info "Updated storage reference from $current_storage_name to $storage_name in VM $vmid config"
    else
        log_error "Could not find storage reference in VM $vmid config"
        return 1
    fi
    
    # Regenerate MAC addresses for network interfaces
    regenerate_mac_addresses "$dst_conf"
    
    log_info "VM $vmid restored"
    return 0
}


perform_restore() {
    local total=${#selected_confs[@]}
    local current=0
    local failed=0
    local success=0
    local skipped=0
    local any_new_ids_used=false

    for conf in "${selected_confs[@]}"; do
        ((current++))
        local dir_name="$(dirname "$conf")"
        local vmid=$(basename "$dir_name")
        local type=$(config_file_type "$conf")

        # Show progress
        local type_upper=$(upper "$type")
        dialog --title "Restoring..." --infobox "Processing $type_upper $vmid ($current/$total)\nType: $type\nFile: $conf" 8 100
        sleep 1

        log_info "Processing $type_upper $vmid ($current/$total) - Type: $type"

        # Check if VM/CT already exists in PVE repository
        if check_vmct_exists "$vmid"; then
            log_info "VM/CT $vmid already exists - initiating conflict resolution"
            
            # Reset conflict resolution variables
            new_vmct_id=""
            conflict_selected_host=""
            
            # Show conflict resolution dialog
            if resolve_vmct_conflict "$vmid" "$type"; then
                # User provided new ID, now ask for host selection
                if select_host_for_conflict "$new_vmct_id" "$type"; then
                    # User selected host, proceed with restore using new ID and host
                    log_info "Restoring VM/CT $vmid as $new_vmct_id to host $conflict_selected_host"
                    
                    case "$type" in
                        ct)
                            if restore_ct "$conf" "$conflict_selected_host" "$new_vmct_id"; then
                                ((success++))
                                any_new_ids_used=true
                                log_info "Successfully restored CT $vmid as $new_vmct_id"
                            else
                                ((failed++))
                                log_error "Failed to restore CT $vmid as $new_vmct_id"
                            fi
                            ;;
                        vm)
                            if restore_vm "$conf" "$conflict_selected_host" "$new_vmct_id"; then
                                ((success++))
                                any_new_ids_used=true
                                log_info "Successfully restored VM $vmid as $new_vmct_id"
                            else
                                ((failed++))
                                log_error "Failed to restore VM $vmid as $new_vmct_id"
                            fi
                            ;;
                        *)
                            ((failed++))
                            log_error "Unknown VM/CT type: $type"
                            ;;
                    esac
                else
                    # User cancelled host selection
                    ((skipped++))
                fi
            else
                # User cancelled conflict resolution
                ((skipped++))
            fi
        else
            # VM/CT doesn't exist, proceed with normal restore
            log_info "VM/CT $vmid does not exist - proceeding with normal restore"
            
            case "$type" in
                ct)
                    if restore_ct "$conf" "$selected_host"; then
                        ((success++))
                        log_info "Successfully restored CT $vmid"
                    else
                        ((failed++))
                        log_error "Failed to restore CT $vmid"
                    fi
                    ;;
                vm)
                    if restore_vm "$conf" "$selected_host"; then
                        ((success++))
                        log_info "Successfully restored VM $vmid"
                    else
                        ((failed++))
                        log_error "Failed to restore VM $vmid"
                    fi
                    ;;
                *)
                    ((failed++))
                    log_error "Unknown VM/CT type: $type"
                    ;;
            esac
        fi
    done

    # Need to initiate fresh config files backup using pve-config-backup daemon
    log_info "Stopping pve-config-backup daemon"
    pve-config-backup stop >> /var/log/pve-tools/pve-tools.log 2>&1
    log_info "Starting pve-config-backup daemon"
    pve-config-backup start >> /var/log/pve-tools/pve-tools.log 2>&1

    # Show final result
    local result_text="Restoration completed!\n\n"
    result_text+="Total processed: $total\n"
    result_text+="Successful: $success\n"
    result_text+="Failed: $failed\n"
    result_text+="Skipped: $skipped\n"
    result_text+="Target host: $selected_host"

    log_info "Restoration completed - Total: $total, Success: $success, Failed: $failed, Skipped: $skipped"
    dialog --msgbox "$result_text" 15 50
    
    # Show static IP address warning only if any VMs were restored with new IDs (due to conflicts)
    if [ $success -gt 0 ] && [ "$any_new_ids_used" = true ]; then
        local warning_text="IMPORTANT: Static IP Address Configuration\n\n"
        warning_text+="If the restored VM use static IP addresses, you must:\n\n"
        warning_text+="1. Start the VM with network isolated/disconnected\n"
        warning_text+="2. Access the VM console\n"
        warning_text+="3. Change static IP addresses to unique values\n"
        warning_text+="4. Reconnect network after IP configuration\n\n"
        warning_text+="This is not required if the VM use DHCP for IP assignment.\n\n"
        warning_text+="To confirm you have read this important information,\ntype 'noted':"
        
        local confirm_input=$(mktemp)
        dialog --ok-label "OK" --no-cancel \
               --inputbox "$warning_text" 22 70 2> "$confirm_input"
        local confirmation=$(cat "$confirm_input" | tr -d ' \t\n\r')
        rm -f "$confirm_input"
        
        # Keep showing dialog until user types "noted"
        while [[ "$confirmation" != "noted" ]]; do
            dialog --ok-label "OK" --no-cancel \
                   --inputbox "$warning_text" 20 70 2> "$confirm_input"
            confirmation=$(cat "$confirm_input" | tr -d ' \t\n\r')
            rm -f "$confirm_input"
        done
    fi
}

restore_from_clone_wizard() {
    # Reset variables for restore wizard
    selected_confs=()
    selected_host=""
    selected_host_index=""
    current_step=1
    select_all_mode=false
    storage_discovery_done=false
    cached_config_list=()  # Clear cached config list
    
    # Reset conflict resolution variables
    new_vmct_id=""
    conflict_selected_host=""
    
    # Ensure storage configuration is synchronized (only once)
    if [[ "$storage_discovery_done" == false ]]; then
        dialog --title "Please wait..." --infobox "Synchronizing storage discovery..." 5 50
        sync_storage_discovery
        storage_discovery_done=true
    fi

    while true; do
        case $current_step in
            1)
                step1_select_config
                case $? in
                    0)  # Next
                        current_step=2
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Stay on same step (retry)
                        ;;
                esac
                ;;
            2)
                step2_select_host
                case $? in
                    0)  # Next
                        current_step=3
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        current_step=1
                        ;;
                esac
                ;;
            3)
                step3_summary
                case $? in
                    0)  # Apply
                        perform_restore
                        return
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        current_step=2
                        ;;
                esac
                ;;
        esac
    done
}

# ADD NFS STORAGE WIZARD FUNCTIONS

# NFS wizard specific variables
selected_nfs_ip=""
selected_pool=""
selected_dataset=""
selected_snapshot=""
selected_nfs_ip_index=""
selected_pool_index=""
selected_dataset_index=""
selected_snapshot_index=""
nfs_current_step=1
nfs_total_steps=5

rest_api_connection_test() {
    local response status

    log_debug "rest_api_connection_test: Starting connection test"
    log_debug "rest_api_connection_test: Target IP: $selected_nfs_ip"
    log_debug "rest_api_connection_test: Target port: $rest_api_port"
    log_debug "rest_api_connection_test: Username: $rest_api_user"
    log_debug "rest_api_connection_test: Password length: ${#rest_api_password}"
    
    local url="https://$selected_nfs_ip:$rest_api_port/api/v4/conn_test"
    log_debug "rest_api_connection_test: Full URL: $url"
    
    response=$(curl -k -s -w "\n%{http_code}" -u "$rest_api_user:$rest_api_password" \
        "$url" 2>&1)
    local curl_exit=$?
    
    log_debug "rest_api_connection_test: curl exit code: $curl_exit"
    log_debug "rest_api_connection_test: raw response: $response"
    
    status=$(echo "$response" | tail -n1)
    local body=$(echo "$response" | sed '$d')
    
    log_debug "rest_api_connection_test: HTTP status: $status"
    log_debug "rest_api_connection_test: Response body: $body"

    if [[ "$status" == "200" ]]; then
        log_info "rest_api_connection_test: Connection test successful"
        return 0
    else
        log_error "REST API connection test failed (HTTP $status)"
        log_error "REST API response body: $body"
        return 1
    fi
}

get_custom_ips() {
    if [[ -f "$CONFIG_FILE" ]]; then
        grep "^custom_ips=" "$CONFIG_FILE" 2>/dev/null | cut -d'=' -f2- | tr ',' '\n' | grep -v '^$'
    fi
}

validate_ip() {
    local ip="$1"
    
    # Check basic format
    if [[ ! "$ip" =~ ^([0-9]{1,3}\.){3}[0-9]{1,3}$ ]]; then
        return 1
    fi
    
    # Check each octet is in valid range (0-255)
    IFS='.' read -ra octets <<< "$ip"
    for octet in "${octets[@]}"; do
        # Remove leading zeros and check range
        octet=$((10#$octet))
        if [[ $octet -lt 0 || $octet -gt 255 ]]; then
            return 1
        fi
    done
    
    return 0
}

ip_already_exists() {
    local check_ip="$1"
    local existing_ips
    
    # Check custom IPs from config
    existing_ips=$(get_custom_ips)
    if [[ -n "$existing_ips" ]]; then
        while IFS= read -r ip; do
            if [[ "$ip" == "$check_ip" ]]; then
                return 0
            fi
        done <<< "$existing_ips"
    fi
    
    # Check IPs from storage.cfg
    local storage_cfg="/etc/pve/storage.cfg"
    if [[ -f "$storage_cfg" ]]; then
        while IFS= read -r ip; do
            if [[ "$ip" == "$check_ip" ]]; then
                return 0
            fi
        done < <(grep server "$storage_cfg" | awk '{print $2}')
    fi
    
    return 1
}

add_custom_ip() {
    local new_ip="$1"
    local current_ips
    current_ips=$(get_custom_ips | tr '\n' ',' | sed 's/,$//')
    
    # Check if IP already exists
    if [[ -n "$current_ips" && "$current_ips" == *"$new_ip"* ]]; then
        return 0
    fi
    
    # Add new IP
    if [[ -n "$current_ips" ]]; then
        current_ips="$current_ips,$new_ip"
    else
        current_ips="$new_ip"
    fi
    
    # Update config file
    if grep -q "^custom_ips=" "$CONFIG_FILE" 2>/dev/null; then
        sed -i "s/^custom_ips=.*/custom_ips=$current_ips/" "$CONFIG_FILE"
    else
        echo "custom_ips=$current_ips" >> "$CONFIG_FILE"
    fi
    
    log_info "Added custom IP: $new_ip"
}

remove_custom_ip() {
    local ip_to_remove="$1"
    local current_ips
    current_ips=$(get_custom_ips | tr '\n' ',' | sed 's/,$//')
    
    # Remove the IP
    current_ips=$(echo "$current_ips" | sed "s/$ip_to_remove,//g" | sed "s/,$ip_to_remove//g" | sed "s/^$ip_to_remove$//g")
    
    # Update config file
    if [[ -n "$current_ips" ]]; then
        sed -i "s/^custom_ips=.*/custom_ips=$current_ips/" "$CONFIG_FILE"
    else
        sed -i "/^custom_ips=/d" "$CONFIG_FILE"
    fi
    
    log_info "Removed custom IP: $ip_to_remove"
}

manage_custom_ips() {
    local custom_ips_array=()
    local menu_items=()
    
    while IFS= read -r line; do
        [[ -n "$line" ]] && custom_ips_array+=("$line")
    done <<< "$(get_custom_ips)"
    
    if [[ ${#custom_ips_array[@]} -eq 0 ]]; then
        dialog --msgbox "No custom IPs found." 8 40
        return
    fi
    
    for ((i=0; i<${#custom_ips_array[@]}; i++)); do
        menu_items+=("$i" "${custom_ips_array[i]}")
    done
    
    local selected_ip
    selected_ip=$(dialog --keep-tite --title "Manage Custom IPs" \
        --ok-label "Delete" --cancel-label "Back" \
        --menu "Select IP to delete:" 15 50 8 \
        "${menu_items[@]}" 3>&1 1>&2 2>&3)
    
    local dialog_result=$?
    
    # Only proceed if user selected an IP (not cancelled)
    if [[ $dialog_result -eq 0 && -n "$selected_ip" ]]; then
        local ip_to_delete="${custom_ips_array[selected_ip]}"
        # Ask for confirmation before deletion
        if dialog --yesno "Delete IP: $ip_to_delete?" 8 40; then
            remove_custom_ip "$ip_to_delete"
            # Only show success message after actual deletion
            dialog --msgbox "IP $ip_to_delete deleted." 8 40
        fi
    fi
}

get_current_storage_servers_ip() {
    local storage_cfg="/etc/pve/storage.cfg"
    local ips=()
    while IFS= read -r line; do
        [[ -n "$line" ]] && ips+=("$line")
    done < <(grep server "$storage_cfg" | awk '{print $2}')

    # Add custom IPs from config file
    local custom_ips
    custom_ips=$(get_custom_ips)
    if [[ -n "$custom_ips" ]]; then
        while IFS= read -r line; do
            [[ -n "$line" ]] && ips+=("$line")
        done <<< "$custom_ips"
    fi

    # Remove duplicates
    readarray -t uniq_ips < <(printf "%s\n" "${ips[@]}" | sort -u)
    uniq_ips+=("Enter new IP")
    uniq_ips+=("Manage custom IPs")

    printf '%s\n' "${uniq_ips[@]}"
}

get_pool_names() {
    local addr="$1"
    local response

    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" -H 'Content-Type: application/json' \
        "https://$addr:$rest_api_port/api/v4/pools")

    local curl_exit=$?

    if [[ $curl_exit -ne 0 ]]; then
        log_error "test_nfs_connection: Failed to connect to NFS server at $addr:$rest_api_port"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "test_nfs_connection: Empty response from server"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get("data", [])
    if not pools:
        sys.exit(1)
    for pool in pools:
        name = pool.get("name")
        if name:
            print(name)
except Exception:
    sys.exit(1)
'
}

get_datasets_in_pool() {
    local pool_name="$1"
    local response

    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" -H 'Content-Type: application/json' \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_datasets_in_pool: Failed to connect to server for datasets"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_datasets_in_pool: Empty response from server (datasets)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)
    for d in entries:
        name = d.get("name")
        origin = d.get("origin")
        if name and origin is None:
            print(name)
except Exception:
    sys.exit(1)
'
}

get_snapshots_in_dataset() {
    local pool_name="$1"
    local dataset_name="$2"
    local response

    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" -H 'Content-Type: application/json' \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_snapshots_in_dataset: Failed to connect to server for snapshots"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_snapshots_in_dataset: Empty response from server (snapshots)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)

    # Create list of snapshots with creation timestamps
    snapshots = []
    for s in entries:
        name = s.get("name")
        creation = s.get("properties", {}).get("creation")
        if name and creation:
            snapshots.append((int(creation), name))

    # Sort by creation timestamp in descending order (newest first)
    snapshots.sort(reverse=True)

    # Print snapshot names in sorted order
    for _, name in snapshots:
        print(name)
except Exception:
    sys.exit(1)
'
}

get_snapshots_with_times() {
    local pool_name="$1"
    local dataset_name="$2"
    local response

    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" -H 'Content-Type: application/json' \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc")

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_snapshots_with_times: Failed to connect to server for snapshots"
        return 1
    fi

    if [[ -z "$response" ]]; then
        log_error "get_snapshots_with_times: Empty response from server (snapshots)"
        return 1
    fi

    printf '%s' "$response" | python3 -c '
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get("data", {}).get("entries", [])
    if not entries:
        sys.exit(1)

    # Create list of snapshots with creation timestamps
    snapshots = []
    for s in entries:
        name = s.get("name")
        creation = s.get("properties", {}).get("creation")
        if name and creation:
            snapshots.append((int(creation), name))

    # Sort by creation timestamp in descending order (newest first)
    snapshots.sort(reverse=True)

    # Print snapshot names and creation times tab-separated
    for creation, name in snapshots:
        print(f"{name}\t{creation}")
except Exception:
    sys.exit(1)
'
}

get_current_timestamp() {
    date +%s
}

calculate_age_seconds() {
    local current_time="$1"
    local creation_time="$2"
    echo $((current_time - creation_time))
}

format_age_human() {
    local age_seconds="$1"
    local age_minutes=$((age_seconds / 60))
    local age_hours=$((age_minutes / 60))
    local age_days=$((age_hours / 24))
    local age_years=$((age_days / 365))
    local output=""
    
    if [[ $age_minutes -lt 60 ]]; then
        echo "${age_minutes}min"
    elif [[ $age_hours -lt 24 ]]; then
        local remaining_minutes=$((age_minutes % 60))
        output="${age_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    elif [[ $age_days -lt 365 ]]; then
        local remaining_hours=$((age_hours % 24))
        local remaining_minutes=$((age_minutes % 60))
        output="${age_days}d"
        [[ $remaining_hours -gt 0 ]] && output+=" ${remaining_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    else
        local remaining_days=$((age_days % 365))
        local remaining_hours=$((age_hours % 24))
        local remaining_minutes=$((age_minutes % 60))
        output="${age_years}y"
        [[ $remaining_days -gt 0 ]] && output+=" ${remaining_days}d"
        [[ $remaining_hours -gt 0 ]] && output+=" ${remaining_hours}h"
        [[ $remaining_minutes -gt 0 ]] && output+=" ${remaining_minutes}min"
        echo "$output"
    fi
}

nfs_step1_select_ip() {
    local line ips=() menu_items=()
    while IFS= read -r line; do
        ips+=("$line")
    done < <(get_current_storage_servers_ip)

    if [ "${#ips[@]}" -eq 0 ]; then
        dialog --msgbox "No IPs found." 8 40
        return 1
    fi

    local i
    for ((i=0; i<${#ips[@]}; i++)); do
        menu_items+=("$i" "${ips[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_nfs_ip_index" ]]; then
        extra_button="--default-item $selected_nfs_ip_index"
    fi

    cmd=(dialog --keep-tite --title "Step 1/$nfs_total_steps: Select $PRODUCT NFS Storage IP (VIP)"
         --ok-label "Next" --cancel-label "Back to Main" $extra_button
         --menu "Choose $PRODUCT NFS Storage IP (VIP):" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_nfs_ip_index="$selected_option"
            if [[ "${ips[selected_option]}" == "Enter new IP" ]]; then
                # Get custom IP input with validation loop
                local input_ip
                while true; do
                    input_ip=$(dialog --keep-tite --title "Enter New IP" --inputbox "Enter the NFS Storage IP address:" 8 40 3>&1 1>&2 2>&3)
                    if [[ $? -ne 0 ]]; then
                        return 2  # User cancelled
                    fi
                    
                    if [[ -z "$input_ip" ]]; then
                        dialog --msgbox "IP address cannot be empty. Please try again." 8 50
                        continue
                    fi
                    
                    if ! validate_ip "$input_ip"; then
                        dialog --msgbox "Invalid IP address format or range. Please enter a valid IP address (e.g., 192.168.1.100)." 10 60
                        continue
                    fi
                    
                    if ip_already_exists "$input_ip"; then
                        dialog --msgbox "IP address $input_ip already exists in configuration. Please enter a different IP." 10 60
                        continue
                    fi
                    
                    # IP is valid and new, break the loop
                    break
                done
                
                selected_nfs_ip="$input_ip"
                # Ask if user wants to save this IP (only for manually entered IPs)
                if dialog --yesno "Save this IP ($input_ip) for future use?" 8 50; then
                    add_custom_ip "$input_ip"
                fi
                return 0
            elif [[ "${ips[selected_option]}" == "Manage custom IPs" ]]; then
                manage_custom_ips
                return 2  # Retry same step
            else
                selected_nfs_ip="${ips[selected_option]}"
                return 0
            fi
            ;;
        1)  # Back to Main
            return 1
            ;;
    esac
}

nfs_step2_select_pool() {
    # Test API connection first
    if ! rest_api_connection_test; then
        dialog --msgbox "API connection test failed for $selected_nfs_ip" 8 50
        return 2  # Go back
    fi

    local line pools=() menu_items=()
    while IFS= read -r line; do
        pools+=("$line")
    done < <(get_pool_names "$selected_nfs_ip")

    if [ "${#pools[@]}" -eq 0 ]; then
        dialog --msgbox "No pools found on server $selected_nfs_ip" 8 50
        return 2  # Go back
    fi

    local i
    for ((i=0; i<${#pools[@]}; i++)); do
        menu_items+=("$i" "${pools[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_pool_index" ]]; then
        extra_button="--default-item $selected_pool_index"
    fi

    cmd=(dialog --keep-tite --title "Step 2/$nfs_total_steps: Select Pool"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Pool from $selected_nfs_ip:" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_pool_index="$selected_option"
            selected_pool="${pools[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step3_select_dataset() {
    local line datasets=() menu_items=()
    while IFS= read -r line; do
        datasets+=("$line")
    done < <(get_datasets_in_pool "$selected_pool")

    if [ "${#datasets[@]}" -eq 0 ]; then
        dialog --msgbox "No datasets found in pool $selected_pool" 8 50
        return 2  # Go back
    fi

    local i
    for ((i=0; i<${#datasets[@]}; i++)); do
        menu_items+=("$i" "${datasets[i]}")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_dataset_index" ]]; then
        extra_button="--default-item $selected_dataset_index"
    fi

    cmd=(dialog --keep-tite --title "Step 3/$nfs_total_steps: Select Dataset"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Dataset from Pool $selected_pool:" 15 60 8)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_dataset_index="$selected_option"
            selected_dataset="${datasets[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step4_select_snapshot() {
    local line snapshots=() snapshot_times=() menu_items=()
    
    while IFS=$'\t' read -r snapshot_name creation_time; do
        snapshots+=("$snapshot_name")
        snapshot_times+=("$creation_time")
    done < <(get_snapshots_with_times "$selected_pool" "$selected_dataset")

    if [ "${#snapshots[@]}" -eq 0 ]; then
        dialog --msgbox "No snapshots found in dataset $selected_dataset" 8 50
        return 2  # Go back
    fi

    local current_time
    current_time=$(get_current_timestamp)
    
    local i
    for ((i=0; i<${#snapshots[@]}; i++)); do
        local snapshot_name="${snapshots[i]}"
        local creation_time="${snapshot_times[i]}"
        local age_text=""
        
        if [[ -n "$creation_time" ]]; then
            local age_seconds
            age_seconds=$(calculate_age_seconds "$current_time" "$creation_time")
            age_text=$(format_age_human "$age_seconds")
        else
            age_text="unknown"
        fi
        
        menu_items+=("$i" "$snapshot_name    $age_text")
    done

    local dialog_selected=$(mktemp)
    local extra_button
    if [[ -n "$selected_snapshot_index" ]]; then
        extra_button="--default-item $selected_snapshot_index"
    fi

    cmd=(dialog --keep-tite --title "Step 4/$nfs_total_steps: Select Snapshot"
         --ok-label "Next" --cancel-label "Back to Main" --extra-button --extra-label "Back" $extra_button
         --menu "Choose $PRODUCT Snapshot from Dataset $selected_dataset:" 18 80 10)
    options=("${menu_items[@]}")
    dialog_menu
    rm -f "$dialog_selected"

    case $dialog_exit_code in
        0)  # Next
            selected_snapshot_index="$selected_option"
            selected_snapshot="${snapshots[selected_option]}"
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

nfs_step5_summary() {
    local clone_name=$(make_clone_name "$selected_pool" "$selected_dataset" "$selected_snapshot")
    local summary_text="Summary of selections:\n\n"
    summary_text+="NFS Server IP: $selected_nfs_ip\n"
    summary_text+="Pool: $selected_pool\n"
    summary_text+="Dataset: $selected_dataset\n"
    summary_text+="Snapshot: $selected_snapshot\n"
    summary_text+="Clone name: $clone_name\n\n"
    summary_text+="Click 'Apply' to create the cloned NFS storage,\nor click 'Back' to modify your selections."

    dialog --keep-tite --title "Step 5/$nfs_total_steps: Confirmation" \
           --ok-label "Apply" --cancel-label "Back to Main" --extra-button --extra-label "Back" \
           --msgbox "$summary_text" 18 80

    case $? in
        0)  # Apply
            return 0
            ;;
        1)  # Back to Main
            return 1
            ;;
        3)  # Back
            return 2
            ;;
    esac
}

extract_timestamp_from_snapname() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapname="$3"

    if [[ $snapname =~ ([0-9]{4}-[0-9]{2}-[0-9]{2})-([0-9]{6}) ]]; then
        local day_part="${BASH_REMATCH[1]}"
        local time_part="${BASH_REMATCH[2]}"
        local hourmin="${time_part:0:4}"
        echo "${day_part}-${hourmin}"
    else
        local response
        response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" -H 'Content-Type: application/json' \
            "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots?page=0&per_page=0&sort_by=name&order=asc")
        local curl_exit=$?
        if [[ $curl_exit -ne 0 ]]; then
            log_error "extract_timestamp_from_snapname: Failed to connect to server for snapshot creation time"
            return 1
        fi

        local creation
        creation=$(printf '%s' "$response" | python3 -c "
import sys, json, datetime
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    found = False
    for snap in entries:
        if snap.get('name') == sys.argv[1]:
            creation = snap.get('properties', {}).get('creation')
            if creation:
                dt = datetime.datetime.fromtimestamp(int(creation))
                print(dt.strftime('%Y-%m-%d-%H%M'))
                found = True
                break
    if not found:
        sys.exit(1)
except Exception:
    sys.exit(1)
" "$snapname")

        if [[ -z "$creation" ]]; then
            log_error "extract_timestamp_from_snapname: Failed to extract creation time from snapshot data"
            return 1
        fi
        echo "$creation"
    fi
}

make_clone_name() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapshot_name="$3"
    local ts
    ts=$(extract_timestamp_from_snapname "$pool_name" "$dataset_name" "$snapshot_name")
    echo "${dataset_name}_clone_${ts}"
}

create_clone_from_snapshot() {
    local pool_name="$1"
    local dataset_name="$2"
    local snapshot_name="$3"
    local clone_name="$4"

    local json_input="{\"name\": \"${clone_name}\"}"

    local response
    response=$(curl -k -s -w "\n%{http_code}" -X POST -u "$rest_api_user:$rest_api_password" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes/$dataset_name/snapshots/$snapshot_name/clones")

    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)

    if [[ $status -eq 200 ]]; then
        log_info "Clone '$clone_name' created"
    else
        log_error "Failed to create clone. Status: $status"
        log_error "Response: $body"
        return 1
    fi
}

create_nfs_share_for_clone() {
    local pool_name="$1"
    local clone_name="$2"

    local path="${pool_name}/${clone_name}"
    local json_input
    json_input=$(cat <<EOF
{
    "name": "$clone_name",
    "path": "$path",
    "nfs": {
        "enabled": true
    }
}
EOF
)

    local response
    response=$(curl -k -s -w "\n%{http_code}" -X POST -u "$rest_api_user:$rest_api_password" \
        -H 'Content-Type: application/json' \
        -d "$json_input" \
        "https://$selected_nfs_ip:$rest_api_port/api/v4/shares")

    local body=$(echo "$response" | sed '$d')
    local status=$(echo "$response" | tail -n1)

    if [[ $status -eq 201 ]]; then
        log_info "NFS share '$clone_name' created at: $path"
    else
        log_error "Failed to create NFS share. Status: $status"
        log_error "Response: $body"
        return 1
    fi
}

get_ha_nodes_csv() {
    get_ha_nodes | paste -sd,
}

add_nfs_storage_to_proxmox() {
    local storage_id="$1"
    local server="$2"
    local export="/$storage_id"
    local nodes_csv="$3"

    local content="images,iso,vztmpl,backup,rootdir,snippets"

    # echo "Adding NFS storage '$storage_id' with:"  :to-do: send all lines to logs
    # echo "  Server: $server"
    # echo "  Export: $export"
    # echo "  Nodes:  $nodes_csv"
    # echo

    pvesh create /storage \
        --storage "$storage_id" \
        --type nfs \
        --server "$server" \
        --export "$export" \
        --content "$content" \
        --nodes "$nodes_csv" > /dev/null 2>&1
}

wait_for_storage_mount() {
    local storage_name="$1"
    local max_wait=30
    local wait_time=0
    
    log_info "Waiting for storage '$storage_name' to become available..."
    
    while [[ $wait_time -lt $max_wait ]]; do
        if [[ -d "/mnt/pve/$storage_name" ]]; then
            log_info "Storage '$storage_name' is now available"
            return 0
        fi
        
        sleep 2
        wait_time=$((wait_time + 2))
        log_debug "Waiting for storage mount... ${wait_time}s/${max_wait}s"
    done
    
    log_error "Storage '$storage_name' failed to mount within ${max_wait}s"
    return 1
}

refresh_storage_cache() {
    local storage_name="$1"
    
    log_info "Refreshing storage cache for '$storage_name'..."
    
    # Force Proxmox to refresh storage information
    pvesh set /nodes/$(hostname)/storage/"$storage_name" --enabled 1 2>/dev/null || true
    
    # Wait a moment for the refresh to take effect
    sleep 2
    
    # Clear any cached storage information
    rm -f /tmp/pve-storage-* 2>/dev/null || true
}

sync_storage_discovery() {
    log_info "Synchronizing storage discovery for restore wizard..."
    
    # Rebuild storage IP mapping to ensure all storage is discovered
    build_storage_ip_map
    
    # Clear any cached storage information
    rm -f /tmp/pve-storage-* 2>/dev/null || true
    
    pvesm status >/dev/null 2>&1
    
    log_info "Storage discovery synchronization complete"
}

wait_for_config_files() {
    local storage_name="$1"
    local max_wait=60
    local wait_time=0
    
    log_info "Waiting for VM/CT config files to be discoverable in storage '$storage_name'..."
    
    while [[ $wait_time -lt $max_wait ]]; do
        local config_count=$(ls -1 /mnt/pve/"$storage_name"/images/*/*.conf 2>/dev/null | wc -l)
        if [[ $config_count -gt 0 ]]; then
            log_info "Found $config_count config files in storage '$storage_name'"
            return 0
        fi
        
        sleep 3
        wait_time=$((wait_time + 3))
        log_debug "Waiting for config files... ${wait_time}s/${max_wait}s"
    done
    
    log_info "No config files found in storage '$storage_name' within ${max_wait}s (this is normal for new storage)"
    return 0
}

set_readonly_conf_files() {
    local storage_name="$1"
    local conf_path="/mnt/pve/$storage_name/images"
    
    log_info "Setting read-only permissions on conf files in storage '$storage_name'..."
    
    # Check if the images directory exists
    if [[ ! -d "$conf_path" ]]; then
        log_info "Images directory '$conf_path' not found - no conf files to process"
        return 0
    fi
    
    # Find all .conf files and set read-only attribute
    local conf_files=()
    local conf_count=0
    
    while IFS= read -r -d '' conf_file; do
        conf_files+=("$conf_file")
        ((conf_count++))
    done < <(find "$conf_path" -name "*.conf" -type f -print0 2>/dev/null)
    
    if [[ $conf_count -eq 0 ]]; then
        log_info "No conf files found in '$conf_path' - nothing to set read-only"
        return 0
    fi
    
    log_info "Found $conf_count conf files, setting read-only permissions..."
    
    local success_count=0
    local error_count=0
    
    for conf_file in "${conf_files[@]}"; do
        if chmod 444 "$conf_file" 2>/dev/null; then
            log_debug "Set read-only permissions on: $conf_file"
            ((success_count++))
        else
            log_error "Failed to set read-only permissions on: $conf_file"
            ((error_count++))
        fi
    done
    
    if [[ $error_count -eq 0 ]]; then
        log_info "Successfully set read-only permissions on $success_count conf files"
    else
        log_error "Set read-only permissions on $success_count files, $error_count failed"
    fi
    
    return 0
}

perform_clone_and_setup() {
    local clone_name=$(make_clone_name "$selected_pool" "$selected_dataset" "$selected_snapshot")

    # Show progress
    dialog --title "Processing..." --infobox "Creating clone: $clone_name..." 6 50
    sleep 1

    if ! create_clone_from_snapshot "$selected_pool" "$selected_dataset" "$selected_snapshot" "$clone_name"; then
        dialog --msgbox "Failed to create clone!" 8 40
        return 1
    fi

    dialog --title "Processing..." --infobox "Creating NFS share..." 6 50
    sleep 1

    if ! create_nfs_share_for_clone "$selected_pool" "$clone_name"; then
        dialog --msgbox "Failed to create NFS share!" 8 40
        return 1
    fi

    dialog --title "Processing..." --infobox "Adding storage to Proxmox..." 6 50
    sleep 1

    local nodes_csv=$(get_ha_nodes_csv)
    if ! add_nfs_storage_to_proxmox "$clone_name" "$selected_nfs_ip" "$nodes_csv"; then
        dialog --msgbox "Failed to add storage to Proxmox!" 8 40
        return 1
    fi

    dialog --title "Processing..." --infobox "Waiting for storage to become available..." 6 50

    if ! wait_for_storage_mount "$clone_name"; then
        dialog --msgbox "Storage was added but failed to mount properly.\nIt may take a few minutes to become available." 8 60
        return 1
    fi

    dialog --title "Processing..." --infobox "Refreshing storage information..." 6 50

    refresh_storage_cache "$clone_name"
    
    # Rebuild storage IP mapping to include the new storage
    build_storage_ip_map

    dialog --title "Processing..." --infobox "Scanning for VM/CT configurations..." 6 50

    wait_for_config_files "$clone_name"

    dialog --title "Processing..." --infobox "Setting read-only permissions on conf files..." 6 50

    set_readonly_conf_files "$clone_name"

    dialog --msgbox "Clone and NFS setup completed successfully!\n\nClone: $clone_name\nNFS Share: $clone_name\nProxmox Storage: $clone_name" 10 60
}

add_nfs_storage_wizard() {
    # Reset NFS wizard variables
    selected_nfs_ip=""
    selected_pool=""
    selected_dataset=""
    selected_snapshot=""
    selected_nfs_ip_index=""
    selected_pool_index=""
    selected_dataset_index=""
    selected_snapshot_index=""
    nfs_current_step=1

    while true; do
        case $nfs_current_step in
            1)
                nfs_step1_select_ip
                case $? in
                    0)  # Next
                        nfs_current_step=2
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Retry same step
                        ;;
                esac
                ;;
            2)
                nfs_step2_select_pool
                case $? in
                    0)  # Next
                        nfs_current_step=3
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=1
                        ;;
                esac
                ;;
            3)
                nfs_step3_select_dataset
                case $? in
                    0)  # Next
                        nfs_current_step=4
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=2
                        ;;
                esac
                ;;
            4)
                nfs_step4_select_snapshot
                case $? in
                    0)  # Next
                        nfs_current_step=5
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=3
                        ;;
                esac
                ;;
            5)
                nfs_step5_summary
                case $? in
                    0)  # Apply
                        perform_clone_and_setup
                        return
                        ;;
                    1)  # Back to Main
                        return
                        ;;
                    2)  # Back
                        nfs_current_step=4
                        ;;
                esac
                ;;
        esac
    done
}

# DELETE CLONED NFS STORAGE FUNCTIONS

# Function: get_server_ip_address_of_given_nfs_storage
get_server_ip_address_of_given_nfs_storage() {
    awk -v name="$1" '
    $1 == "nfs:" && $2 == name { in_block = 1; next }
    in_block && $1 == "server" { print $2; exit }
    /^nfs:/ && $2 != name { in_block = 0 }
    ' /etc/pve/storage.cfg
}

# Function: get_pve_NFS_storage
get_pve_NFS_storage() {
    local storage_cfg="/etc/pve/storage.cfg"

    if [[ ! -f "$storage_cfg" ]]; then
        log_error "get_nfs_server_ip_of_given_storage: $storage_cfg not found"
        return 1
    fi

    # Parse storage.cfg to extract NFS storage names
    awk '
    /^nfs:/ {
        # Extract storage name after "nfs: "
        storage_name = $2
        print storage_name
    }
    ' "$storage_cfg"
}

# Function: auto_select_nfs_ip
auto_select_nfs_ip() {
    local storage_name="$1"  # Optional: specific storage name to match
    local available_ips=()
    
    log_debug "auto_select_nfs_ip: Called with storage_name='$storage_name'"
    log_debug "auto_select_nfs_ip: Current selected_nfs_ip='$selected_nfs_ip'"
    
    # Get all unique NFS server IPs from storage config
    while IFS= read -r ip; do
        [[ -n "$ip" ]] && available_ips+=("$ip")
    done < <(grep "server" /etc/pve/storage.cfg | awk '{print $2}' | sort -u)
    
    log_debug "auto_select_nfs_ip: Found ${#available_ips[@]} IP(s): ${available_ips[*]}"
    
    if [[ ${#available_ips[@]} -eq 0 ]]; then
        log_error "auto_select_nfs_ip: No NFS server IPs found in storage config"
        return 1
    fi
    
    # If only one IP, use it
    if [[ ${#available_ips[@]} -eq 1 ]]; then
        selected_nfs_ip="${available_ips[0]}"
        log_info "auto_select_nfs_ip: Using single NFS IP: $selected_nfs_ip"
        return 0
    fi
    
    # If specific storage name provided, try to match it first
    if [[ -n "$storage_name" ]]; then
        local storage_ip
        storage_ip=$(get_server_ip_address_of_given_nfs_storage "$storage_name")
        if [[ -n "$storage_ip" ]]; then
            # Test if this IP works
            local temp_selected_nfs_ip="$selected_nfs_ip"
            selected_nfs_ip="$storage_ip"
            if rest_api_connection_test >/dev/null 2>&1; then
                log_info "auto_select_nfs_ip: Using storage-specific IP: $selected_nfs_ip"
                return 0
            else
                selected_nfs_ip="$temp_selected_nfs_ip"
                log_debug "auto_select_nfs_ip: Storage-specific IP $storage_ip failed connection test"
            fi
        fi
    fi
    
    # Try each IP sequentially
    for ip in "${available_ips[@]}"; do
        log_debug "auto_select_nfs_ip: Testing IP: $ip"
        local temp_selected_nfs_ip="$selected_nfs_ip"
        selected_nfs_ip="$ip"
        if rest_api_connection_test >/dev/null 2>&1; then
            log_info "auto_select_nfs_ip: Successfully connected to IP: $selected_nfs_ip"
            return 0
        else
            selected_nfs_ip="$temp_selected_nfs_ip"
            log_debug "auto_select_nfs_ip: IP $ip failed connection test"
        fi
    done
    
    # All IPs failed, ask user to select
    log_info "auto_select_nfs_ip: All IPs failed, prompting user for selection"
    
    # Reuse existing IP selection logic
    local line ips=() menu_items=()
    for ip in "${available_ips[@]}"; do
        ips+=("$ip")
    done
    ips+=("Enter new IP")
    
    local i
    for ((i=0; i<${#ips[@]}; i++)); do
        menu_items+=("$i" "${ips[i]}")
    done
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "NFS Server Selection"
         --ok-label "Select" --cancel-label "Cancel"
         --menu "Multiple NFS servers found. Choose one:" 15 60 8)
    local options=("${menu_items[@]}")
    
    local selected_option dialog_exit_code
    exec 3>&1
    selected_option=$(dialog "${cmd[@]}" "${options[@]}" 2>&1 1>&3)
    dialog_exit_code=$?
    exec 3>&-
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # User selected an option
            if [[ "${ips[selected_option]}" == "Enter new IP" ]]; then
                # Get custom IP input
                local input_ip
                input_ip=$(dialog --keep-tite --title "Enter New IP" --inputbox "Enter the NFS Storage IP address:" 8 40 3>&1 1>&2 2>&3)
                if [[ $? -eq 0 && -n "$input_ip" ]]; then
                    if [[ "$input_ip" =~ ^([0-9]{1,3}\.){3}[0-9]{1,3}$ ]]; then
                        selected_nfs_ip="$input_ip"
                        log_info "auto_select_nfs_ip: User provided IP: $selected_nfs_ip"
                        return 0
                    else
                        log_error "auto_select_nfs_ip: Invalid IP address format: $input_ip"
                        return 1
                    fi
                else
                    log_error "auto_select_nfs_ip: User cancelled IP input"
                    return 1
                fi
            else
                selected_nfs_ip="${ips[selected_option]}"
                log_info "auto_select_nfs_ip: User selected IP: $selected_nfs_ip"
                return 0
            fi
            ;;
        1)  # User cancelled
            log_error "auto_select_nfs_ip: User cancelled IP selection"
            return 1
            ;;
    esac
    
    return 1
}

# Function: ensure_nfs_ip_selected
ensure_nfs_ip_selected() {
    local storage_name="$1"  # Optional: specific storage name
    
    log_debug "ensure_nfs_ip_selected: Called with storage_name='$storage_name'"
    log_debug "ensure_nfs_ip_selected: Current selected_nfs_ip='$selected_nfs_ip'"
    
    # If IP is already selected, just return success
    if [[ -n "$selected_nfs_ip" ]]; then
        log_debug "ensure_nfs_ip_selected: IP already selected, returning success"
        return 0
    fi
    
    log_debug "ensure_nfs_ip_selected: No IP selected, trying auto-select"
    
    # Try to auto-select IP
    if auto_select_nfs_ip "$storage_name"; then
        log_debug "ensure_nfs_ip_selected: auto_select_nfs_ip succeeded, selected_nfs_ip='$selected_nfs_ip'"
        return 0
    else
        log_error "ensure_nfs_ip_selected: Failed to select NFS IP"
        return 1
    fi
}

# Function: get_pools
get_pools() {
    local nfs_ip="$1"
    
    log_debug "get_pools: Starting function"
    log_debug "get_pools: nfs_ip='$nfs_ip'"
    log_debug "get_pools: rest_api_user='$rest_api_user'"
    log_debug "get_pools: rest_api_port='$rest_api_port'"
    
    # If no IP provided, try to use global selected_nfs_ip
    if [[ -z "$nfs_ip" ]]; then
        if ! ensure_nfs_ip_selected; then
            log_error "get_pools: ensure_nfs_ip_selected failed"
            log_error "get_pools: Failed to select NFS IP"
            return 1
        fi
        nfs_ip="$selected_nfs_ip"
        log_debug "get_pools: Using global selected_nfs_ip='$nfs_ip'"
    fi

    # Make REST API call to get all pools
    local response
    local url="https://$nfs_ip:$rest_api_port/api/v4/pools"
    log_debug "get_pools: Making request to: $url"
    
    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" \
        -H 'Content-Type: application/json' \
        "$url" 2>/dev/null)

    local curl_exit=$?
    log_debug "get_pools: curl exit code: $curl_exit"
    
    if [[ $curl_exit -ne 0 ]]; then
        log_error "get_pools: curl failed with exit code $curl_exit"
        log_error "get_pools: Failed to connect to server"
        return 1
    fi

    # Check if response is empty or invalid
    if [[ -z "$response" ]]; then
        log_error "get_pools: Empty response from server"
        return 1
    fi

    # Log the raw response for debugging
    log_debug "get_pools: Raw response: ${response:0:200}..."
    
    # Check for HTTP error responses
    if [[ "$response" =~ ^[0-9]{3}[[:space:]] ]]; then
        log_error "get_pools: HTTP error response: $response"
        return 1
    fi
    
    # Parse JSON response to extract pool names
    printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    pools = data.get('data', [])
    for pool in pools:
        name = pool.get('name')
        if name:
            print(name)
except json.JSONDecodeError as e:
    print(f'get_pools: JSON decode error: {e}', file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f'get_pools: Error: {e}', file=sys.stderr)
    sys.exit(1)
"

    local python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        log_error "get_pools: Failed to parse JSON response"
        return 1
    fi
}

get_nfs_server_ip_of_given_storage() {
    local storage="$1"
    awk -v storage="$storage" '
        # When we hit an nfs: line, check if its name matches
        /^nfs:[[:space:]]+/ {
            in_block = ($2 == storage)
        }
        # While in the right block, look for a “server” line
        in_block && /^[[:space:]]*server[[:space:]]+/ {
            print $2     # print the IP
            exit         # and quit immediately
        }
    ' /etc/pve/storage.cfg
}


# Function: get_dataset_pool_name
get_dataset_pool_name() {
    local dataset_name="$1"
    local nfs_ip="$2"

    if [[ -z "$dataset_name" ]]; then
        log_error "get_dataset_pool_name: dataset_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    log_debug "get_dataset_pool_name: Looking for dataset '$dataset_name' on IP '$nfs_ip'"

    # Get all pools
    local pools
    pools=$(get_pools "$nfs_ip")
    if [[ $? -ne 0 ]]; then
        log_error "get_dataset_pool_name: Failed to get pools list"
        return 1
    fi

    # Loop through each pool and check if dataset exists
    while IFS= read -r pool_name; do
        if [[ -z "$pool_name" ]]; then
            continue
        fi

        # Get nas-volumes for this pool
        local response
        response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" \
            -H 'Content-Type: application/json' \
            "https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes" 2>/dev/null)

        local curl_exit=$?
        if [[ $curl_exit -ne 0 ]]; then
            continue  # Skip this pool if we can't access it
        fi

        # Check if dataset exists in this pool
        local dataset_found
        dataset_found=$(printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            print('FOUND')
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
")

        if [[ "$dataset_found" == "FOUND" ]]; then
            echo "$pool_name"
            return 0
        fi
    done <<< "$pools"

    # Dataset not found in any pool
    # echo "Error: Dataset '$dataset_name' not found in any pool" >&2
    return 1
}

# Function: is_clone
is_clone() {
    local volume_name="$1"
    local nfs_ip="$2"

    if [[ -z "$volume_name" ]]; then
        echo "Error: volume_name is required" >&2
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    # Parse pool and dataset from volume_name
    local pool_name="${volume_name%%/*}"
    local dataset_name="${volume_name#*/}"

    if [[ -z "$pool_name" || -z "$dataset_name" || "$pool_name" == "$dataset_name" ]]; then
        echo "Error: Invalid volume_name format. Expected: pool_name/dataset_name" >&2
        return 1
    fi

    log_debug "is_clone: Checking if '$volume_name' is a clone on IP '$nfs_ip'"

    # Make REST API call to get nas-volumes for the pool
    local response
    response=$(curl -k -s -X GET -u "$rest_api_user:$rest_api_password" \
        -H 'Content-Type: application/json' \
        "https://$nfs_ip:$rest_api_port/api/v4/pools/$pool_name/nas-volumes" 2>/dev/null)

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        echo "is_clone: Error: Failed to connect to server" >&2
        return 1
    fi

    # Parse JSON response to find the dataset and check its origin
    local origin
    origin=$(printf '%s' "$response" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    entries = data.get('data', {}).get('entries', [])
    for d in entries:
        if d.get('name') == '$dataset_name':
            origin = d.get('origin')
            if origin is None:
                print('None')
            else:
                print(origin)
            sys.exit(0)
    print('NOT_FOUND')
except Exception as e:
    print('ERROR', file=sys.stderr)
    sys.exit(1)
")

    local python_exit=$?
    if [[ $python_exit -ne 0 ]]; then
        echo "is_clone: Error: Failed to parse JSON response" >&2
        return 1
    fi

    if [[ "$origin" == "NOT_FOUND" ]]; then
        echo "Error: Dataset '$dataset_name' not found in pool '$pool_name'" >&2
        return 1
    fi

    if [[ "$origin" == "None" ]]; then
        # origin is None, so it's a dataset (not a clone)
        return 1
    else
        # origin is set, so it's a clone
        return 0
    fi
}

# Function: delete_cloned_dataset
delete_cloned_dataset() {
    local dataset_name="$1"
    local nfs_ip="$2"

    if [[ -z "$dataset_name" ]]; then
        log_error "get_dataset_pool_name: dataset_name is required"
        return 1
    fi

    if [[ -z "$nfs_ip" ]]; then
        log_error "get_dataset_pool_name: nfs_ip is required"
        return 1
    fi

    log_info "Checking dataset: $dataset_name on IP: $nfs_ip"

    # Find which pool this dataset belongs to
    local pool_name
    pool_name=$(get_dataset_pool_name "$dataset_name" "$nfs_ip")
    if [[ $? -ne 0 ]]; then
        # echo "Error: Dataset '$dataset_name' not found in any pool" >&2
        return 1
    fi

    log_info "Dataset '$dataset_name' found in pool '$pool_name'"

    # Check if this dataset is a clone
    local volume_name="$pool_name/$dataset_name"
    if ! is_clone "$volume_name" "$nfs_ip"; then
        log_error "Dataset '$dataset_name' is not a clone. Refusing to delete original dataset."
        return 1
    fi

    log_info "Confirmed: Dataset '$dataset_name' is a clone. Proceeding with deletion..."

    # Make REST API call to delete the dataset
    local url="https://${nfs_ip}:${rest_api_port}/api/v4/pools/${pool_name}/nas-volumes/${dataset_name}"

    local response
    response=$(curl -k -s -X DELETE \
        -u "$rest_api_user:$rest_api_password" \
        -H 'Content-Type: application/json' \
        -d '{}' \
        -w "\n%{http_code}\n%{redirect_url}" \
        "$url" 2>/dev/null)

    local curl_exit=$?
    if [[ $curl_exit -ne 0 ]]; then
        log_error "Failed to connect to server for deletion"
        return 1
    fi

    # Parse curl response (body, http_code, redirect_url)
    local response_lines
    readarray -t response_lines <<< "$response"
    local response_body="${response_lines[0]}"
    local http_code="${response_lines[1]}"
    local redirect_url="${response_lines[2]}"

    if [[ "$http_code" == "204" ]]; then
        log_info "Success: Dataset '$dataset_name' has been deleted"
        return 0
    else
        log_error "Failed to delete dataset '$dataset_name'. HTTP code: $http_code"
        if [[ -n "$response_body" ]]; then
            log_error "Response: $response_body"
        fi
        return 1
    fi
}

# Function: get_cloned_storage_list
get_cloned_storage_list() {
    # Check if we have NFS IP configured

    # Get all NFS storages from PVE
    local nfs_storages
    nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        log_error "Failed to get NFS storages from PVE"
        return 1
    fi

    if [[ -z "$nfs_storages" ]]; then
        # No NFS storages found, return successfully but with no output
        return 0
    fi

    # Loop through each NFS storage and check if it's a clone
    while IFS= read -r storage_name; do
        if [[ -z "$storage_name" ]]; then
            continue
        fi

        # Get the IP address for this storage
        local storage_ip
        storage_ip=$(get_storage_ip "$storage_name")
        if [[ $? -ne 0 ]]; then
            log_error "get_cloned_storage_list: No IP mapping found for storage '$storage_name'"
            continue
        fi

        # Find which pool this storage belongs to
        local pool_name
        pool_name=$(get_dataset_pool_name "$storage_name" "$storage_ip")
        if [[ $? -ne 0 ]]; then
            # Storage not found in any pool, skip it
            continue
        fi

        # Check if this storage is a clone
        local volume_name="$pool_name/$storage_name"
        if is_clone "$volume_name" "$storage_ip"; then
            echo "$storage_name"
        fi

    done <<< "$nfs_storages"
}

# Function: delete_pve_NFS_storage
delete_pve_NFS_storage() {
    local pve_storage_name="$1"

    if [[ -z "$pve_storage_name" ]]; then
        echo "Error: pve_storage_name is required" >&2
        return 1
    fi

    log_info "Processing PVE NFS storage: $pve_storage_name"

    # Step 1: Check if given storage is a PVE NFS storage
    local pve_nfs_storages
    pve_nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        echo "Error: Failed to get PVE NFS storages" >&2
        return 1
    fi

    local is_pve_nfs_storage=false
    while IFS= read -r storage; do
        if [[ "$storage" == "$pve_storage_name" ]]; then
            is_pve_nfs_storage=true
            break
        fi
    done <<< "$pve_nfs_storages"

    if [[ "$is_pve_nfs_storage" == "false" ]]; then
        log_error "'$pve_storage_name' is not a PVE NFS storage"
        return 1
    fi

    log_info "Confirmed: '$pve_storage_name' is a PVE NFS storage"

    # Step 2: Get the IP address for this storage
    local storage_ip
    storage_ip=$(get_storage_ip "$pve_storage_name")
    if [[ $? -ne 0 ]]; then
        echo "delete_pve_NFS_storage: Error: No IP mapping found for storage '$pve_storage_name'" >&2
        return 1
    fi

    log_info "Storage '$pve_storage_name' mapped to IP '$storage_ip'"

    # Step 3: Find which pool this storage belongs to
    local pool_name
    pool_name=$(get_dataset_pool_name "$pve_storage_name" "$storage_ip")
    if [[ $? -ne 0 ]]; then
        # echo "Error: Storage '$pve_storage_name' not found in any pool" >&2
        return 1
    fi

    log_info "Storage '$pve_storage_name' found in pool '$pool_name'"

    # Step 4: Check if this storage is a clone dataset
    local volume_name="$pool_name/$pve_storage_name"
    if ! is_clone "$volume_name" "$storage_ip"; then
        log_error "Storage '$pve_storage_name' is not a clone dataset. Refusing to delete original dataset."
        return 1
    fi

    log_info "Confirmed: Storage '$pve_storage_name' is a clone dataset"

    # Step 5: Delete the cloned dataset via REST API
    log_info "Deleting cloned dataset..."
    if ! delete_cloned_dataset "$pve_storage_name" "$storage_ip"; then
        log_error "Failed to delete cloned dataset '$pve_storage_name'"
        return 1
    fi

    log_info "Successfully deleted cloned dataset '$pve_storage_name'"

    # Step 6: Remove PVE storage configuration
    log_info "Removing PVE storage configuration..."
    if ! pvesm remove "$pve_storage_name"; then
        log_error "Failed to remove PVE storage '$pve_storage_name'"
        return 1
    fi

    log_info "Successfully removed PVE storage configuration '$pve_storage_name'"

    # Step 7: Run cleanup of inactive NFS storage
    log_info "Running cleanup of inactive NFS storage..."
    if ! cleanup_inactive_nfs_storage; then
        log_error "cleanup_inactive_nfs_storage failed, but main deletion was successful"
    else
        log_info "NFS storage cleanup complete"
    fi

    log_info "Storage '$pve_storage_name' deleted"
    return 0
}

# Function: get_cloned_storage_list_with_ips
get_cloned_storage_list_with_ips() {
    # Get all NFS storages from PVE
    local nfs_storages
    nfs_storages=$(get_pve_NFS_storage)
    if [[ $? -ne 0 ]]; then
        log_error "Failed to get NFS storages from PVE"
        return 1
    fi

    if [[ -z "$nfs_storages" ]]; then
        # No NFS storages found, return successfully but with no output
        return 0
    fi

    # Check each storage and build list with IPs
    local cloned_storages_with_ips=()
    while IFS= read -r storage_name; do
        if [[ -z "$storage_name" ]]; then
            continue
        fi

        # Get NFS IP for this storage
        local nfs_ip
        nfs_ip=$(get_storage_ip "$storage_name")
        if [[ $? -ne 0 ]]; then
            log_error "get_cloned_storage_list_with_ips: No IP mapping found for storage '$storage_name'"
            continue
        fi

        # Find which pool this storage belongs to
        local pool_name
        pool_name=$(get_dataset_pool_name "$storage_name" "$nfs_ip")
        if [[ $? -ne 0 ]]; then
            # Storage not found in any pool, skip it
            continue
        fi

        # Check if this storage is a clone
        local volume_name="$pool_name/$storage_name"
        if is_clone "$volume_name" "$nfs_ip"; then
            cloned_storages_with_ips+=("$storage_name:$nfs_ip")
        fi

    done <<< "$nfs_storages"

    # Output results
    for item in "${cloned_storages_with_ips[@]}"; do
        echo "$item"
    done
}

# Function: check_vmct_using_storage
check_vmct_using_storage() {
    local storage_name="$1"
    local temp_file=$(mktemp)
    
    log_debug "check_vmct_using_storage: Checking if VMs/CTs are using storage '$storage_name'"
    
    # Check all VM and CT configuration files
    for conf_file in /etc/pve/nodes/*/qemu-server/*.conf /etc/pve/nodes/*/lxc/*.conf; do
        if [[ -f "$conf_file" ]]; then
            # Get VM/CT ID from filename
            local vmct_id=$(basename "$conf_file" .conf)
            
            # Check if this VM/CT uses the specified storage
            local storage_match=false
            if grep -q "^rootfs:.*${storage_name}:" "$conf_file" 2>/dev/null; then
                storage_match=true
                log_debug "check_vmct_using_storage: Found rootfs match in $conf_file"
            elif grep -E -q "^${DISK_TYPES}[0-9]+:.*${storage_name}:" "$conf_file" 2>/dev/null; then
                storage_match=true
                log_debug "check_vmct_using_storage: Found disk match in $conf_file"
            fi
            
            if [[ "$storage_match" == "true" ]]; then
                
                # Determine type (VM or CT)
                local vmct_type="VM"
                if [[ "$conf_file" == */lxc/* ]]; then
                    vmct_type="CT"
                fi
                
                # Get VM/CT name from config (handle both formats: "name: value" and "name=value")
                local vmct_name=$(grep -E "^name[[:space:]]*[:=]" "$conf_file" 2>/dev/null | sed -E 's/^name[[:space:]]*[:=][[:space:]]*//' | head -1)
                if [[ -z "$vmct_name" ]]; then
                    vmct_name="(no name)"
                fi
                
                # Get tags from config (handle both formats: "tags: value" and "tags=value")
                local tags=$(grep -E "^tags[[:space:]]*[:=]" "$conf_file" 2>/dev/null | sed -E 's/^tags[[:space:]]*[:=][[:space:]]*//' | head -1)
                if [[ -z "$tags" ]]; then
                    tags="(no tags)"
                fi
                
                # Write to temp file in format: ID|TYPE|NAME|TAGS (using pipe delimiter to avoid space issues)
                echo "$vmct_id|$vmct_type|$vmct_name|$tags" >> "$temp_file"
                
                log_debug "check_vmct_using_storage: Found $vmct_type $vmct_id ($vmct_name) using storage '$storage_name'"
            fi
        fi
    done
    
    # Log temp file contents for debugging
    if [[ -s "$temp_file" ]]; then
        log_debug "check_vmct_using_storage: Found usage data in temp file:"
        while IFS= read -r line; do
            log_debug "check_vmct_using_storage: $line"
        done < "$temp_file"
        log_debug "check_vmct_using_storage: Returning temp file path: $temp_file"
        printf '%s\n' "$temp_file"
        return 0
    else
        log_debug "check_vmct_using_storage: No VMs/CTs found using storage '$storage_name'"
        rm -f "$temp_file"
        return 1
    fi
}

# Function: show_vmct_usage_warning
show_vmct_usage_warning() {
    local storage_name="$1"
    local usage_file="$2"
    
    log_debug "show_vmct_usage_warning: Showing warning for storage '$storage_name'"
    log_debug "show_vmct_usage_warning: Usage file path: $usage_file"
    
    # Check if usage file exists and has content
    if [[ ! -f "$usage_file" ]]; then
        log_error "show_vmct_usage_warning: Usage file '$usage_file' does not exist"
        return 1
    fi
    
    if [[ ! -s "$usage_file" ]]; then
        log_error "show_vmct_usage_warning: Usage file '$usage_file' is empty"
        return 1
    fi
    
    log_debug "show_vmct_usage_warning: Usage file contents:"
    while IFS= read -r line; do
        log_debug "show_vmct_usage_warning: File line: '$line'"
    done < "$usage_file"
    
    # Build warning message
    local warning_msg="WARNING: The following VMs/CTs are using storage '$storage_name':\n\n"
    local vm_count=0
    local has_vms=false
    local has_cts=false
    
    while IFS='|' read -r vmct_id vmct_type vmct_name tags; do
        log_debug "show_vmct_usage_warning: Read line: ID='$vmct_id', Type='$vmct_type', Name='$vmct_name', Tags='$tags'"
        
        # Skip empty lines
        if [[ -z "$vmct_id" ]]; then
            log_debug "show_vmct_usage_warning: Skipping empty line"
            continue
        fi
        
        # Track which types we have
        if [[ "$vmct_type" == "vm" ]]; then
            has_vms=true
        elif [[ "$vmct_type" == "ct" ]]; then
            has_cts=true
        fi
        
        warning_msg="${warning_msg}ID: $vmct_id | Type: $vmct_type | Name: $vmct_name | Tags: $tags\n"
        ((vm_count++))
        
        log_debug "show_vmct_usage_warning: Added VM/CT to warning: ID=$vmct_id, Type=$vmct_type, Name=$vmct_name, Tags=$tags"
    done < "$usage_file"
    
    log_debug "show_vmct_usage_warning: Total VMs/CTs processed: $vm_count"
    
    # Build appropriate type description
    local type_desc=""
    if [[ "$has_vms" == true && "$has_cts" == true ]]; then
        type_desc="VMs/CTs"
    elif [[ "$has_vms" == true ]]; then
        type_desc="VMs"
    elif [[ "$has_cts" == true ]]; then
        type_desc="CTs"
    else
        type_desc="VMs/CTs"
    fi
    
    warning_msg="${warning_msg}\nDeleting this storage will make these $type_desc unusable!\n\nDo you want to continue with deletion?"
    
    log_debug "show_vmct_usage_warning: Final warning message:"
    log_debug "show_vmct_usage_warning: $warning_msg"
    
    # Show warning dialog with Yes/No/Back options
    local dialog_result=$(mktemp)
    dialog --keep-tite \
        --title "Storage In Use Warning" \
        --yes-label "Continue Delete" \
        --no-label "Cancel" \
        --extra-button --extra-label "Back" \
        --defaultno \
        --yesno "$warning_msg" 20 80
    
    local exit_code=$?
    rm -f "$dialog_result"
    
    # Return appropriate exit code
    # 0 = Yes (Continue Delete), 1 = No (Cancel), 3 = Extra (Back)
    # With --defaultno, Cancel is highlighted but Continue Delete is still "Yes"
    case $exit_code in
        0) return 0 ;;  # Continue Delete
        1) return 1 ;;  # Cancel
        3) return 3 ;;  # Back
        *) return 1 ;;  # Default to cancel
    esac
}

# Function: delete_cloned_nfs_storage_wizard
delete_cloned_nfs_storage_wizard() {
    # Check if REST API is configured
    if [[ -z "$rest_api_user" || -z "$rest_api_password" || -z "$rest_api_port" ]]; then
        dialog --msgbox "REST API configuration is incomplete. Please use Setup to configure it first." 8 60
        return 1
    fi

    # Show progress message while fetching data
    dialog --infobox "Fetching Cloned Volumes ..." 5 35

    # Get list of cloned storages with their IPs
    local cloned_storages_with_ips
    cloned_storages_with_ips=$(get_cloned_storage_list_with_ips)
    if [[ $? -ne 0 ]]; then
        dialog --msgbox "Failed to get cloned storage list. Please check your NFS server connection and configuration." 8 60
        return 1
    fi

    if [[ -z "$cloned_storages_with_ips" ]]; then
        dialog --msgbox "No cloned NFS storages found." 8 40
        return 1
    fi

    # Build menu options
    local menu_items=()
    local index=1
    while IFS= read -r storage_with_ip; do
        if [[ -n "$storage_with_ip" ]]; then
            local storage_name="${storage_with_ip%:*}"
            local nfs_ip="${storage_with_ip#*:}"
            menu_items+=("$index" "$storage_name ($nfs_ip)")
            ((index++))
        fi
    done <<< "$cloned_storages_with_ips"

    # Show selection dialog with warning
    local dialog_selected=$(mktemp)
    dialog --keep-tite \
        --title "Delete Cloned NFS Storage" \
        --ok-label "Continue" --cancel-label "Cancel" \
        --msgbox "WARNING: This action will delete the cloned NFS storage in Proxmox VE, as well as the corresponding cloned dataset in $VENDOR $PRODUCT on which it is based.\n\nThis operation is irreversible!" 10 60

    local exit_code=$?
    if [[ $exit_code -ne 0 ]]; then
        rm -f "$dialog_selected"
        return 1
    fi

    # Storage selection loop to handle back button
    while true; do
        # Show storage selection menu
        dialog --keep-tite \
            --title "Select Cloned NFS Storage to Delete" \
            --ok-label "Select" --cancel-label "Cancel" \
            --menu "Choose a cloned NFS storage to delete:" 18 100 10 \
            "${menu_items[@]}" 2> "$dialog_selected"

        exit_code=$?
        local selected_index=$(cat "$dialog_selected")
        rm -f "$dialog_selected"

        if [[ $exit_code -ne 0 || -z "$selected_index" ]]; then
            return 1
        fi

        # Get the selected storage name and its NFS IP
        local selected_storage
        local selected_nfs_ip_final
        local current_index=1
        while IFS= read -r storage_with_ip; do
            if [[ -n "$storage_with_ip" ]]; then
                if [[ "$current_index" == "$selected_index" ]]; then
                    selected_storage="${storage_with_ip%:*}"
                    selected_nfs_ip_final="${storage_with_ip#*:}"
                    break
                fi
                ((current_index++))
            fi
        done <<< "$cloned_storages_with_ips"

        # Set the NFS IP for the selected storage
        selected_nfs_ip="$selected_nfs_ip_final"

        if [[ -z "$selected_storage" ]]; then
            dialog --msgbox "Error: Could not determine selected storage." 8 50
            return 1
        fi

        # Check if VMs/CTs are using this storage
        local usage_file
        if usage_file=$(check_vmct_using_storage "$selected_storage"); then
            # Storage is in use, show warning
            log_info "Storage '$selected_storage' is in use by VMs/CTs"
            
            show_vmct_usage_warning "$selected_storage" "$usage_file"
            local warning_result=$?
            
            # Clean up temp file
            rm -f "$usage_file"
            
            case $warning_result in
                0)  # Continue Delete
                    log_info "User chose to continue deletion despite VM/CT usage"
                    break
                    ;;
                1)  # Cancel
                    log_info "User cancelled deletion due to VM/CT usage"
                    return 1
                    ;;
                3)  # Back
                    log_info "User chose to go back to storage selection"
                    continue
                    ;;
            esac
        else
            # No VMs/CTs using this storage, proceed
            log_info "No VMs/CTs are using storage '$selected_storage'"
            break
        fi
    done

    # Final confirmation dialog - user must type "delete"
    local confirm_input=$(mktemp)
    dialog --inputbox "To confirm deletion of storage\n'$selected_storage'\ntype 'delete':" 10 70 2> "$confirm_input"
    exit_code=$?
    local confirmation=$(trimq "$(cat "$confirm_input")")
    rm -f "$confirm_input"

    if [[ $exit_code -ne 0 || "$confirmation" != "delete" ]]; then
        dialog --msgbox "Deletion cancelled." 8 40
        return 1
    fi

    # Show progress dialog
    dialog --infobox "Deleting cloned NFS storage '$selected_storage'...

This may take a moment." 8 50

    # Execute deletion
    if delete_pve_NFS_storage "$selected_storage"; then
        dialog --msgbox "Successfully deleted cloned NFS storage '$selected_storage'." 8 60
    else
        dialog --msgbox "Error: Failed to delete cloned NFS storage '$selected_storage'. Check the log file for details." 8 70
    fi
}

# VM/CT MANAGER - BULK ACTIONS

# VM/CT Manager variables
vm_manager_selected_vms=()
vm_manager_action_counts=()

# Get VMs with hostpci (passthrough) for VM/CT Manager
vm_manager_get_vms_with_hostpci() {
    grep -Rl hostpci /etc/pve/nodes/*/qemu-server/*.conf 2>/dev/null | \
        sed -nE 's#.*/([0-9]+)\.conf#\1#p' | sort -u
}

# Get all VMs and CTs from entire Proxmox cluster for VM/CT Manager
vm_manager_get_vm_ct_list() {
    local vm_list=()
    local hostpci_vms
    
    # Get VMs with hostpci for skipping
    readarray -t hostpci_vms < <(vm_manager_get_vms_with_hostpci)
    
    # Get cluster-wide resources using pvesh
    while IFS='|' read -r vmid name type status node tags; do
        if [[ -n "$vmid" && "$vmid" != "VMID" ]]; then
            # Skip VMs with hostpci (passthrough)
            if [[ "$type" == "vm" ]] && [[ " ${hostpci_vms[*]} " == *" $vmid "* ]]; then
                log_debug "Skipping vm:$vmid (hostpci present)"
                continue
            fi
            
            # Skip VMs/CTs with joviandss in name
            if [[ "$name" == *"joviandss"* ]]; then
                log_debug "Skipping $type:$vmid (joviandss in name: $name)"
                continue
            fi
            
            vm_list+=("$vmid|$name|$type|$status|$tags")
        fi
    done < <(pvesh get /cluster/resources --type vm --output-format=json 2>/dev/null | \
        python3 -c "
import sys, json
try:
    resources = json.load(sys.stdin)
    for resource in resources:
        vmid = resource.get('vmid', '')
        name = resource.get('name', 'unnamed')
        res_type = resource.get('type', '')
        status = resource.get('status', 'unknown')
        node = resource.get('node', '')
        tags = resource.get('tags', '')
        lock = resource.get('lock', '')
        
        # Map resource type to our format
        if res_type == 'qemu':
            vm_type = 'vm'
        elif res_type == 'lxc':
            vm_type = 'ct'
        else:
            continue
            
        # Clean up tags (empty if None)
        if not tags:
            tags = ''
            
        # Check if VM/CT is suspended (has suspended lock or paused status)
        if lock == 'suspended' or status == 'paused':
            status = 'suspended'
            
        print(f'{vmid}|{name}|{vm_type}|{status}|{node}|{tags}')
except Exception as e:
    print(f'Error: {e}', file=sys.stderr)
")
    
    printf '%s\n' "${vm_list[@]}"
}

# Find longest name and tag for formatting in VM/CT Manager
vm_manager_get_max_lengths() {
    local max_name_length=0
    local max_tag_length=0
    local vm_data
    
    while IFS='|' read -r vmid name type status tags; do
        local name_length=${#name}
        local tag_length=${#tags}
        
        if [[ $name_length -gt $max_name_length ]]; then
            max_name_length=$name_length
        fi
        
        if [[ $tag_length -gt $max_tag_length ]]; then
            max_tag_length=$tag_length
        fi
    done <<< "$1"
    
    # Minimum lengths
    if [[ $max_name_length -lt 10 ]]; then
        max_name_length=10
    fi
    
    if [[ $max_tag_length -lt 4 ]]; then
        max_tag_length=4
    fi
    
    echo "$max_name_length|$max_tag_length"
}

# VM/CT selection dialog for VM/CT Manager
vm_manager_select_vm_ct() {
    local vm_ct_data
    local select_all_mode=false
    local max_name_length
    local max_tag_length
    local menu_items=()
    
    # Get VM/CT data
    vm_ct_data=$(vm_manager_get_vm_ct_list)
    if [[ -z "$vm_ct_data" ]]; then
        dialog --title "Error" --msgbox "No VMs or CTs found in the cluster." 8 50
        return 1
    fi
    
    # Calculate max name and tag lengths for formatting
    local max_lengths=$(vm_manager_get_max_lengths "$vm_ct_data")
    IFS='|' read -r max_name_length max_tag_length <<< "$max_lengths"
    
    while true; do
        menu_items=()
        local select_label
        
        if $select_all_mode; then
            select_label="Deselect All"
        else
            select_label="Select All"
        fi
        
        # Build menu items
        while IFS='|' read -r vmid name type status tags; do
            if [[ -n "$vmid" ]]; then
                # Format display string with proper spacing
                local formatted_name=$(printf "%-${max_name_length}s" "$name")
                local formatted_tags=$(printf "%-${max_tag_length}s" "$tags")
                local display_string=$(printf "(%s) [%s] %-2s %s" "$formatted_name" "$formatted_tags" "$type" "$status")
                
                # Check if this VM/CT was previously selected
                local selection_status="off"
                for selected_vm in "${vm_manager_selected_vms[@]}"; do
                    if [[ "$selected_vm" == "$vmid" ]]; then
                        selection_status="on"
                        break
                    fi
                done
                
                menu_items+=("$vmid" "$display_string" "$selection_status")
            fi
        done <<< "$vm_ct_data"
        
        # Create dialog
        cmd=(dialog --keep-tite --title "VM/CT Manager - Select VM/CT"
             --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
             --checklist "Choose VM/CT to manage (use SPACE to select/deselect):" 20 120 12)
        options=("${menu_items[@]}")
        
        dialog_menu
        
        case $dialog_exit_code in
            0)  # Next
                if [[ -n "$selected_option" ]]; then
                    # Parse selected VMs/CTs
                    vm_manager_selected_vms=()
                    IFS=' ' read -ra ADDR <<< "$selected_option"
                    for vmid in "${ADDR[@]}"; do
                        vm_manager_selected_vms+=("$vmid")
                    done
                    log_info "Selected VMs/CTs: ${vm_manager_selected_vms[*]}"
                    return 0
                else
                    dialog --title "Warning" --msgbox "Please select at least one VM or CT." 8 50
                fi
                ;;
            1)  # Back
                return 1
                ;;
            3)  # Select All/Deselect All
                if $select_all_mode; then
                    # Deselect all
                    vm_manager_selected_vms=()
                    select_all_mode=false
                    log_debug "Deselected all VMs/CTs"
                else
                    # Select all
                    vm_manager_selected_vms=()
                    while IFS='|' read -r vmid name type status tags; do
                        if [[ -n "$vmid" ]]; then
                            vm_manager_selected_vms+=("$vmid")
                        fi
                    done <<< "$vm_ct_data"
                    select_all_mode=true
                    log_debug "Selected all VMs/CTs: ${vm_manager_selected_vms[*]}"
                fi
                ;;
        esac
    done
}

# Confirm critical action with detailed explanation for VM/CT Manager
vm_manager_confirm_critical_action() {
    local action="$1"
    local vm_count="${#vm_manager_selected_vms[@]}"
    local description=""
    local warning=""
    
    case "$action" in
        "shutdown")
            description="GRACEFUL SHUTDOWN - Sends shutdown signal to guest OS"
            warning="- Safe: Saves all data and applications\\n- Speed: Slower (waits for guest OS)\\n- Recommended for normal operations"
            ;;
        "stop")
            description="FORCE STOP - Immediately stops VM/CT without graceful shutdown"
            warning="- UNSAFE: May cause data loss or corruption\\n- Speed: Fast (immediate)\\n- Use only when shutdown fails"
            ;;
        "reboot")
            description="GRACEFUL REBOOT - Restarts VM/CT with proper shutdown first"
            warning="- Safe: Saves all data before restart\\n- Speed: Medium (graceful shutdown + restart)\\n- Recommended for normal restarts"
            ;;
        "reset")
            description="HARD RESET - Force restart like pressing physical reset button"
            warning="- VERY UNSAFE: High risk of data corruption\\n- Speed: Fastest (immediate reset)\\n- Use ONLY when system is completely frozen"
            ;;
        "suspend")
            description="SUSPEND - Pause VM/CT to memory, preserving current state"
            warning="- Safe: Preserves all running applications\\n- Speed: Fast pause/resume\\n- Use: Temporary resource management"
            ;;
        *)
            return 0  # No confirmation needed for non-critical actions
            ;;
    esac
    
    local prompt_text="CRITICAL ACTION: $description\\n\\nEffects on $vm_count VM/CT(s):\\n$warning\\n\\nTo confirm this action, type '$action':"
    
    while true; do
        # Confirmation input with description
        local confirm_input=$(mktemp)
        dialog --title "CONFIRM CRITICAL ACTION" --inputbox "$prompt_text" 18 80 2> "$confirm_input"
        local exit_code=$?
        local confirmation=$(trimq "$(cat "$confirm_input")")
        rm -f "$confirm_input"
        
        # User cancelled
        if [[ $exit_code -ne 0 ]]; then
            return 1
        fi
        
        # Correct input
        if [[ "$confirmation" == "$action" ]]; then
            return 0
        fi
        
        # Wrong input - show error and retry
        dialog --title "INPUT ERROR" --msgbox "You typed: '$confirmation'\\n\\nExpected: '$action'\\n\\nPlease try again or use Cancel to abort." 10 60
    done
}

# Action selection dialog for VM/CT Manager
vm_manager_select_action() {
    local actions=(
        "start" "Start selected VM/CT"
        "shutdown" "Graceful shutdown of selected VM/CT"
        "stop" "Force stop selected VM/CT"
        "reboot" "Reboot selected VM/CT"
        "reset" "Reset selected VM/CT"
        "suspend" "Suspend selected VM/CT to memory"
        "resume" "Resume suspended VM/CT"
    )
    
    cmd=(dialog --keep-tite --title "VM/CT Manager - Select Action"
         --ok-label "Execute" --cancel-label "Back"
         --menu "Choose action to perform on selected VM/CT:" 15 60 8)
    options=("${actions[@]}")
    
    dialog_menu
    
    case $dialog_exit_code in
        0)  # Execute
            if [[ -n "$selected_option" ]]; then
                log_info "Selected action: $selected_option"
                return 0
            fi
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Get VM/CT type, current status and node from cluster for VM/CT Manager
vm_manager_get_vm_type_status_node() {
    local vmid="$1"
    
    # Get VM/CT info from cluster resources
    local vm_info=$(pvesh get /cluster/resources --type vm --output-format=json 2>/dev/null | \
        python3 -c "
import sys, json
try:
    resources = json.load(sys.stdin)
    for resource in resources:
        if str(resource.get('vmid', '')) == '$vmid':
            res_type = resource.get('type', '')
            status = resource.get('status', 'unknown')
            node = resource.get('node', '')
            lock = resource.get('lock', '')
            
            # Map resource type to our format
            if res_type == 'qemu':
                vm_type = 'vm'
            elif res_type == 'lxc':
                vm_type = 'ct'
            else:
                vm_type = 'unknown'
                
            # Check if VM/CT is suspended (has suspended lock or paused status)
            if lock == 'suspended' or status == 'paused':
                status = 'suspended'
                
            print(f'{vm_type}|{status}|{node}')
            sys.exit(0)
    print('unknown|unknown|unknown')
except Exception as e:
    print('unknown|unknown|unknown')
    print(f'Error: {e}', file=sys.stderr)
")
    
    if [[ -n "$vm_info" && "$vm_info" != "unknown|unknown|unknown" ]]; then
        echo "$vm_info"
        return 0
    fi
    
    echo "unknown|unknown|unknown"
    return 1
}

# Get VM/CT detailed info including name and tags
vm_manager_get_vm_details() {
    local vmid="$1"
    
    # Get VM/CT info from cluster resources
    local vm_info=$(pvesh get /cluster/resources --type vm --output-format=json 2>/dev/null | \
        python3 -c "
import sys, json
try:
    resources = json.load(sys.stdin)
    for resource in resources:
        if str(resource.get('vmid', '')) == '$vmid':
            res_type = resource.get('type', '')
            status = resource.get('status', 'unknown')
            node = resource.get('node', '')
            lock = resource.get('lock', '')
            name = resource.get('name', '')
            
            # Map resource type to our format
            if res_type == 'qemu':
                vm_type = 'vm'
            elif res_type == 'lxc':
                vm_type = 'ct'
            else:
                vm_type = 'unknown'
                
            # Check if VM/CT is suspended (has suspended lock or paused status)
            if lock == 'suspended' or status == 'paused':
                status = 'suspended'
                
            print(f'{vm_type}|{status}|{node}|{name}')
            sys.exit(0)
    print('unknown|unknown|unknown|')
except Exception as e:
    print('unknown|unknown|unknown|')
    print(f'Error: {e}', file=sys.stderr)
")
    
    # Get tags from VM/CT config if available
    local vm_type_status_node_name="$vm_info"
    local vm_type
    IFS='|' read -r vm_type _ _ _ <<< "$vm_type_status_node_name"
    
    local tags=""
    if [[ "$vm_type" == "vm" ]]; then
        # Get tags from qemu config
        tags=$(pvesh get /cluster/resources --type vm --output-format=json 2>/dev/null | \
            python3 -c "
import sys, json
try:
    resources = json.load(sys.stdin)
    for resource in resources:
        if str(resource.get('vmid', '')) == '$vmid':
            tags = resource.get('tags', '')
            print(tags)
            sys.exit(0)
    print('')
except Exception as e:
    print('')
    print(f'Error: {e}', file=sys.stderr)
")
    elif [[ "$vm_type" == "ct" ]]; then
        # Get tags from lxc config
        tags=$(pvesh get /cluster/resources --type vm --output-format=json 2>/dev/null | \
            python3 -c "
import sys, json
try:
    resources = json.load(sys.stdin)
    for resource in resources:
        if str(resource.get('vmid', '')) == '$vmid':
            tags = resource.get('tags', '')
            print(tags)
            sys.exit(0)
    print('')
except Exception as e:
    print('')
    print(f'Error: {e}', file=sys.stderr)
")
    fi
    
    if [[ -n "$vm_info" && "$vm_info" != "unknown|unknown|unknown|" ]]; then
        echo "${vm_info}|${tags}"
        return 0
    fi
    
    echo "unknown|unknown|unknown||"
    return 1
}

# Execute action on VM/CT for VM/CT Manager
vm_manager_execute_action() {
    local vmid="$1"
    local action="$2"
    local vm_type_status_node
    local vm_type
    local current_status
    local vm_node
    local current_node
    local cmd_to_run
    local result
    
    # Get VM type, current status and node
    vm_type_status_node=$(vm_manager_get_vm_type_status_node "$vmid")
    IFS='|' read -r vm_type current_status vm_node <<< "$vm_type_status_node"
    
    # Get current node name
    current_node=$(hostname)
    
    log_debug "VM/CT $vmid: type=$vm_type, status=$current_status, node=$vm_node, action=$action"
    log_debug "Raw vm_type_status_node response: '$vm_type_status_node'"
    log_debug "Current node: $current_node, VM node: $vm_node"
    
    # Check if action is needed
    case "$action" in
        "start")
            if [[ "$current_status" == "running" ]]; then
                log_info "VM/CT $vmid already running, skipping start"
                return 2  # Skip
            elif [[ "$current_status" == "suspended" ]]; then
                log_info "VM/CT $vmid is suspended, skipping start (use resume instead)"
                return 2  # Skip
            fi
            ;;
        "stop"|"shutdown")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid already stopped, skipping $action"
                return 2  # Skip
            fi
            ;;
        "reboot"|"reset")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid is stopped, skipping $action (requires running VM/CT)"
                return 2  # Skip
            fi
            ;;
        "suspend")
            if [[ "$current_status" == "stopped" ]]; then
                log_info "VM/CT $vmid is stopped, skipping suspend (requires running VM/CT)"
                return 2  # Skip
            elif [[ "$current_status" == "suspended" ]]; then
                log_info "VM/CT $vmid already suspended, skipping suspend"
                return 2  # Skip
            fi
            ;;
        "resume")
            if [[ "$current_status" != "suspended" ]]; then
                log_info "VM/CT $vmid is not suspended, skipping resume (status: $current_status)"
                return 2  # Skip
            fi
            ;;
    esac
    
    # Build command
    if [[ "$vm_type" == "vm" ]]; then
        local base_cmd="qm $action $vmid"
        log_debug "Building VM command: $base_cmd"
    elif [[ "$vm_type" == "ct" ]]; then
        # For containers, reset should be reboot
        if [[ "$action" == "reset" ]]; then
            local base_cmd="pct reboot $vmid"
            log_debug "Building CT command (reset->reboot): $base_cmd"
        else
            local base_cmd="pct $action $vmid"
            log_debug "Building CT command: $base_cmd"
        fi
    else
        log_error "Unknown VM/CT type '$vm_type' for $vmid"
        return 1
    fi
    
    # Execute command on correct node
    if [[ "$vm_node" == "$current_node" ]]; then
        # Execute locally
        cmd_to_run="$base_cmd"
        log_debug "Executing locally: $cmd_to_run"
        log_info "Executing locally: $cmd_to_run"
        
        result=$(bash -c "$cmd_to_run" 2>&1)
        local exit_code=$?
        
        log_debug "Command result: exit_code=$exit_code, output='$result'"
        
        if [[ $exit_code -eq 0 ]]; then
            log_info "Action $action completed successfully for VM/CT $vmid"
            return 0
        else
            log_error "Action $action failed for VM/CT $vmid: $result (exit code: $exit_code)"
            return 1
        fi
    else
        # Execute on remote node using pvesh
        if [[ "$vm_type" == "vm" ]]; then
            cmd_to_run="pvesh create /nodes/$vm_node/qemu/$vmid/status/$action"
        elif [[ "$vm_type" == "ct" ]]; then
            # For containers, reset should be reboot in pvesh as well
            if [[ "$action" == "reset" ]]; then
                cmd_to_run="pvesh create /nodes/$vm_node/lxc/$vmid/status/reboot"
                log_debug "Building remote CT command (reset->reboot): $cmd_to_run"
            else
                cmd_to_run="pvesh create /nodes/$vm_node/lxc/$vmid/status/$action"
            fi
        fi
        
        log_debug "Executing on remote node $vm_node: $cmd_to_run"
        log_info "Executing on node $vm_node: $cmd_to_run"
        
        result=$(bash -c "$cmd_to_run" 2>&1)
        local exit_code=$?
        
        log_debug "Remote command result: exit_code=$exit_code, output='$result'"
        
        if [[ $exit_code -eq 0 ]]; then
            log_info "Action $action completed successfully for VM/CT $vmid on node $vm_node"
            return 0
        else
            log_error "Action $action failed for VM/CT $vmid on node $vm_node: $result (exit code: $exit_code)"
            return 1
        fi
    fi
}

# Execute actions with progress for VM/CT Manager
vm_manager_execute_actions() {
    local action="$1"
    local total_count=${#vm_manager_selected_vms[@]}
    local success_count=0
    local skip_count=0
    local error_count=0
    local current=0
    
    log_debug "Starting batch action '$action' on $total_count VMs/CTs"
    log_debug "Selected VMs/CTs: ${vm_manager_selected_vms[*]}"
    
    # Initialize action counts
    vm_manager_action_counts=("$success_count" "$skip_count" "$error_count")
    
    for vmid in "${vm_manager_selected_vms[@]}"; do
        current=$((current + 1))
        local progress=$((current * 100 / total_count))
        
        # Get VM details for display
        local vm_details=$(vm_manager_get_vm_details "$vmid")
        local vm_type vm_name vm_tags
        IFS='|' read -r vm_type _ _ vm_name vm_tags <<< "$vm_details"
        
        # Convert vm_type to display format
        local display_type
        if [[ "$vm_type" == "vm" ]]; then
            display_type="VM"
        elif [[ "$vm_type" == "ct" ]]; then
            display_type="CT"
        else
            display_type="VM/CT"
        fi
        
        # Build display name with tags
        local display_name="$vmid"
        if [[ -n "$vm_name" ]]; then
            display_name="$vmid ($vm_name)"
        fi
        if [[ -n "$vm_tags" ]]; then
            display_name="$display_name [$vm_tags]"
        fi
        
        # Show progress
        echo "$progress" | dialog --title "Processing $display_type" --gauge "Executing $action on $display_type $display_name ($current/$total_count)" 8 90
        
        # Execute action
        vm_manager_execute_action "$vmid" "$action"
        local exit_code=$?
        
        case $exit_code in
            0)  # Success
                success_count=$((success_count + 1))
                ;;
            1)  # Error
                error_count=$((error_count + 1))
                ;;
            2)  # Skip
                skip_count=$((skip_count + 1))
                ;;
        esac
        
        # Small delay for user feedback
        sleep 0.5
    done
    
    # Update action counts
    vm_manager_action_counts=("$success_count" "$skip_count" "$error_count")
}

# Show final summary for VM/CT Manager
vm_manager_show_summary() {
    local action="$1"
    local success_count="${vm_manager_action_counts[0]}"
    local skip_count="${vm_manager_action_counts[1]}"
    local error_count="${vm_manager_action_counts[2]}"
    local total_count=${#vm_manager_selected_vms[@]}
    
    local summary_text="Action: $action\n\n"
    summary_text+="Total VMs/CTs processed: $total_count\n"
    summary_text+="Successful: $success_count\n"
    summary_text+="Skipped: $skip_count\n"
    summary_text+="Errors: $error_count\n\n"
    
    if [[ $error_count -gt 0 ]]; then
        summary_text+="Check log file for error details: $LOG_FILE"
    fi
    
    dialog --ok-label "Continue" --title "Action Summary" --msgbox "$summary_text" 15 60
    log_info "Action summary - Success: $success_count, Skipped: $skip_count, Errors: $error_count"
}

# VM/CT Manager - Bulk Actions main function
vm_ct_manager_bulk_actions() {
    # Check if running as root (required for Proxmox commands)
    if [[ $EUID -ne 0 ]]; then
        dialog --title "Error" --msgbox "VM/CT Manager must be run as root to access Proxmox VE commands." 8 60
        return 1
    fi
    
    # Check if Proxmox commands are available
    if ! command -v qm >/dev/null 2>&1 || ! command -v pct >/dev/null 2>&1; then
        dialog --title "Error" --msgbox "Proxmox VE commands (qm/pct) not found. Please run on a Proxmox VE node." 10 60
        return 1
    fi
    
    while true; do
        # Step 1: Select VMs/CTs
        if ! vm_manager_select_vm_ct; then
            return 0  # Back to main menu
        fi
        
        # Step 2: Select action
        if ! vm_manager_select_action; then
            continue  # Go back to VM selection
        fi
        
        # Step 3: Confirm critical actions and execute
        if ! vm_manager_confirm_critical_action "$selected_option"; then
            continue  # Go back to action selection if cancelled
        fi
        vm_manager_execute_actions "$selected_option"
        
        # Step 4: Show summary
        vm_manager_show_summary "$selected_option"
        
        return 0  # Back to main menu
    done
}

# PVE HA RESOURCES FUNCTIONS

# Configuration
GROUP="ha-nodes"

# Function: get_groups
get_groups() {
  ha-manager groupconfig 2>/dev/null | grep -E '^group:' | awk '{print $2}'
}

# Function: get_vms_with_hostpci
get_vms_with_hostpci() {
  grep -Rl hostpci /etc/pve/nodes/*/qemu-server/*.conf 2>/dev/null | \
    sed -nE 's#.*/([0-9]+)\.conf#\1#p' | sort -u
}

# Get all PVE nodes
get_all_nodes() {
    local nodes_result
    nodes_result=$(timeout 30 pvesh get /nodes --output-format=json 2>/dev/null | python3 -c '
import sys, json
try:
    nodes = json.load(sys.stdin)
    # extract and sort the "node" values, then join with spaces
    print(" ".join(sorted(n["node"] for n in nodes)))
except:
    sys.exit(1)
' 2>/dev/null)
    
    if [ $? -eq 0 ] && [ -n "$nodes_result" ]; then
        echo "$nodes_result"
        return 0
    else
        log_error "Failed to get PVE nodes"
        return 1
    fi
}

# Select first HA node
select_first_ha_node() {
    local nodes=() menu_items=()
    local line
    
    # Get available nodes
    local all_nodes
    all_nodes=$(get_all_nodes)
    if [ -z "$all_nodes" ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Split nodes into array
    readarray -t nodes <<< "$(echo "$all_nodes" | tr ' ' '\n')"
    
    if [ "${#nodes[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Build menu items
    local i
    for ((i=0; i<${#nodes[@]}; i++)); do
        menu_items+=("$i" "${nodes[i]}")
    done
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Select First HA Node"
           --ok-label "Next" --cancel-label "Back to Main"
           --menu "Choose first PVE node for HA group \"$GROUP\":" 15 60 8)
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    if [ $dialog_exit_code -eq 0 ] && [ -n "$selected_option" ]; then
        FIRST_NODE="${nodes[selected_option]}"
        log_info "User selected first HA node: $FIRST_NODE"
        return 0
    else
        return 1
    fi
}

# Select second HA node
select_second_ha_node() {
    local nodes=() menu_items=()
    local line
    
    # Get available nodes
    local all_nodes
    all_nodes=$(get_all_nodes)
    if [ -z "$all_nodes" ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Split nodes into array
    readarray -t nodes <<< "$(echo "$all_nodes" | tr ' ' '\n')"
    
    if [ "${#nodes[@]}" -eq 0 ]; then
        dialog --msgbox "No PVE nodes found." 8 40
        return 1
    fi
    
    # Build menu items excluding first node
    local i
    for ((i=0; i<${#nodes[@]}; i++)); do
        if [ "${nodes[i]}" != "$FIRST_NODE" ]; then
            menu_items+=("$i" "${nodes[i]}")
        fi
    done
    
    if [ "${#menu_items[@]}" -eq 0 ]; then
        dialog --msgbox "No additional nodes available for HA group." 8 50
        return 1
    fi
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Select Second HA Node"
           --ok-label "Next" --cancel-label "Back"
           --menu "Choose second PVE node for HA group \"$GROUP\":" 15 60 8)
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            if [ -n "$selected_option" ]; then
                SECOND_NODE="${nodes[selected_option]}"
                log_info "User selected second HA node: $SECOND_NODE"
                return 0
            else
                return 1
            fi
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Confirm HA node selection
confirm_ha_nodes() {
    local nodes_text="First node:  $FIRST_NODE\nSecond node: $SECOND_NODE"
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Confirm HA Node Selection"
           --yes-label "Next" --no-label "Back"
           --yesno "Selected nodes for HA group \"$GROUP\":\n\n$nodes_text\n\nProceed with these nodes?" 12 60)
    local options=()
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            GROUP_NODES=("$FIRST_NODE" "$SECOND_NODE")
            NODES_CSV=$(IFS=, ; echo "${GROUP_NODES[*]}")
            log_info "Confirmed HA nodes: $NODES_CSV"
            return 0
            ;;
        1)  # Back
            return 1
            ;;
    esac
}

# Get all VM/CT resources from cluster
get_all_vm_ct_resources() {
    pvesh get /cluster/resources --type vm --output-format=json | \
    python3 -c "
import sys, json
resources = json.load(sys.stdin)
for resource in resources:
    if resource['type'] in ['qemu', 'lxc']:
        vmid = resource['vmid']
        name = resource.get('name', 'N/A')
        vm_type = 'vm' if resource['type'] == 'qemu' else 'ct'
        status = resource.get('status', 'unknown')
        node = resource.get('node', 'unknown')
        
        # Get maximum field lengths for alignment
        print(f\"{vmid}|{name}|{vm_type}|{status}|{node}\")
"
}

# Calculate maximum field lengths for formatting
get_max_field_lengths() {
    local resources="$1"  # Accept resources as parameter to avoid duplicate calls
    
    local max_name_length=4  # minimum for "Name"
    local max_node_length=4  # minimum for "Node"
    
    while IFS='|' read -r vmid name vm_type status node; do
        if [ ${#name} -gt $max_name_length ]; then
            max_name_length=${#name}
        fi
        if [ ${#node} -gt $max_node_length ]; then
            max_node_length=${#node}
        fi
    done <<< "$resources"
    
    echo "$max_name_length|$max_node_length"
}

# Select VM/CT for HA Resources
select_vm_ct_for_ha() {
    local resources=() menu_items=() vmids=()
    
    # Get all VM/CT resources once
    local all_resources_data
    all_resources_data=$(get_all_vm_ct_resources)
    
    # Get HA manager status once to avoid multiple calls
    local ha_manager_status=$(ha-manager status)
    
    if [ -z "$all_resources_data" ]; then
        dialog --msgbox "No VMs/CTs found in the cluster." 8 40
        return 1
    fi
    
    # Parse resources data
    while IFS='|' read -r vmid name vm_type status node; do
        if [ -n "$vmid" ]; then
            resources+=("$vmid|$name|$vm_type|$status|$node")
            vmids+=("$vmid")
        fi
    done <<< "$all_resources_data"
    
    if [ "${#resources[@]}" -eq 0 ]; then
        dialog --msgbox "No VMs/CTs found in the cluster." 8 40
        return 1
    fi
    
    # Get max field lengths for formatting using cached data
    local max_lengths
    max_lengths=$(get_max_field_lengths "$all_resources_data")
    local max_name_length=$(echo "$max_lengths" | cut -d'|' -f1)
    local max_node_length=$(echo "$max_lengths" | cut -d'|' -f2)
    
    # Process each resource entry
    for resource in "${resources[@]}"; do
        IFS='|' read -r vmid name vm_type status node <<< "$resource"
        
        # Skip hostpci VMs
        local skip_hostpci=false
        if [[ "$vm_type" == "vm" ]] && [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            skip_hostpci=true
        fi
        
        # Skip if already in HA
        local skip_ha=false
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "${vm_type}:${vmid}"; then
            skip_ha=true
        fi
        
        # Format display string
        local formatted_name=$(printf "%-${max_name_length}s" "$name")
        local formatted_node=$(printf "%-${max_node_length}s" "$node")
        
        local display_string=$(printf "%s  %s  %s  %s" "$formatted_name" "$vm_type" "$status" "$formatted_node")
        
        # Add suffix if skipped
        if [ "$skip_hostpci" = true ]; then
            display_string+="  (hostpci - will be skipped)"
        elif [ "$skip_ha" = true ]; then
            display_string+="  (already in HA - will be skipped)"
        fi
        
        # Check if this VM/CT was previously selected
        local item_status="off"
        for selected_vmid in "${SELECTED_VMIDS[@]}"; do
            if [[ "$selected_vmid" == "$vmid" ]]; then
                item_status="on"
                break
            fi
        done
        
        menu_items+=("$vmid" "$display_string" "$item_status")
    done
    
    local dialog_selected=$(mktemp)
    local select_label="Select All"
    if [[ "$select_all_mode" == true ]]; then
        select_label="Deselect All"
    fi
    
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: Select VM/CT for HA Resources"
         --ok-label "Next" --cancel-label "Back" --extra-button --extra-label "$select_label"
         --checklist "Choose VM/CT to add to HA resources (use SPACE to select/deselect):" 20 120 10)
    local options=("${menu_items[@]}")
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Next
            if [[ -z "$selected_option" ]]; then
                dialog --msgbox "Please select at least one VM/CT to add to HA resources." 8 50
                return 2  # Stay on same step
            fi
            
            # Parse selected VM IDs
            SELECTED_VMIDS=()
            IFS=' ' read -ra temp_vmids <<< "$selected_option"
            for vmid in "${temp_vmids[@]}"; do
                # Remove quotes if present
                vmid=$(echo "$vmid" | tr -d '"')
                SELECTED_VMIDS+=("$vmid")
            done
            return 0
            ;;
        1)  # Back
            return 1
            ;;
        3)  # Select All / Deselect All
            if [[ "$select_all_mode" == false ]]; then
                # Select all
                SELECTED_VMIDS=()
                for vmid in "${vmids[@]}"; do
                    SELECTED_VMIDS+=("$vmid")
                done
                select_all_mode=true
            else
                # Deselect all
                SELECTED_VMIDS=()
                select_all_mode=false
            fi
            return 2  # Stay on same step
            ;;
    esac
}

# Show confirmation dialog
show_confirmation_dialog() {
    local selected_count="${#SELECTED_VMIDS[@]}"
    local hostpci_count="${#HOSTPCI_VMS[@]}"
    
    # Get HA manager status once to avoid multiple calls
    local ha_manager_status=$(ha-manager status)
    
    # Count VMs/CTs that will be skipped
    local skip_count=0
    local skip_details=""
    
    for vmid in "${SELECTED_VMIDS[@]}"; do
        # Check if VM has hostpci
        if [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            skip_details+="- VM $vmid (hostpci present)\n"
            ((skip_count++))
        fi
        
        # Check if already in HA
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "vm:$vmid" || echo "$ha_manager_status" | awk '{print $1}' | grep -qx "ct:$vmid"; then
            skip_details+="- VM/CT $vmid (already in HA)\n"
            ((skip_count++))
        fi
    done
    
    local info_text="Summary of VM/CT selection for HA resources:\n\n"
    info_text+="Selected VM/CT count: $selected_count\n"
    info_text+="Target HA group: $GROUP\n"
    info_text+="Target HA nodes: $NODES_CSV\n\n"
    
    if [ $skip_count -gt 0 ]; then
        info_text+="Items that will be skipped ($skip_count):\n$skip_details\n"
    fi
    
    local process_count=$((selected_count - skip_count))
    info_text+="Items to be processed: $process_count\n\n"
    info_text+="Do you want to continue?"
    
    local dialog_selected=$(mktemp)
    local cmd=(dialog --keep-tite --title "Step $current_step/$total_steps: HA Resources Summary"
           --yes-label "Execute" --no-label "Back"
           --yesno "$info_text" 20 80)
    local options=()
    
    dialog_menu
    rm -f "$dialog_selected"
    
    case $dialog_exit_code in
        0)  # Execute
            log_info "User confirmed HA resources addition operation"
            return 0
            ;;
        1)  # Back
            log_info "User went back from HA resources addition confirmation"
            return 1
            ;;
    esac
}

# Process and add resources
process_ha_resources() {
    local total_processed=0
    local total_added=0
    local total_skipped=0
    local results_text=""
    
    # Get HA manager status once to avoid multiple calls
    local ha_manager_status=$(ha-manager status)
    
    # Get type information for selected VMs/CTs
    local vm_type_map
    vm_type_map=$(pvesh get /cluster/resources --type vm --output-format=json | \
    python3 -c "
import sys, json
resources = json.load(sys.stdin)
mapping = {'qemu': 'vm', 'lxc': 'ct'}
for resource in resources:
    if resource['type'] in mapping:
        print(f\"{resource['vmid']}:{mapping[resource['type']]}\")
")
    
    # Count total selected resources for progress
    local total_resources=${#SELECTED_VMIDS[@]}
    
    # Process each selected VM/CT
    for vmid in "${SELECTED_VMIDS[@]}"; do
        ((total_processed++))
        
        # Get VM/CT type
        local vm_type
        vm_type=$(echo "$vm_type_map" | grep "^$vmid:" | cut -d':' -f2)
        if [ -z "$vm_type" ]; then
            log_error "Cannot determine type for VM/CT $vmid"
            results_text+="Failed to determine type for $vmid\n"
            continue
        fi
        
        local sid="${vm_type}:${vmid}"
        
        # Update progress
        local progress=$((total_processed * 100 / total_resources))
        echo "$progress" | dialog --gauge "Processing selected VM/CT resources...\nCurrent: $sid" 8 60 0
        
        # Skip hostpci VMs
        if [[ "$vm_type" == "vm" ]] && [[ " ${HOSTPCI_VMS[*]} " == *" $vmid "* ]]; then
            log_info "Skipping vm:$vmid (hostpci present)"
            results_text+="Skipped vm:$vmid (hostpci present)\n"
            ((total_skipped++))
            continue
        fi
        
        # Skip if already in HA group
        if echo "$ha_manager_status" | awk '{print $1}' | grep -qx "$sid"; then
            log_info "Skipping $sid (already in HA)"
            results_text+="Skipped $sid (already in HA)\n"
            ((total_skipped++))
            continue
        fi
        
        # Add to HA group
        if ha-manager add "$sid" --state started --group "$GROUP" \
            --comment "HA ${vmid} on ${NODES_CSV}" 2>/dev/null; then
            log_info "Added $sid to HA group \"$GROUP\""
            results_text+="Added $sid to HA group\n"
            ((total_added++))
        else
            log_error "Failed to add $sid to HA group"
            results_text+="Failed to add $sid\n"
        fi
    done
    
    # Show results
    local summary="Operation completed:\n\n"
    summary+="Total processed: $total_processed\n"
    summary+="Successfully added: $total_added\n"
    summary+="Skipped: $total_skipped\n\n"
    summary+="Details:\n$results_text"
    
    dialog --keep-tite --title "HA Resources Addition Results" \
           --ok-label "OK" \
           --msgbox "$summary" 20 80
    
    log_info "HA resources processing completed: $total_added added, $total_skipped skipped"
}

# Main wizard function
add_vmct_to_ha_resources() {
    log_info "Starting Add VM/CT to HA Resources wizard"
    
    # Initialize wizard variables
    SELECTED_VMIDS=()
    current_step=1
    total_steps=5
    select_all_mode=false
    
    # Collect VMIDs using PCI passthrough (needed for selection display)
    readarray -t HOSTPCI_VMS < <(get_vms_with_hostpci)
    
    while true; do
        case $current_step in
            1)
                # Step 1: Select first HA node
                log_info "Wizard: Starting step 1 - Select first HA node"
                select_first_ha_node
                case $? in
                    0) 
                        log_info "Wizard: Step 1 completed successfully, moving to step 2"
                        current_step=2
                        ;;
                    1) 
                        log_info "User canceled first node selection"
                        return 0
                        ;;
                esac
                ;;
            2)
                # Step 2: Select second HA node
                log_info "Wizard: Starting step 2 - Select second HA node"
                select_second_ha_node
                local step2_result=$?
                log_info "Wizard: Step 2 returned code: $step2_result"
                case $step2_result in
                    0) current_step=3 ;;  # Next
                    1) current_step=1 ;;  # Back
                esac
                ;;
            3)
                # Step 3: Confirm node selection
                confirm_ha_nodes
                case $? in
                    0) current_step=4 ;;  # Next
                    1) current_step=2 ;;  # Back
                esac
                ;;
            4)
                # Step 4: Select VM/CT
                select_vm_ct_for_ha
                case $? in
                    0) current_step=5 ;;  # Next
                    1) current_step=3 ;;  # Back
                    2) ;;                 # Stay on same step
                esac
                ;;
            5)
                # Step 5: Show confirmation dialog and execute
                show_confirmation_dialog
                case $? in
                    0) # Execute
                        # Ensure HA group exists
                        if ! get_groups | grep -qw "$GROUP"; then
                            log_info "Group '$GROUP' does not exist. Creating restricted group on nodes: $NODES_CSV"
                            ha-manager groupadd "$GROUP" \
                                --nodes "$NODES_CSV" \
                                --restricted 1 \
                                --comment "Auto-created by pve-tools"
                        fi
                        
                        # Process resources
                        process_ha_resources
                        
                        log_info "Add VM/CT to HA Resources wizard completed"
                        return 0
                        ;;
                    1) current_step=4 ;;  # Back
                esac
                ;;
        esac
    done
}

# MAIN SCRIPT EXECUTION

# Cleanup background processes and temporary files
cleanup_background_processes() {
    if [ -n "$PVE_CONFIG_BACKUP_PID" ]; then
        if kill -0 "$PVE_CONFIG_BACKUP_PID" 2>/dev/null; then
            log_info "Terminating background pve-config-backup check (PID: $PVE_CONFIG_BACKUP_PID)"
            kill "$PVE_CONFIG_BACKUP_PID" 2>/dev/null
        fi
    fi
    
    # Clean up temporary files
    rm -f /tmp/pve-config-backup-status /tmp/pve-config-backup-check.log
}

# Main script entry point
main() {
    # Initialize logging system first
    if ! init_logging; then
        echo "Error: Failed to initialize logging system" >&2
        exit 1
    fi
    
    log_info "Starting PVE Tools script"
    
    # Check for dialog dependency and install if missing
    check_for_dialog_and_install_if_missing
    
    # Initialize configuration
    if ! init_config; then
        log_error "Failed to initialize configuration"
        exit 1
    fi
    
    # Check if any NFS storage exists
    if ! check_nfs_storage_exists; then
        dialog --title "No NFS Storage Found" \
               --msgbox "No NFS storage found in Proxmox VE. Please connect one first." \
               8 60 \
               --ok-label "Exit"
        log_info "Script terminated: No NFS storage found"
        exit 0
    fi
    
    # Build storage to IP mapping table
    build_storage_ip_map
    
    log_info "PVE Tools initialized"
    
    # Check for updates
    if check_for_updates; then
        prompt_for_update
    fi
    
    # Check for pve-config-backup updates
    check_and_update_pve_config_backup
    
    # Start pve-config-backup check in background
    PVE_CONFIG_BACKUP_STATUS="checking"
    PVE_CONFIG_BACKUP_RESULT=""
    (
        ensure_pve_config_backup_running > /tmp/pve-config-backup-check.log 2>&1
        echo "COMPLETED:$(date '+%H:%M:%S')" > /tmp/pve-config-backup-status
    ) &
    PVE_CONFIG_BACKUP_PID=$!
    
    log_info "Started pve-config-backup check in background (PID: $PVE_CONFIG_BACKUP_PID)"
    
    # Setup cleanup on exit
    trap 'cleanup_background_processes' EXIT
    
    # Main application loop
    clear
    while true; do
        main_menu
    done
}

# Only run main if this script is executed directly (not sourced)
if [ "${BASH_SOURCE[0]}" = "${0}" ]; then
    # Handle version argument before any terminal operations
    if [ "$1" = "version" ]; then
        VERSION_MODE=true
        echo "$VERSION"
        exit 0
    fi
    main "$@"
fi
